<!DOCTYPE HTML>
<html lang="zh-cn" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>TensorStack AI 计算平台 - 安装手册</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">
        <meta name="robots" content="noindex">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/sidebar.css">
        <link rel="stylesheet" href="theme/custom.css">

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="overview.html">概述</a></li><li class="spacer"></li><li class="chapter-item expanded "><a href="online/index.html"><strong aria-hidden="true">1.</strong> 在线安装</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="online/inventory/index.html"><strong aria-hidden="true">1.1.</strong> 设置 ansible inventory</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="online/inventory/basic-settings.html"><strong aria-hidden="true">1.1.1.</strong> 基本设置</a></li><li class="chapter-item expanded "><a href="online/inventory/advanced-settings.html"><strong aria-hidden="true">1.1.2.</strong> 高级设置</a></li></ol></li><li class="chapter-item expanded "><a href="online/prepare-nodes.html"><strong aria-hidden="true">1.2.</strong> 准备节点</a></li><li class="chapter-item expanded "><a href="online/k8s-index.html"><strong aria-hidden="true">1.3.</strong> 安装 K8s</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="online/k8s-install.html"><strong aria-hidden="true">1.3.1.</strong> 基本安装</a></li><li class="chapter-item expanded "><a href="online/cri.html"><strong aria-hidden="true">1.3.2.</strong> CRI 配置</a></li><li class="chapter-item expanded "><a href="online/cni.html"><strong aria-hidden="true">1.3.3.</strong> CNI 配置</a></li><li class="chapter-item expanded "><a href="online/k8s-userns.html"><strong aria-hidden="true">1.3.4.</strong> 设置 User Namespace</a></li><li class="chapter-item expanded "><a href="online/k8s-storage.html"><strong aria-hidden="true">1.3.5.</strong> 设置集群存储</a></li><li class="chapter-item expanded "><a href="online/k8s-ops.html"><strong aria-hidden="true">1.3.6.</strong> 集群维护</a></li><li class="chapter-item expanded "><a href="online/k8s-install-faqs.html"><strong aria-hidden="true">1.3.7.</strong> 常见问题</a></li><li class="chapter-item expanded "><a href="online/k8s-post-install.html"><strong aria-hidden="true">1.3.8.</strong> 安装后配置</a></li></ol></li><li class="chapter-item expanded "><a href="online/k8s-components/index.html"><strong aria-hidden="true">1.4.</strong> 安装 K8s 组件</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="online/k8s-components/istio.html"><strong aria-hidden="true">1.4.1.</strong> Istio</a></li><li class="chapter-item expanded "><a href="online/k8s-components/knative.html"><strong aria-hidden="true">1.4.2.</strong> Knative</a></li><li class="chapter-item expanded "><a href="online/k8s-components/metrics-server.html"><strong aria-hidden="true">1.4.3.</strong> Metrics Server</a></li><li class="chapter-item expanded "><a href="online/k8s-components/elastic-search.html"><strong aria-hidden="true">1.4.4.</strong> Elastic Search</a></li><li class="chapter-item expanded "><a href="online/k8s-components/loki.html"><strong aria-hidden="true">1.4.5.</strong> Loki</a></li><li class="chapter-item expanded "><a href="online/k8s-components/monitoring.html"><strong aria-hidden="true">1.4.6.</strong> 监控相关</a></li><li class="chapter-item expanded "><a href="online/k8s-components/gatekeeper.html"><strong aria-hidden="true">1.4.7.</strong> Gatekeeper</a></li></ol></li><li class="chapter-item expanded "><a href="hardware/index.html"><strong aria-hidden="true">1.5.</strong> 安装硬件支持</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="hardware/nvidia/index.html"><strong aria-hidden="true">1.5.1.</strong> NVIDIA</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="hardware/nvidia/gpu-operator.html"><strong aria-hidden="true">1.5.1.1.</strong> GPU Operator</a></li><li class="chapter-item expanded "><a href="hardware/nvidia/network-operator.html"><strong aria-hidden="true">1.5.1.2.</strong> Network Operator</a></li></ol></li><li class="chapter-item expanded "><a href="hardware/amd/index.html"><strong aria-hidden="true">1.5.2.</strong> AMD</a></li><li class="chapter-item expanded "><a href="hardware/enflame/index.html"><strong aria-hidden="true">1.5.3.</strong> 燧原 Enflame</a></li><li class="chapter-item expanded "><a href="hardware/hygon/index.html"><strong aria-hidden="true">1.5.4.</strong> 海光 Hygon</a></li><li class="chapter-item expanded "><a href="hardware/huawei/index.html"><strong aria-hidden="true">1.5.5.</strong> 华为</a></li><li class="chapter-item expanded "><a href="hardware/iluvatar/index.html"><strong aria-hidden="true">1.5.6.</strong> 天数智芯 iluvatar</a></li><li class="chapter-item expanded "><a href="hardware/metax/index.html"><strong aria-hidden="true">1.5.7.</strong> 沐曦 MetaX</a></li></ol></li><li class="chapter-item expanded "><a href="online/products/index.html"><strong aria-hidden="true">1.6.</strong> 安装 TensorStack AI 计算平台</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="online/products/pre-install.html"><strong aria-hidden="true">1.6.1.</strong> 安装前准备</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="online/products/pre-install/t9k-auditing.html"><strong aria-hidden="true">1.6.1.1.</strong> 审计日志</a></li></ol></li><li class="chapter-item expanded "><a href="online/products/install-uc-mode.html"><strong aria-hidden="true">1.6.2.</strong> 安装产品-User Console 模式</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="online/products/install-uc.html"><strong aria-hidden="true">1.6.2.1.</strong> 安装产品</a></li><li class="chapter-item expanded "><a href="online/products/register-app.html"><strong aria-hidden="true">1.6.2.2.</strong> 注册 APP</a></li></ol></li><li class="chapter-item expanded "><a href="online/products/install-traditional-mode.html"><strong aria-hidden="true">1.6.3.</strong> 安装产品-传统模式</a></li><li class="chapter-item expanded "><a href="online/products/post-install.html"><strong aria-hidden="true">1.6.4.</strong> 安装后配置</a></li><li class="chapter-item expanded "><a href="online/products/post-install-optional.html"><strong aria-hidden="true">1.6.5.</strong> 安装后可选配置</a></li></ol></li><li class="chapter-item expanded "><a href="online/correctness-checking.html"><strong aria-hidden="true">1.7.</strong> 正确性检查</a></li><li class="chapter-item expanded "><a href="online/registry/harbor.html"><strong aria-hidden="true">1.8.</strong> 安装 Harbor Registry</a></li><li class="chapter-item expanded "><a href="online/storage-service/index.html"><strong aria-hidden="true">1.9.</strong> 安装存储服务</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="online/storage-service/minio.html"><strong aria-hidden="true">1.9.1.</strong> MinIO</a></li><li class="chapter-item expanded "><a href="online/storage-service/nfs.html"><strong aria-hidden="true">1.9.2.</strong> NFS 和 StorageClass</a></li><li class="chapter-item expanded "><a href="online/storage-service/ceph.html"><strong aria-hidden="true">1.9.3.</strong> Ceph</a></li><li class="chapter-item expanded "><a href="online/storage-service/lustre.html"><strong aria-hidden="true">1.9.4.</strong> Lustre</a></li><li class="chapter-item expanded "><a href="online/storage-service/gpfs.html"><strong aria-hidden="true">1.9.5.</strong> GPFS</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="offline/index.html"><strong aria-hidden="true">2.</strong> 离线安装</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="offline/prepare-offline-packages/index.html"><strong aria-hidden="true">2.1.</strong> 准备离线安装包</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="offline/prepare-offline-packages/kubespray.html"><strong aria-hidden="true">2.1.1.</strong> Kubespray</a></li><li class="chapter-item expanded "><a href="offline/prepare-offline-packages/k8s-components.html"><strong aria-hidden="true">2.1.2.</strong> K8s 组件</a></li><li class="chapter-item expanded "><a href="offline/prepare-offline-packages/products.html"><strong aria-hidden="true">2.1.3.</strong> 产品</a></li></ol></li><li class="chapter-item expanded "><a href="offline/install/index.html"><strong aria-hidden="true">2.2.</strong> 安装</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="offline/install/k8s.html"><strong aria-hidden="true">2.2.1.</strong> K8s</a></li><li class="chapter-item expanded "><a href="offline/install/k8s-components.html"><strong aria-hidden="true">2.2.2.</strong> K8s 组件</a></li><li class="chapter-item expanded "><a href="offline/install/products.html"><strong aria-hidden="true">2.2.3.</strong> 产品</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="update/index.html"><strong aria-hidden="true">3.</strong> 产品升级</a></li><li class="chapter-item expanded "><a href="appendix/index.html"><strong aria-hidden="true">4.</strong> 附录</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="appendix/install-docker.html"><strong aria-hidden="true">4.1.</strong> 在线安装 Docker</a></li><li class="chapter-item expanded "><a href="appendix/install-docker-compose.html"><strong aria-hidden="true">4.2.</strong> 在线安装 Docker Compose</a></li><li class="chapter-item expanded "><a href="appendix/configure-docker-insecure-registry.html"><strong aria-hidden="true">4.3.</strong> 配置 Docker Insecure Registry</a></li><li class="chapter-item expanded "><a href="appendix/install-s3cmd.html"><strong aria-hidden="true">4.4.</strong> 在线安装 s3cmd</a></li><li class="chapter-item expanded "><a href="appendix/k8s-install-notes.html"><strong aria-hidden="true">4.5.</strong> 安装 K8s 注释</a></li><li class="chapter-item expanded "><a href="appendix/generate-k8s-file-and-image-list.html"><strong aria-hidden="true">4.6.</strong> 生成 K8s 文件和镜像列表</a></li><li class="chapter-item expanded "><a href="appendix/generate-t9k-product-image-list.html"><strong aria-hidden="true">4.7.</strong> 生成 T9k 产品镜像列表</a></li><li class="chapter-item expanded "><a href="appendix/modify-helm-chart.html"><strong aria-hidden="true">4.8.</strong> Helm Chart 修改</a></li><li class="chapter-item expanded "><a href="appendix/manually-install-mlnx-ofed-driver.html"><strong aria-hidden="true">4.9.</strong> 手动安装 MLNX_OFED 驱动</a></li><li class="chapter-item expanded "><a href="appendix/ansible-vars.html"><strong aria-hidden="true">4.10.</strong> ansible vars</a></li><li class="chapter-item expanded "><a href="appendix/ansible-debugging.html"><strong aria-hidden="true">4.11.</strong> ansible debugging</a></li><li class="chapter-item expanded "><a href="appendix/manage-domain-certificate.html"><strong aria-hidden="true">4.12.</strong> 管理域名证书</a></li><li class="chapter-item expanded "><a href="appendix/container-runtime-cli.html"><strong aria-hidden="true">4.13.</strong> CRI 命令行工具</a></li><li class="chapter-item expanded "><a href="appendix/cluster-admin-installation-configuration.html"><strong aria-hidden="true">4.14.</strong> 集群管理安装配置</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">TensorStack AI 计算平台 - 安装手册</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="概述"><a class="header" href="#概述">概述</a></h1>
<p>“TensorStack AI 计算平台” 是面向 AI 集群的系统软件，针对 AI 集群的硬件架构和 AI 领域的计算任务的特性和需求，提供稳定、可扩展的 AI 技术平台能力，服务 AI 技术研发和规模化落地。</p>
<p>AI 集群的拥有者可以使用这套软件，构建自己的 “AI 私有云” 或 “AI 混合云” 基础设施服务。</p>
<aside class="note info">
<div class="title">“TensorStack AI 计算平台” 的功能</div>
<p>作为基础设施软件，平台提供两方面的能力：</p>
<ol>
<li><strong>计算服务</strong>：针对 AI 集群使用者，向上支持各种 AI 计算场景，例如：云端开发环境、模型训练、部署推理服务、应用开发等；</li>
<li><strong>集群管理</strong>：针对 AI 集群的管理、运维人员，提供方便的机制，实施各种资源、安全、数据等管理策略。</li>
</ol>
</aside>
<p>通过先进的架构和丰富的 API + 系统服务，“TensorStack AI 计算平台” 合理地隐藏了分布式并行、异构计算、加速计算等技术的复杂细节，提高了抽象层次，并为 AI 领域的各种计算提供了针对性的支持，极大地提升了 AI 技术研究、开发、应用的工作效率。</p>
<figure class="architecture">
  <img alt="t9k-arch" src="./assets/overview/t9k-arch.png" />
  <figcaption>图 1：TensorStack AI 计算平台为 AI 集群提供先进的 AI 基础设施能力。APIs 层提供了可扩展、可编程、云原生的系统服务；Apps 层为多样化的应用场景提供全面、完善的支持：用户可根据需求，安装各种 Apps（IDE、LLM 开发框架、并行训练管理、推理服务管理、资源管理工具、完整的 AI 应用等），满足 AI 研究、开发和应用等业务需求。</figcaption>
</figure>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="在线安装"><a class="header" href="#在线安装">在线安装</a></h1>
<h2 id="概述-1"><a class="header" href="#概述-1">概述</a></h2>
<p>如果可以访问 Internet，并可以方便地从 Docker Hub 拉取镜像，下载 Linux 的 packages 等，可采用在线安装的方式。</p>
<p>反之，如果网络访问受限，则可采用<a href="online/../offline/index.html">离线安装</a>模式。</p>
<p>安装过程中，我们将主要使用 ansible 安装 Kubernetes 集群和 OS 系统组件，使用 helm 安装 T9k 产品，步骤如下：</p>
<ol>
<li><a href="online/./inventory/index.html">设置 ansible inventory</a>；</li>
<li><a href="online/./prepare-nodes.html">准备节点</a>；</li>
<li><a href="online/./k8s-index.html">安装基础 K8s</a>；</li>
<li><a href="online/./k8s-components/index.html">安装 K8s 的一些扩展组件</a>，例如 Istio、Knative 等；</li>
<li><a href="online/../hardware/hardware.html">安装特定的硬件支持</a>，例如 NVIDIA GPU 等；</li>
<li><a href="online/./products/index.html">安装 TensorStack AI 计算平台</a>；</li>
<li><a href="online/./correctness-checking.html">检查安装的正确性</a>。</li>
</ol>
<p>另外可选的步骤包括：</p>
<ul>
<li><a href="online/./registry/harbor.html">安装 Harbor Registry</a>。</li>
<li><a href="online/./storage-service/index.html">安装存储服务</a>，例如 MinIO、Ceph 等。</li>
</ul>
<figure class="architecture">
  <img alt="process" src="online/../assets/online/process.drawio.svg" />
  <figcaption>图 1：在线安装流程。图左侧从上到下分别为准备步骤、核心步骤和检查步骤；图右侧为可选步骤，应根据需求选择性地执行。</figcaption>
</figure>
<h2 id="下一步"><a class="header" href="#下一步">下一步</a></h2>
<p>我们首先准备工具和环境：<a href="online/./inventory/index.html">设置 ansible inventory</a>。</p>
<h2 id="参考"><a class="header" href="#参考">参考</a></h2>
<ul>
<li><a href="https://www.ansible.com/">https://www.ansible.com/</a></li>
<li><a href="https://helm.sh/">https://helm.sh/</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="设置-ansible-inventory"><a class="header" href="#设置-ansible-inventory">设置 ansible inventory</a></h1>
<p>我们使用 <a target="_blank" rel="noopener noreferrer" href="https://docs.ansible.com/">ansible</a> 安装 K8s 及各种辅助组件，因此，我们需要准备一台电脑作为 <a target="_blank" rel="noopener noreferrer" href="https://docs.ansible.com/ansible/latest/network/getting_started/basic_concepts.html">ansible 控制节点</a>，以运行 ansible 命令，并在这个控制节点上，准备 ansible 的 <a target="_blank" rel="noopener noreferrer" href="https://docs.ansible.com/ansible/latest/inventory_guide/intro_inventory.html">inventory</a>。</p>
<figure class="architecture">
  <img alt="ansible" src="online/inventory/../../assets/online/inventory.drawio.svg" width="80%" />
  <figcaption>图 1：使用 ansible 系统脚本化安装和配置 K8s 集群及其组件。</figcaption>
</figure>
<h2 id="下一步-1"><a class="header" href="#下一步-1">下一步</a></h2>
<p>我们可以开始 inventory 的 <a href="online/inventory/./basic-settings.html">基本设置</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="基本设置"><a class="header" href="#基本设置">基本设置</a></h1>
<h2 id="目的"><a class="header" href="#目的">目的</a></h2>
<ol>
<li>准备好使用 ansible 的环境；</li>
<li>确认目标集群服务器可通过 ansible 访问。</li>
</ol>
<h2 id="前提条件"><a class="header" href="#前提条件">前提条件</a></h2>
<p>可通过网络访问集群的服务器，并具备适当的（root 或者 sudo）访问凭证。</p>
<h2 id="准备环境"><a class="header" href="#准备环境">准备环境</a></h2>
<p>首先，在 ansible 控制节点上设置环境，按照如下步骤执行：</p>
<ol>
<li>克隆相关的 git repos</li>
<li>安装 ansible</li>
<li>复制 inventory 模版</li>
</ol>
<h3 id="克隆-repos"><a class="header" href="#克隆-repos">克隆 repos</a></h3>
<pre><code class="language-bash"># create directory and clone repos
mkdir -p ~/ansible
cd ~/ansible

git clone git@github.com:t9k/ks-clusters.git
git clone git@github.com:t9k/kubespray.git
</code></pre>
<p>将 kubespray 切换到合适分支：</p>
<pre><code class="language-bash"># 将 kubespray 切换到合适分支，例如 kubernetes-1.25.9
cd kubespray
git checkout -b kubernetes-&lt;version&gt; origin/kubernetes-&lt;version&gt;
</code></pre>
<h3 id="安装-ansible"><a class="header" href="#安装-ansible">安装 ansible</a></h3>
<p>使用 conda 管理 python 环境：</p>
<pre><code class="language-bash">conda create -n kubespray python=3.10
conda activate kubespray
</code></pre>
<p>安装 ansible：</p>
<pre><code class="language-bash"># Use -i https://pypi.tuna.tsinghua.edu.cn/simple or other pypi index may help with slow connections.
python -m pip install -r kubespray/requirements.txt
</code></pre>
<p>如果无 Internet 链接，可使用本地 python package：</p>
<pre><code class="language-bash"># use offline package directory
python -m pip install --no-index \
    --find-links=&lt;python-packages-folder&gt; -r kubespray/requirements.txt
</code></pre>
<p>确认 ansible 安装成功：</p>
<pre><code class="language-bash">ansible --version
</code></pre>
<h3 id="复制-inventory-模版"><a class="header" href="#复制-inventory-模版">复制 inventory 模版</a></h3>
<p>集群的所有配置等存放在环境变量 <code>T9K_CLUSTER</code> 指向的子目录中：</p>
<pre><code class="language-bash"># 注意：当使用合适的集群名字
T9K_CLUSTER=demo

# 创建目录
mkdir -p ~/ansible/$T9K_CLUSTER &amp;&amp; cd ~/ansible/$T9K_CLUSTER
</code></pre>
<p>另外，推荐使用 git 对此 inventory 进行版本管理：</p>
<pre><code class="language-bash"># recommended: version this folder
git init .
</code></pre>
<p>复制模版文件：</p>
<pre><code class="language-bash">cd ~/ansible/$T9K_CLUSTER

# for some default configs
cp ../ks-clusters/inventory/ansible.cfg .

# copy a sample inventory to current directory
cp -r ../ks-clusters/inventory/sample-&lt;variant&gt; inventory
</code></pre>
<blockquote>
<p>此步骤复制的 <code>inventory</code> 子目录里的内容详情见：<a href="online/inventory/./inventory-advanced.html#inventory-%E7%BB%93%E6%9E%84">inventory 结构</a>。</p>
</blockquote>
<h2 id="设置-inventory"><a class="header" href="#设置-inventory">设置 inventory</a></h2>
<aside class="note">
<div class="title">注意</div>
<p>此部分需要根据集群的实际情况进行填写，例如服务器节点等。</p>
</aside>
<p>在 <code>inventory.ini</code> 中填入服务器信息（参考：<a href="online/inventory/./inventory-advanced.html#%E8%8A%82%E7%82%B9%E7%BB%84">节点组</a>）：</p>
<pre><code class="language-bash">cd ~/ansible/$T9K_CLUSTER

vim inventory/inventory.ini
</code></pre>
<p>确认 inventory 设置正常：</p>
<pre><code class="language-bash"># 确认 server 列表
ansible-inventory -i inventory/inventory.ini --list

# 测试可访问
ansible all -m ping -i inventory/inventory.ini
</code></pre>
<p>有关 inventory 更多的设置，请参考 <a href="online/inventory/./inventory-advanced.html">inventory 高级设置</a>。</p>
<h2 id="下一步-2"><a class="header" href="#下一步-2">下一步</a></h2>
<p>准备好 ansible inventory 之后，即可进行下一步的 <a href="online/inventory/../prepare-nodes.html">准备节点</a> 工作。</p>
<h2 id="参考-1"><a class="header" href="#参考-1">参考</a></h2>
<p><a href="https://docs.ansible.com/">https://docs.ansible.com/</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="高级设置"><a class="header" href="#高级设置">高级设置</a></h1>
<h2 id="更多设置"><a class="header" href="#更多设置">更多设置</a></h2>
<h3 id="使用-jump-host"><a class="header" href="#使用-jump-host">使用 jump host</a></h3>
<p>如果需要通过跳板机来连接到 K8s 集群的节点，则使用以下参数来运行 ansible 命令：</p>
<pre><code class="language-bash">ansible all -m ping -i inventory/inventory.ini \
  -e 'ansible_ssh_common_args=&quot;-o ProxyCommand=\&quot;ssh -q -W %h:%p &lt;user&gt;@&lt;bastion-host&gt;\&quot;&quot;'
</code></pre>
<p>通过 ansible-playbook 命令运行 Kubespray 的脚本时，你可以通过 <a href="online/inventory/advanced-settings.html#%E9%85%8D%E7%BD%AE-bastion-host">配置 bastion host</a> 来省略这里的参数。</p>
<h3 id="配置-bastion-host"><a class="header" href="#配置-bastion-host">配置 bastion host</a></h3>
<p>Kubespray 的脚本支持使用 Bastion Host，可在 inventory 中设置 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible/ansible.md#bastion-host">Bastion host</a>。</p>
<p>修改 <code>inventory/inventory.ini</code> 文件，设置以下内容：</p>
<pre><code class="language-ini">## configure a bastion host if your nodes are not directly reachable
[bastion]
bastion ansible_host=&lt;x.x.x.x&gt; ansible_user=&lt;some_user&gt;
</code></pre>
<p>然后照常执行安装流程即可。</p>
<h3 id="使用-ansible-vault"><a class="header" href="#使用-ansible-vault">使用 ansible vault</a></h3>
<p>如果需要保存敏感信息，例如 ansible become password，可以使用本章的方式进行配置。</p>
<p>首先创建一个文件保存 ansible vault 的密码：</p>
<aside class="note warning">
<div class="title">警告</div>
<p><code>vault-password.txt</code> 文件应当设置私有的可读权限，以避免泄漏 vault 的密码。该文件也可以使用更加安全的方式，例如 MacOS 的 <a target="_blank" rel="noopener noreferrer" href="https://support.apple.com/guide/keychain-access/what-is-keychain-access-kyca1083/mac">Keychain Access</a> 来保存。</p>
</aside>
<pre><code class="language-bash"># 从 stdin 读入 vault 密码，并保存
cat &gt; ~/ansible/.vault-password.txt

chmod 600 ~/ansible/.vault-password.txt
</code></pre>
<p>创建 <code>vault.yml</code> 以保存变量的值：</p>
<aside class="note warning">
<div class="title">警告</div>
<p>虽然 <code>vault.yml</code> 被加密，但不要放在公开的代码仓库中，以防止暴力破解等风险。</p>
</aside>
<pre><code class="language-bash"># 创建一个文件夹来保存 vault.yml
mkdir -p ~/ansible/$T9K_CLUSTER &amp;&amp; cd ~/ansible/$T9K_CLUSTER

grep ^vault.yml .gitignore || echo vault.yml &gt;&gt; .gitignore

# 使用上面创建的 vault password 加密
ansible-vault create ~/ansible/$T9K_CLUSTER/vault.yml \
  --vault-password-file ~/ansible/.vault-password.txt
</code></pre>
<p>ansible-vault 命令会打开编辑器，在其中输入：</p>
<pre><code>ansible_become_password: &lt;your-become-password&gt;
</code></pre>
<p>保存之后，ansible-vault 会加密 <code>vault.yml</code>，可通过以下命令编辑文件：</p>
<pre><code class="language-bash">ansible-vault edit ~/ansible/$T9K_CLUSTER/vault.yml \
  --vault-password-file ~/ansible/.vault-password.txt
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>如需要给特定 node 设置单独的 <code>ansible_become_pass</code>，可在 inventory 中单独设置其 var ：</p>
<pre><code class="language-ini">[all]
node1 ansible_become_pass=&quot;{{ node1.vault_ansible_become_pass }}&quot;
node2 ansible_become_pass=&quot;{{ node2.vault_ansible_become_pass }}&quot;
</code></pre>
<p>在 ansible-vault 打开的编辑器中设定 var 的值：</p>
<pre><code class="language-yaml">node1:
  vault_ansible_become_pass: &lt;become-password-for-node1&gt;
node2:
  vault_ansible_become_pass: &lt;become-password-for-node2&gt;
</code></pre>
</aside>
<p>运行脚本时，使用 ansible vault 中保存的 ansible become password 时的安装命令为：</p>
<pre><code class="language-bash">ansible-playbook ../kubespray/cluster.yml \
  -i inventory/inventory.ini \
  --become \
  -e &quot;@~/ansible/$T9K_CLUSTER/vault.yml&quot; \
  --vault-password-file=~/ansible/.vault-password.txt
</code></pre>
<h3 id="复制-ssh-公钥"><a class="header" href="#复制-ssh-公钥">复制 SSH 公钥</a></h3>
<p>我们可以一次性地通过 ansible 命令将控制节点的 SSH key 复制到所有受控节点上，以方便之后直接使用 SSH key 进行身份验证。</p>
<p>根据<a href="https://stackoverflow.com/questions/42835626/ansible-to-use-the-ssh-connection-type-with-passwords-you-must-install-the-s">讨论</a>，控制节点需要安装 sshpass，以支持 ansible 使用 Password Authentication：</p>
<pre><code class="language-bash"># for macOS
brew install esolitos/ipa/sshpass

# for ubuntu
sudo apt install sshpass -y
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>使用默认配置的 SSH Server 支持 Password Authentication 功能。如果修改过 SSH Server 的配置，请确认 SSH Server 设置中不包含 &quot;PasswordAuthentication no&quot;。</p>
</aside>
<p>使用 Password Authentication 的 ad-hoc 命令，将本机的 SSH 公钥复制到所有节点 (group all) 上：</p>
<pre><code class="language-bash">ansible all \
  -i inventory/inventory.ini \
  -m authorized_key \
  -a &quot;user=&lt;user&gt; key={{ lookup('file', '~/.ssh/id_rsa.pub') }}&quot; \
  --ask-pass 
</code></pre>
<p>验证：</p>
<pre><code class="language-bash">ansible -i inventory/inventory.ini -m ping all
</code></pre>
<h2 id="inventory-结构"><a class="header" href="#inventory-结构">inventory 结构</a></h2>
<h3 id="节点组"><a class="header" href="#节点组">节点组</a></h3>
<p><code>inventory.ini</code> 文件中定义了多个节点组，解释如下。</p>
<p><strong>Kubespray 节点组</strong></p>
<p>Kubespray playbooks 使用 <code>inventory.ini</code> 中的如下分组：</p>
<ul>
<li><code>all</code> - 集群所有节点，可在此设置 <code>ansible_host</code>, <code>ansible_user</code> 等额外信息；</li>
<li><code>kube_control_plane</code> - 集群控制平面的节点；</li>
<li><code>etcd</code> - etcd 服务的节点；</li>
<li><code>kube_node</code> - K8s 集群的工作节点；</li>
<li><code>ingress-node</code> - 运行 Ingress controller 的节点，</li>
<li><code>bastion</code> - 指定 bastion 节点，以支持访问无法直接访问的节点。</li>
</ul>
<aside class="note info">
<div class="title">节点分组</div>
<ul>
<li>如果一个节点仅在 <code>kube_node</code> 中，它会作为工作节点加入 K8s 集群；</li>
<li>如果一个节点仅在 <code>kube_control_plane</code> 中，它会作为 control plane 节点加入 K8s 集群，并添加 <code>node-role.kubernetes.io/control-plane:NoSchedule</code> 的 Taint；</li>
<li>如果一个节点同时在 <code>kube_node</code> 和 <code>kube_control_plane</code> 中，它会作为 control plane 节点加入 K8s 集群，但不添加 <code>node-role.kubernetes.io/control-plane:NoSchedule</code> 的 Taint。</li>
</ul>
</aside>
<p><strong>其他分组</strong></p>
<p>下列分组被非 Kubespray playbooks 使用：</p>
<ul>
<li><code>chronyserver</code> -  使用 <code>t9k-playbooks/2-sync-time.yml</code> 安装 chrony 时，此 group 指定 chrony server 节点；</li>
<li><code>chronyclients</code> - 同上，安装 chrony 时，此 group 指定 chrony client 节点；</li>
<li><code>nfs_server</code> -  使用 <code>t9k-playbooks/10-install-nfs.yml</code> 安装 nfs 作为集群存储时，需要用这个 group 指定 nfs server 节点（nfs server 只使用一个节点，即此 group 中的第一个节点）。</li>
</ul>
<h3 id="目录结构"><a class="header" href="#目录结构">目录结构</a></h3>
<p>查看文件树：</p>
<pre><code class="language-bash"># edit this inventory to suit your needs
tree inventory/
</code></pre>
<p>输出：</p>
<pre><code>inventory/
├── group_vars
│   ├── all
│   │   ├── all.yml
│   │   ├── docker.yml
│   │   ├── download.yml
│   │   └── etcd.yml
│   └── k8s_cluster
│       ├── addons.yml
│       └── k8s-cluster.yml
├── inventory.ini
└── patches
    ├── kube-controller-manager+merge.yaml
    └── kube-scheduler+merge.yaml
</code></pre>
<h3 id="设置的-variables"><a class="header" href="#设置的-variables">设置的 variables</a></h3>
<p>进一步的查看设置的 variables：</p>
<pre><code class="language-bash"># review variables
grep -Ev &quot;^$|^\s*#&quot; inventory/group_vars/all/all.yml
grep -Ev &quot;^$|^\s*#&quot; inventory/group_vars/all/docker.yml
grep -Ev &quot;^$|^\s*#&quot; inventory/group_vars/all/download.yml
grep -Ev &quot;^$|^\s*#&quot; inventory/group_vars/all/etcd.yml

grep -Ev &quot;^$|^\s*#&quot; inventory/group_vars/k8s_cluster/addons.yml
grep -Ev &quot;^$|^\s*#&quot; inventory/group_vars/k8s_cluster/k8s-cluster.yml
</code></pre>
<h2 id="参考-2"><a class="header" href="#参考-2">参考</a></h2>
<p><a href="https://docs.ansible.com/">https://docs.ansible.com/</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="准备节点"><a class="header" href="#准备节点">准备节点</a></h1>
<h2 id="目的-1"><a class="header" href="#目的-1">目的</a></h2>
<ol>
<li>使用 ansible 对集群服务器做基本的配置;</li>
<li>获取并审查节点基本信息，以保证接下来的安装工作能够正确执行。</li>
</ol>
<h2 id="前提条件-1"><a class="header" href="#前提条件-1">前提条件</a></h2>
<p>完成<a href="online/./inventory/index.html">设置 ansible inventory</a> 中的工作。</p>
<p>确认 inventory 可用：</p>
<pre><code class="language-bash"># 进入为此次安装准备的 inventory 目录
cd ~/ansible/$T9K_CLUSTER 

# 确认 server 列表
ansible-inventory -i inventory/inventory.ini --list

# 测试可访问
ansible all -m ping -i inventory/inventory.ini
</code></pre>
<h2 id="获取节点信息"><a class="header" href="#获取节点信息">获取节点信息</a></h2>
<p>运行脚本：</p>
<pre><code class="language-bash">ansible-playbook ../ks-clusters/t9k-playbooks/0-gather-information.yml \
  -i inventory/inventory.ini \
  --become -K
</code></pre>
<p>运行结束后，可在 ansible 控制节点中查看保存的信息：</p>
<pre><code class="language-bash">ls /tmp/facts/
</code></pre>
<p>特别地，确认 GPU、网络设备等信息是否符合预期。</p>
<h2 id="禁用-ubuntu-自动更新"><a class="header" href="#禁用-ubuntu-自动更新">禁用 Ubuntu 自动更新</a></h2>
<aside class="note">
<div class="title">注意</div>
<p>如果为临时测试目的，可跳过这一步。</p>
</aside>
<p>目前支持在 Ubuntu 20.04 或 22.04 上安装 K8s，其他版本将会适时提供支持。</p>
<aside class="note warning">
<div class="title">警告</div>
<p>该脚本中包含重启节点的操作。</p>
</aside>
<p>运行脚本：</p>
<pre><code class="language-bash">ansible-playbook ../ks-clusters/t9k-playbooks/1-disable-auto-upgrade.yml \
  -i inventory/inventory.ini \
  --become -K
</code></pre>
<h2 id="设置时钟同步"><a class="header" href="#设置时钟同步">设置时钟同步</a></h2>
<aside class="note">
<div class="title">注意</div>
<p>如果为临时测试目的，可跳过这一步。</p>
</aside>
<p>运行脚本：</p>
<pre><code class="language-bash">ansible-playbook ../ks-clusters/t9k-playbooks/2-sync-time.yml \
  -i inventory/inventory.ini \
  --become -K \
  -e &lt;chrony_server_ip&gt; \
  -e chrony_client_ip_range=&lt;chrony_client_ip_range_1&gt;,&lt;chrony_client_ip_range_2&gt;
</code></pre>
<p>其中的变量说明如下：</p>
<ol>
<li><code>chrony_server_ip</code>：运行 chrony server 节点的 IP 地址。</li>
<li><code>chrony_client_ip_range_1</code>：chrony 生效的 IP 地址网段，例如 <code>1.2.3.4/24</code>，可设置一个或多个网段，使用逗号分割。</li>
</ol>
<p>也可以直接在 YAML 中设置变量（在 ks-clusters/t9k-playbooks/group_vars/all/all.yml 中）：</p>
<pre><code class="language-bash">chrony_server_ip: &lt;chrony_server_ip&gt; # 1.2.3.4
chrony_client_ip_range:
- &lt;chrony_client_ip_range_1&gt; # 1.2.3.4/24
- &lt;chrony_client_ip_range_2&gt; # 100.0.0.1/8
</code></pre>
<h2 id="下一步-3"><a class="header" href="#下一步-3">下一步</a></h2>
<p>准备好节点之后，我们可进行 <a href="online/./k8s-index.html">安装 K8s</a> 的工作。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装-k8s"><a class="header" href="#安装-k8s">安装 K8s</a></h1>
<p>通过运行 ansible 脚本安装 K8s，步骤如下：</p>
<ol>
<li><a href="online/./k8s-install.html">基本安装</a>，完成一个最基本的 K8s 安装；</li>
<li><a href="online/./k8s-storage.html">设置集群存储</a>，为 K8s 集群设置存储服务，配置 StorageClass。</li>
</ol>
<p>本节其他部分可选，例如增加/减少集群节点等。</p>
<h2 id="下一步-4"><a class="header" href="#下一步-4">下一步</a></h2>
<p>完成 K8s 安装之后，需要进行 <a href="online/./k8s-components/index.html">安装 K8s 组件</a> 的步骤，以在 K8s 安装其他必须的服务。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装-k8s-1"><a class="header" href="#安装-k8s-1">安装 K8s</a></h1>
<h2 id="目的-2"><a class="header" href="#目的-2">目的</a></h2>
<p>完成一个最基本的 K8s 集群安装。</p>
<h2 id="前提条件-2"><a class="header" href="#前提条件-2">前提条件</a></h2>
<p>准备好了 inventory 并且服务器节点满足要求，可按照前述 <a href="online/./inventory/index.html">准备 ansible inventory</a> 和 <a href="online/./prepare-nodes.html">准备节点</a> 步骤执行。</p>
<h2 id="配置"><a class="header" href="#配置">配置</a></h2>
<p>本章描述如何设置 K8s 安装使用的版本, 容器运行时, CNI 插件, Ingress, LoadBalancer 等选项。你可以通过修改 inventory 中的变量来配置上述选项。这些变量位于 inventory 目录中 <code>inventory/group_vars/</code>。</p>
<h3 id="配置总览"><a class="header" href="#配置总览">配置总览</a></h3>
<blockquote>
<p>详细说明请参考 Kubespray 文档 <a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible/vars.md">Configurable Parameters in Kubespray</a>。下面仅列出较为重要的一些设置。</p>
</blockquote>
<h4 id="k8s-clusteryml"><a class="header" href="#k8s-clusteryml">k8s-cluster.yml</a></h4>
<ul>
<li><code>kube_version</code>: 设置 K8s 版本。</li>
<li><code>container_manager</code>: 设置容器运行时。</li>
<li><code>kube_network_plugin</code>: 设置 CNI 插件。</li>
<li><code>kube_proxy_mode</code>: 设置 Kube proxy 代理模式。</li>
<li><code>kube_service_addresses</code>: 分配给 service 的 IP 地址范围。</li>
<li><code>kube_pods_subnet</code>: 分配给 Pod 的 IP 地址范围。</li>
<li><code>kube_vip_enabled</code>: 启用 kube-vip，详见 <a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ingress/kube-vip.md">kube-vip</a>。</li>
<li><code>loadbalancer_apiserver</code>: 设置 apiserver 的负载均衡器，详见 <a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/ha-mode.md">HA endpoints for K8s</a>。</li>
<li><code>kubernetes_audit</code>: 控制是否启用 kubernetes audit。</li>
</ul>
<h4 id="addonsyml"><a class="header" href="#addonsyml">addons.yml</a></h4>
<ul>
<li><code>ingress_nginx_enabled</code>: 启用 Ingress NGINX 功能（在安装 K8s 集群后自动安装 Ingress NGINX 作为 ingress 控制器）。</li>
<li><code>helm_enabled</code>: 在 K8s 控制节点安装 helm 命令行工具。</li>
<li><code>metrics_server_enabled</code>: 启用 Metrics Server 功能。</li>
</ul>
<h4 id="allyml"><a class="header" href="#allyml">all.yml</a></h4>
<ul>
<li><code>upstream_dns_servers</code>: 设置集群使用的上游 DNS 服务器，建议与当前环境中的 DNS 配置一致。</li>
</ul>
<h3 id="设置-kubernetes-审计"><a class="header" href="#设置-kubernetes-审计">设置 Kubernetes 审计</a></h3>
<p>如果你想启用 <a href="online/./products/pre-install/t9k-monitoring.html#%E5%90%AF%E7%94%A8-t9k-%E5%AE%A1%E8%AE%A1%E6%97%A5%E5%BF%97">T9k 审计日志</a>，请确保 <code>k8s-cluster.yaml</code> 文件中包含下列内容：</p>
<aside class="note">
<div class="title">注意</div>
<p>不建议你自行修改下列的 audit_policy_custom_rules 字段，以避免影响 T9k 审计日志的功能。</p>
</aside>
<pre><code class="language-yaml"># audit log for kubernetes
kubernetes_audit: true
audit_log_path: /var/log/audit/kube-apiserver-audit.log
# num days
audit_log_maxage: 30
# the num of audit logs to retain
audit_log_maxbackups: 10
# the max size in MB to retain
audit_log_maxsize: 100
# policy file
audit_policy_file: &quot;{{ kube_config_dir }}/audit-policy/apiserver-audit-policy.yaml&quot;
# custom audit policy rules (to replace the default ones)
audit_policy_custom_rules: |
  # ProxyOperation in t9k-system namespace by anybody or serviceaccounts
  - level: Request
    namespaces: [&quot;t9k-system&quot;]
    verbs: [&quot;create&quot;]
    resources:
    - group: &quot;tensorstack.dev&quot;
      resources: [&quot;proxyoperations&quot;]
  # ignore object variation caused by serviceaccounts
  - level: None
    userGroups:
    - system:serviceaccounts
  # ignore audting events caused by k8s system components
  - level: None
    userGroups:
    - system:nodes
  # ignore audting events caused by k8s system components
  - level: None
    users:
    - system:kube-controller-manager
    - system:kube-scheduler
    - system:apiserver
  # ignore events &amp; accessreview
  - level: None
    resources:
    - group: &quot;&quot;
      resources: [&quot;events&quot;]
    - group: &quot;authorization.k8s.io&quot;
      resources: [&quot;localsubjectaccessreviews&quot;,&quot;selfsubjectaccessreviews&quot;,&quot;selfsubjectrulesreviews&quot;,&quot;subjectaccessreviews&quot;]
  # namespace-scoped objects in system namespaces
  - level: RequestResponse
    verbs: [&quot;create&quot;,&quot;update&quot;,&quot;patch&quot;,&quot;delete&quot;]
    namespaces: [&quot;t9k-system&quot;,&quot;t9k-monitoring&quot;,&quot;ingress-nginx&quot;,&quot;istio-system&quot;,&quot;knative-serving&quot;,&quot;kube-system&quot;,&quot;t9k-syspub&quot;]
  # cluster-wide objects
  - level: RequestResponse
    verbs: [&quot;create&quot;,&quot;update&quot;,&quot;patch&quot;,&quot;delete&quot;]
    namespaces:
    - &quot;&quot;
  # ignore others
  - level: None
</code></pre>
<h3 id="设置代理"><a class="header" href="#设置代理">设置代理</a></h3>
<p>如果你需要为包管理工具、容器运行时设置代理。</p>
<p>在 <code>all.yml</code> 中设置如下参数即可：</p>
<pre><code class="language-yaml">http_proxy: &quot;&lt;proxy-server&gt;&quot;
https_proxy: &quot;&lt;proxy-server&gt;&quot;
https_proxy_cert_file: &quot;&quot;
no_proxy: &quot;127.0.0.0/8,localhost,192.168.0.0/16,...&quot;
</code></pre>
<ul>
<li><code>http_proxy</code>：设置 HTTP 代理的 URL，例如 <code>http://proxy.example.com:8080</code>。</li>
<li><code>https_proxy</code>：设置 HTTPS 代理的 URL，例如 <code>https://proxy.example.com:8080</code>。</li>
<li><code>https_proxy_cert_file</code>：如果需要自定义 CA 证书来访问代理，请设置证书文件的路径。例如 <code>/path/to/ca.crt</code>。</li>
<li><code>no_proxy</code>：设置不使用代理的地址列表，地址之间使用逗号分隔。例如 <code>127.0.0.0/8,localhost,192.168.0.0/16,.example.com</code>。</li>
</ul>
<p>如果不需要为包管理工具设置代理，则增加以下设置：</p>
<pre><code class="language-yaml">skip_http_proxy_on_os_packages: true
</code></pre>
<h3 id="设置-cri"><a class="header" href="#设置-cri">设置 CRI</a></h3>
<p>使用不同的容器运行时，请参考 <a href="online/./cri.html">CRI 配置</a>。</p>
<p>如需使用 <a target="_blank" rel="noopener noreferrer" href="https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/">User Namespaces</a>，请参考 <a href="online/./k8s-userns.html">设置 User Namespace</a>。</p>
<h3 id="设置-cni"><a class="header" href="#设置-cni">设置 CNI</a></h3>
<p>使用不同的容器网络实现，请参考 <a href="online/./cni.html">CNI 配置</a>。</p>
<h3 id="设置-ingress"><a class="header" href="#设置-ingress">设置 Ingress</a></h3>
<p>如果你需要安装 Ingress 控制器，Kubespray 目前支持 <a href="https://kubernetes.github.io/ingress-nginx/">NGINX Ingress 控制器</a>。</p>
<p>在 <code>addons.yml</code> 中设置如下参数即可：</p>
<pre><code class="language-yaml">ingress_nginx_enabled: true
</code></pre>
<h3 id="设置-load-balancer"><a class="header" href="#设置-load-balancer">设置 Load Balancer</a></h3>
<p>如果你需要为 K8s API Server 配置一个 Load Balancer，Kubespray 目前支持 <a href="https://kube-vip.io/">kube-vip</a>。</p>
<p>在 <code>k8s-cluster.yml</code> 中设置如下参数即可：</p>
<pre><code class="language-yaml"># Kube-proxy proxyMode configuration.
# Can be ipvs, iptables
kube_proxy_mode: ipvs

# configure arp_ignore and arp_announce to avoid answering ARP queries from kube-ipvs0 interface
# must be set to true for MetalLB, kube-vip(ARP enabled) to work
kube_proxy_strict_arp: true

# reference: https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ingress/kube-vip.md#kube-vip
# Enable kube vip as HA for control-plane, requires a Virtual IP
kube_vip_enabled: true
kube_vip_controlplane_enabled: true
kube_vip_address: &lt;your-virtual-ip-address&gt;
loadbalancer_apiserver:
  address: &quot;{{ kube_vip_address }}&quot;
  port: 6443

# use ARP mode :
kube_vip_arp_enabled: true
</code></pre>
<p>上述各项参数的含义详见 <a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ingress/kube-vip.md">Kubespray 文档</a>，其中：</p>
<ul>
<li>为了使 kube-vip 能够正常工作，当 <code>kube_proxy_mode</code> 为 <code>ipvs</code> 时，<code>kube_proxy_strict_arp</code> 必须为 <code>true</code></li>
<li><code>kube_vip_enabled</code> 为 <code>true</code> 表示启用 kube-vip</li>
<li><code>kube_vip_controlplane_enabled</code> 为 <code>true</code> 表示启用 kube-vip 针对 K8s 控制平面的高可用功能，即分配一个虚拟 IP 地址作为各个 K8s API Server 实例的外部负载均衡器</li>
<li><code>kube_vip_address</code> 表示一个可用的虚拟 IP 地址，kube-vip 将发送 ARP 广播，将发送给该虚拟 IP 地址的请求转发给一个 K8s API Server 实例</li>
<li><code>loadbalancer_apiserver</code> 表示其他服务将通过该虚拟 IP 地址及 6443 端口来访问 K8s API Server</li>
<li><code>kube_vip_arp_enabled</code> 为 <code>true</code> 表示启用 kube-vip 的 ARP 模式</li>
</ul>
<h2 id="安装-k8s-2"><a class="header" href="#安装-k8s-2">安装 K8s</a></h2>
<p>进入为此次安装准备的 inventory 目录：</p>
<pre><code class="language-bash">cd ~/ansible/$T9K_CLUSTER 
</code></pre>
<p>运行 ansible 脚本，以安装 K8s 集群。</p>
<p>方法 1 - 交互式输入 become password：</p>
<pre><code class="language-bash">ansible-playbook ../kubespray/cluster.yml \
  -i inventory/inventory.ini \
  --become -K
</code></pre>
<p>方法 2 - 使用 ansible vault 中保存的 become password：</p>
<pre><code class="language-bash">ansible-playbook ../kubespray/cluster.yml \
    -i inventory/inventory.ini \
    --become \
    -e &quot;@~/ansible/$T9K_CLUSTER/vault.yml&quot; \
    --vault-password-file=~/ansible/.vault-password.txt
</code></pre>
<aside class="note">
<div class="title">参数解释</div>
<pre><code>--become: 使用其他用户运行操作，默认使用 root 用户。
-K: 询问 become 所需的权限升级密码 (become password)。
-e: 设置额外的变量，@说明通过文件传入。
--vault-password-file: 保存了 vault 密码的文件。
</code></pre>
</aside>
<blockquote>
<p>使用 ansible 安装 K8s 过程的更多详情，请参考：<a href="online/../appendix/k8s-install-notes.html#%E8%BF%87%E7%A8%8B%E8%A7%A3%E9%87%8A">安装 K8s 注释 &gt; 过程解释</a></p>
</blockquote>
<h2 id="获取-kubeconfig"><a class="header" href="#获取-kubeconfig">获取 kubeconfig</a></h2>
<p>集群安装成功之后，可获取其 kubeconfig，以开始使用。</p>
<h3 id="从-inventory-获取"><a class="header" href="#从-inventory-获取">从 inventory 获取</a></h3>
<p>如果设置了安装过程中复制 kubeconfig（<code>kubeconfig_localhost: true</code>，文件 <code>group_vars/k8s_cluster/k8s-cluster.yml</code>），可以在 <code>inventory/artifacts</code> 目录中找到 <code>admin.conf</code>（cluster-admin 权限）：</p>
<pre><code class="language-bash">cp inventory/artifacts/admin.conf \
    ~/.kube/example-cluster.conf
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>ks-clusters 的 git repo 里已经配置了 <code>.gitignore</code> 文件以避免 <code>admin.conf</code> 文件被保存到 git repo 中，但仍需要谨慎操作，避免错误地把 <code>admin.conf</code> 放入 git 中，造成安全隐患。</p>
</aside>
<h3 id="从-control-plane-节点获取"><a class="header" href="#从-control-plane-节点获取">从 control-plane 节点获取</a></h3>
<p>无论是否设置了 <code>kubeconfig_localhost</code>，都可以直接从 control-plane 节点获取 kubeconfig。</p>
<p>假设 <code>master01</code> 是一个 control-plane 节点，其 IP 为 <code>100.64.100.11</code>。</p>
<ol>
<li>
<p>复制 kubeconfig 文件</p>
<pre><code class="language-bash">ssh -t master01 'sudo cat /root/.kube/config' |tee ~/.kube/example-cluster.conf
sed -i &quot;1d&quot; $HOME/.kube/example-cluster.conf
</code></pre>
</li>
<li>
<p>替换 kubeconfig 中的 server 地址</p>
<p>如果未配置 HA 模式，则直接使用 control-plane 节点的 IP 地址 + 端口（<code>100.64.100.11:6443</code>）：</p>
<pre><code class="language-bash">sed -i 's|^    server: https://.*|    server: https://100.64.100.11:6443|' \
    ~/.kube/example-cluster.conf
</code></pre>
<p>如果通过 kube-vip 配置了 HA 模式，则应当使用 kube-vip 的虚拟 IP 地址 + 端口，或其他 HA 场景的适当设置。</p>
</li>
</ol>
<h2 id="集群检查"><a class="header" href="#集群检查">集群检查</a></h2>
<p>验证 kubeconfig 可用，并查看集群中的节点信息：</p>
<pre><code class="language-bash">KUBECONFIG=~/.kube/example-cluster.conf kubectl get node
</code></pre>
<h2 id="下一步-5"><a class="header" href="#下一步-5">下一步</a></h2>
<ul>
<li><a href="online/./k8s-storage.html">设置集群存储</a></li>
</ul>
<h2 id="参考-3"><a class="header" href="#参考-3">参考</a></h2>
<ul>
<li><a href="online/../appendix/k8s-install-notes.html">使用 ansible 安装 K8s 过程的注释</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="cri-配置"><a class="header" href="#cri-配置">CRI 配置</a></h1>
<p>本文档说明如何配置 inventory，以使用各种容器运行时来运行 K8s 集群。</p>
<aside class="note">
<div class="title">注意</div>
<p>使用说明：</p>
<ol>
<li>在应用本文档中提供的配置时，请先在 inventory 中查找是否已经存在相应的变量设置。如果存在，进行确认或者修改；如果不存在，需要在 inventory 中添加相应的变量设置。</li>
<li>inventory 的 <code>all/etcd.yml</code> 和 <code>k8s_cluster/k8s-cluster.yml</code> 文件中都包含了 <code>container_manager</code> 变量的设置。对于不加入 K8s 集群中的 etcd 节点，只有前者会生效；对于加入 K8s 集群中的节点，后者的设置优先级更高。通常我们建议同时修改这两个变量。</li>
<li>Kubespray 提供了默认的下载设置，因此你可以在 inventory 中省略版本和下载地址相关的设置。</li>
<li>如果你需要安装自定义版本的容器运行时及其组件，除了配置下载设置外，还需要确认本地 kubespray 的 <a href="https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubespray-defaults/defaults/main/checksums.yml">checksums.yml</a> 文件中已经包含了相应版本的 checksum。如果没有，你需要在 inventory 或者 <code>checksums.yml</code> 中添加相应的 checksum。</li>
</ol>
</aside>
<h2 id="docker"><a class="header" href="#docker">Docker</a></h2>
<p>Docker 不兼容 Kubernetes 创建的容器运行接口（Container Runtime Interface），不过 <a href="https://github.com/Mirantis/cri-dockerd">cri-dockerd</a> 项目填补了 Docker Engine 和 CRI 之间的空白。本章说明如何配置 cri-dockerd 和 Docker 作为 K8s 的容器运行时。</p>
<p>设置 container manager 为 Docker:</p>
<pre><code class="language-yaml">container_manager: docker
</code></pre>
<h3 id="下载设置"><a class="header" href="#下载设置">下载设置</a></h3>
<p>设置 docker 及相关组件的版本：</p>
<pre><code class="language-yaml">docker_version: '20.10'
docker_containerd_version: 1.6.16
cri_dockerd_version: 0.3.4
</code></pre>
<p>设置下载地址：</p>
<pre><code class="language-yaml"># Ubuntu docker-ce repo
docker_ubuntu_repo_base_url: &quot;https://download.docker.com/linux/ubuntu&quot;
docker_ubuntu_repo_gpgkey: 'https://download.docker.com/linux/ubuntu/gpg'
docker_ubuntu_repo_repokey: '9DC858229FC7DD38854AE2D88D81803C0EBFCD88'
# cri-dockerd download url
cri_dockerd_download_url: &quot;{{ files_repo }}/github.com/Mirantis/cri-dockerd/releases/download/v{{ cri_dockerd_version }}/cri-dockerd-{{ cri_dockerd_version }}.{{ image_arch }}.tgz&quot;
</code></pre>
<h3 id="可选设置"><a class="header" href="#可选设置">可选设置</a></h3>
<p>设置 Docker 的存储驱动程序：</p>
<pre><code class="language-yaml">docker_storage_options: -s overlay2
</code></pre>
<p>设置 Registry 镜像站：</p>
<pre><code class="language-yaml">docker_registry_mirrors:
  - https://registry.dockermirror.com
</code></pre>
<p>设置 Docker 的默认 ulimit 值：</p>
<pre><code class="language-yaml">docker_options: &quot;--default-ulimit=memlock=-1:-1 --default-ulimit=stack=67108864:67108864&quot;
</code></pre>
<p>设置 Docker 日志文件的大小和数量：</p>
<pre><code class="language-yaml"># Rotate container stderr/stdout logs at 10m and keep last 5
docker_log_opts: &quot;--log-opt max-size=10m --log-opt max-file=5&quot;
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>Kubespray 默认设置了 <code>kubelet_logfiles_max_size: 10Mi</code>，我们建议将 <code>docker_log_opts</code> 中的 <code>max-size</code> 设置为相同的大小，以避免 <a href="https://github.com/Mirantis/cri-dockerd/issues/35">Failed ReopenContainerLog</a> 的问题。</p>
</aside>
<h2 id="containerd"><a class="header" href="#containerd">containerd</a></h2>
<p>设置 container manager 为 containerd:</p>
<pre><code class="language-yaml">container_manager: containerd
</code></pre>
<h3 id="底层容器运行时"><a class="header" href="#底层容器运行时">底层容器运行时</a></h3>
<p>containerd 具有配置多个底层容器运行时的功能，可以与 Kubernetes 的 <a href="https://kubernetes.io/docs/concepts/containers/runtime-class/">RuntimeClass</a> 功能一起使用。</p>
<p>containerd 使用 runc 作为默认的底层容器运行时，其配置如下：</p>
<pre><code class="language-yaml">containerd_default_runtime: &quot;runc&quot;

containerd_runc_runtime:
  name: runc
  type: &quot;io.containerd.runc.v2&quot;
  engine: &quot;&quot;
  root: &quot;&quot;
  base_runtime_spec: cri-base.json
  options:
    systemdCgroup: &quot;{{ containerd_use_systemd_cgroup | ternary('true', 'false') }}&quot;
    binaryName: &quot;{{ bin_dir }}/runc&quot;
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>不推荐修改 <code>containerd_default_runtime</code>，因为 Kubespray 的相关支持不完善。</p>
</aside>
<p>你可以为 containerd 配置额外的底层容器运行时，例如 kata：</p>
<pre><code class="language-yaml">kata_containers_enabled: true

containerd_additional_runtimes:
 - name: kata
   type: &quot;io.containerd.kata.v2&quot;
   engine: &quot;&quot;
   root: &quot;&quot;
</code></pre>
<p>可用的额外容器运行时种类，及其下载设置请参阅：<a href="online/cri.html#%E9%99%84%E5%BD%95%E9%A2%9D%E5%A4%96%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6%E8%AE%BE%E7%BD%AE">额外容器运行时设置</a>。</p>
<p>如果你想了解更多关于 containerd 底层容器运行时的配置信息，请参考 containerd 的运行时文档 <a href="https://github.com/containerd/containerd/blob/main/docs/cri/config.md#runtime-classes">runtime classes in containerd</a>。</p>
<h3 id="下载设置-1"><a class="header" href="#下载设置-1">下载设置</a></h3>
<p>设置 containerd 及相关组件版本：</p>
<pre><code class="language-yaml">containerd_version: 1.7.13
runc_version: v1.1.12
nerdctl_version: &quot;1.7.1&quot;
</code></pre>
<p>设置下载地址：</p>
<pre><code class="language-yaml">containerd_download_url: &quot;{{ files_repo }}/github.com/containerd/containerd/releases/download/v{{ containerd_version }}/containerd-{{ containerd_version }}-linux-{{ image_arch }}.tar.gz&quot;
nerdctl_download_url: &quot;{{ files_repo }}/github.com/containerd/nerdctl/releases/download/v{{ nerdctl_version }}/nerdctl-{{ nerdctl_version }}-{{ ansible_system | lower }}-{{ image_arch }}.tar.gz&quot;
runc_download_url: &quot;{{ files_repo }}/github.com/opencontainers/runc/releases/download/{{ runc_version }}/runc.{{ image_arch }}&quot;
</code></pre>
<h3 id="可选设置-1"><a class="header" href="#可选设置-1">可选设置</a></h3>
<p>设置 Registry 镜像站：</p>
<pre><code class="language-yaml">containerd_registries_mirrors:
 - prefix: docker.io
   mirrors:
    - host: https://registry.dockermirror.com
      capabilities: [&quot;pull&quot;, &quot;resolve&quot;]
      skip_verify: false
</code></pre>
<h2 id="cri-o"><a class="header" href="#cri-o">CRI-O</a></h2>
<p>CRI-O 是一个适用于 Kubernetes 的轻量级容器运行时。</p>
<p>设置 container manager 为 CRI-O:</p>
<pre><code class="language-yaml">container_manager: crio
</code></pre>
<p>根据 Kubespray 文档，还需要设置以下变量：</p>
<pre><code class="language-yaml">download_container: false
skip_downloads: false # 这一项与 Kubespray 的默认值相同，可以省略
</code></pre>
<h3 id="底层容器运行时-1"><a class="header" href="#底层容器运行时-1">底层容器运行时</a></h3>
<p>CRI-O 具有配置多个底层容器运行时的功能，可以与 Kubernetes 的 <a href="https://kubernetes.io/docs/concepts/containers/runtime-class/">RuntimeClass</a> 功能一起使用。</p>
<p>CRI-O 使用 runc 作为默认的底层容器运行时，其配置如下：</p>
<pre><code class="language-yaml">crio_runtimes:
  - name: runc
    path: &quot;{{ bin_dir }}/runc&quot;
    type: oci
    root: /run/runc
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>不推荐修改 CRI-O 的默认容器运行时，因为 Kubespray 的相关支持不完善。</p>
</aside>
<p>你可以为 CRI-O 配置额外的底层容器运行时，例如启用 crun：</p>
<pre><code class="language-yaml">crun_enabled: true
</code></pre>
<p>你不需要考虑 CRI-O 的配置。Kubespray 会根据预设的以下变量，自动将 crun 加入到 CRI-O 的底层容器运行时配置中：</p>
<pre><code class="language-yaml">crun_runtime:
  name: crun
  path: &quot;{{ bin_dir }}/crun&quot;
  type: oci
  root: /run/crun
</code></pre>
<p>可用的额外容器运行时种类，及其下载设置请参阅：<a href="online/cri.html#%E9%99%84%E5%BD%95%E9%A2%9D%E5%A4%96%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6%E8%AE%BE%E7%BD%AE">额外容器运行时设置</a>。</p>
<h3 id="下载设置-2"><a class="header" href="#下载设置-2">下载设置</a></h3>
<p>设置 CRI-O 及相关组件版本：</p>
<pre><code class="language-yaml">crio_supported_versions:
  v1.28: v1.28.1
  v1.27: v1.27.1
  v1.26: v1.26.4
crio_version: &quot;{{ crio_supported_versions[kube_major_version] }}&quot;
runc_version: v1.1.12
skopeo_version: &quot;v1.13.2&quot;
</code></pre>
<blockquote>
<p>说明：变量 <code>kube_major_version</code> 是由变量 <code>kube_version</code> 派生出来的，具体来说，就是将 <code>kube_version</code> 的版本号最后一级去掉后的结果。</p>
</blockquote>
<p>设置下载地址：</p>
<pre><code class="language-yaml">crio_download_base: &quot;download.opensuse.org/repositories/devel:kubic:libcontainers:stable&quot;
crio_download_crio: &quot;http://{{ crio_download_base }}:/cri-o:/&quot;
crio_download_url: &quot;https://storage.googleapis.com/cri-o/artifacts/cri-o.{{ image_arch }}.{{ crio_version }}.tar.gz&quot;
skopeo_download_url: &quot;{{ files_repo }}/github.com/lework/skopeo-binary/releases/download/{{ skopeo_version }}/skopeo-linux-{{ image_arch }}&quot;
runc_download_url: &quot;{{ files_repo }}/github.com/opencontainers/runc/releases/download/{{ runc_version }}/runc.{{ image_arch }}&quot;
</code></pre>
<h3 id="可选设置-2"><a class="header" href="#可选设置-2">可选设置</a></h3>
<p>设置 Registry 镜像站：</p>
<pre><code class="language-yaml">crio_registries:
  - prefix: docker.io
    insecure: false
    blocked: false
    location: docker.io
    unqualified: false
    mirrors:
      - location: registry.dockermirror.com
        insecure: false
</code></pre>
<h2 id="示例"><a class="header" href="#示例">示例</a></h2>
<p>这里列举了三个分别使用 Docker, containerd 和 CRI-O 作为容器运行时的 inventory：</p>
<ul>
<li><a href="https://github.com/t9k/ks-clusters/tree/master/inventory/sample-multi-1.28.6-docker">Docker 容器运行时</a></li>
<li><a href="https://github.com/t9k/ks-clusters/tree/master/inventory/sample-multi-1.28.6-containerd">containerd 容器运行时</a></li>
<li><a href="https://github.com/t9k/ks-clusters/tree/master/inventory/sample-multi-1.28.6-crio">CRI-O 容器运行时</a></li>
</ul>
<p>你可以做如下修改后，直接使用这些示例 inventory 安装相应的 K8s 集群：</p>
<ul>
<li><a href="online/./inventory/basic-settings.html#%E8%AE%BE%E7%BD%AE-inventory">修改 <code>inventory.ini</code></a></li>
<li>根据实际网络环境设置 <code>group_vars/all/all.yml</code> 中的 <code>upstream_dns_servers</code> 变量</li>
</ul>
<h2 id="附录额外容器运行时设置"><a class="header" href="#附录额外容器运行时设置">附录：额外容器运行时设置</a></h2>
<p>containerd 和 CRI-O 支持安装额外的底层容器运行时，本章说明这些底层容器运行时相关的变量配置。</p>
<p>设置是否安装该容器运行时：</p>
<pre><code class="language-yaml">kata_containers_enabled: false
gvisor_enabled: false
crun_enabled: false
youki_enabled: false
</code></pre>
<p>容器运行时版本：</p>
<pre><code class="language-yaml">crun_version: 1.8.5
kata_containers_version: 3.1.3
youki_version: 0.1.0
gvisor_version: 20230807
</code></pre>
<p>容器运行时下载地址：</p>
<pre><code class="language-yaml">crun_download_url: &quot;https://github.com/containers/crun/releases/download/{{ crun_version }}/crun-{{ crun_version }}-linux-{{ image_arch }}&quot;
youki_download_url: &quot;https://github.com/containers/youki/releases/download/v{{ youki_version }}/youki_{{ youki_version | regex_replace('\\.', '_') }}_linux.tar.gz&quot;
kata_containers_download_url: &quot;https://github.com/kata-containers/kata-containers/releases/download/{{ kata_containers_version }}/kata-static-{{ kata_containers_version }}-{{ ansible_architecture }}.tar.xz&quot;
# gVisor only supports amd64 and uses x86_64 to in the download link
gvisor_runsc_download_url: &quot;https://storage.googleapis.com/gvisor/releases/release/{{ gvisor_version }}/{{ ansible_architecture }}/runsc&quot;
gvisor_containerd_shim_runsc_download_url: &quot;https://storage.googleapis.com/gvisor/releases/release/{{ gvisor_version }}/{{ ansible_architecture }}/containerd-shim-runsc-v1&quot;
</code></pre>
<h2 id="参考-4"><a class="header" href="#参考-4">参考</a></h2>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/CRI/cri-o.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/CRI/cri-o.md</a></li>
<li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/CRI/containerd.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/CRI/containerd.md</a></li>
<li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/CRI/docker.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/CRI/docker.md</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="cni-配置"><a class="header" href="#cni-配置">CNI 配置</a></h1>
<p>本文档说明如何为 K8s 集群安装不同的 CNI（Container Network Interface）。</p>
<aside class="note">
<div class="title">注意</div>
<p>在应用本文档中提供的配置时，请先在 inventory 中查找是否已经存在相应的变量设置。如果存在，进行确认或者修改；如果不存在，需要在 inventory 中添加相应的变量设置。</p>
</aside>
<h2 id="calico"><a class="header" href="#calico">Calico</a></h2>
<p>设置 CNI 为 Calico：</p>
<pre><code class="language-yaml">kube_network_plugin: calico
</code></pre>
<h3 id="下载设置-3"><a class="header" href="#下载设置-3">下载设置</a></h3>
<p>设置 Calico 的版本及镜像地址：</p>
<pre><code class="language-yaml">calico_version: &quot;v3.25.1&quot;
calico_node_image_repo: &quot;{{ docker_image_repo }}/calico-node&quot;
calico_cni_image_repo: &quot;{{ docker_image_repo }}/calico-cni&quot;
calico_flexvol_image_repo: &quot;{{ docker_image_repo }}/calico-pod2daemon-flexvol&quot;
calico_policy_image_repo: &quot;{{ docker_image_repo }}/calico-kube-controllers&quot;
</code></pre>
<h2 id="cilium"><a class="header" href="#cilium">Cilium</a></h2>
<p>Cilium 官方支持的安装方式为 Helm，通过 values.yaml 传入不同的参数来相应地改变所安装的 YAML 配置文件。Kubespray 在自己的仓库中维护了一份安装 Cilium 的 YAML 配置文件，但无法及时跟进 Cilium 的新特性，例如 L2 Annoucement、Gateway API 等。因此，这里选择在 Kubespray 安装 K8s 集群时先不安装 CNI，Kubespray 运行完毕后，再通过 Helm 安装 Cilium。</p>
<h3 id="安装-k8s-集群"><a class="header" href="#安装-k8s-集群">安装 K8s 集群</a></h3>
<p>设置 <code>kube_network_plugin</code> 的值为 <code>cni</code>，Kubespray 将仅完成一些基本配置，而不实际安装一个具体的 CNI：</p>
<pre><code class="language-yaml">kube_network_plugin: cni
</code></pre>
<p>Cilium 要求设置 <code>kube_owner</code> 的值为 <code>root</code>：</p>
<pre><code class="language-yaml">kube_owner: root
</code></pre>
<p>如需启用 Cilium 的 L2 Annoucement、Gateway API 等高级功能，不能安装 kube-proxy：</p>
<pre><code class="language-yaml">kube_proxy_remove: true
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>Cilium 的 <a href="https://docs.cilium.io/en/v1.15/network/l2-announcements/">L2 Annoucement</a>、<a href="https://docs.cilium.io/en/v1.15/network/servicemesh/gateway-api/gateway-api/">Gateway API</a> 等高级功能要求启用 <a href="https://docs.cilium.io/en/v1.15/network/kubernetes/kubeproxy-free/">Kubernetes Without kube-proxy</a> 特性，而这项特性存在以下<a href="https://docs.cilium.io/en/v1.15/network/kubernetes/kubeproxy-free/#limitations">局限性</a>：</p>
<ol>
<li>
<p>当集群存储为 Ceph 时，所有节点的 Linux kernel 版本必须为 5.8 及以上。</p>
</li>
<li>
<p>当集群存储为 NFS 时，所有节点的 Linux kernel 版本必须为 6.6 及以上。</p>
</li>
</ol>
</aside>
<p>运行 Kubespray 安装 K8s 集群，安装完成后，所有 Node 和 Pod 将处于 not ready 状态，因为尚不存在可用的 CNI；Cilium 安装完毕后将自动恢复正常。</p>
<h3 id="安装-cilium"><a class="header" href="#安装-cilium">安装 Cilium</a></h3>
<h4 id="基本功能安装"><a class="header" href="#基本功能安装">基本功能安装</a></h4>
<p>如果只需要 Cilium 作为 CNI 的基本功能，运行以下命令通过 Helm 安装 Cilium：</p>
<pre><code class="language-bash">cd ks-clusters
cd additionals/cilium

helm repo add cilium https://helm.cilium.io/
helm repo update
helm install cilium cilium/cilium --version 1.15.5 -n kube-system --values ./values-basic.yaml
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>注意检查 <code>values-basic.yaml</code> 的内容，确保符合实际安装环境。</p>
</aside>
<p><code>values-basic.yaml</code> 示例如下：</p>
<pre><code class="language-yaml">containerRuntime: # only needed when container runtime is cri-o
  integration: crio
image:
  repository: &quot;quay.io/cilium/cilium&quot;
  tag: &quot;v1.15.5&quot;
operator:
  image:
    repository: &quot;quay.io/cilium/operator&quot;
    tag: &quot;v1.15.5&quot;
ipam:
  operator:
    clusterPoolIPv4PodCIDRList: [&quot;10.233.64.0/18&quot;] # must equal to {{ kube_pods_subnet }}
    clusterPoolIPv4MaskSize: 24 # must equal to {{ kube_network_node_prefix }}
</code></pre>
<p>查看 Cilium 运行状态：</p>
<pre><code class="language-bash">kubectl get pod -n kube-system -l app.kubernetes.io/part-of=cilium
</code></pre>
<h4 id="高级功能安装"><a class="header" href="#高级功能安装">高级功能安装</a></h4>
<p>如果需要安装 Cilium 的 L2 Annoucement、Gateway API 等高级功能，运行以下命令通过 Helm 安装 Cilium：</p>
<pre><code class="language-bash">cd ks-clusters
cd additionals/cilium

kubectl create -f ./gateway-api

helm repo add cilium https://helm.cilium.io/
helm repo update
helm install cilium cilium/cilium --version 1.15.5 -n kube-system --values ./values-advanced.yaml
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>注意检查 <code>values-advanced.yaml</code> 的内容，确保符合实际安装环境。</p>
</aside>
<p><code>values-advanced.yaml</code> 示例如下：</p>
<pre><code class="language-yaml">containerRuntime: # only needed when container runtime is cri-o
  integration: crio
image:
  repository: &quot;quay.io/cilium/cilium&quot;
  tag: &quot;v1.15.5&quot;
operator:
  image:
    repository: &quot;quay.io/cilium/operator&quot;
    tag: &quot;v1.15.5&quot;
ipam:
  operator:
    clusterPoolIPv4PodCIDRList: [&quot;10.233.64.0/18&quot;] # must equal to {{ kube_pods_subnet }}
    clusterPoolIPv4MaskSize: 24 # must equal to {{ kube_network_node_prefix }}
kubeProxyReplacement: true
k8sServiceHost: x.x.x.x # PLEASE CHANGE THIS: must equal to {{ kube_apiserver_ip }}
k8sServicePort: 6443 # must equal to {{ kube_apiserver_port }}
k8sClientRateLimit:
  qps: 50
  burst: 100
l2announcements:
  enabled: true
gatewayAPI:
  enabled: true
</code></pre>
<p>查看 Cilium 运行状态：</p>
<pre><code class="language-bash">kubectl get gatewayclass cilium
kubectl get pod -n kube-system -l app.kubernetes.io/part-of=cilium
</code></pre>
<h2 id="参考-5"><a class="header" href="#参考-5">参考</a></h2>
<ul>
<li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/CNI/calico.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/CNI/calico.md</a></li>
<li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/CNI/cilium.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/CNI/cilium.md</a></li>
<li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/CNI/cni.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/CNI/cni.md</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="设置-user-namespace"><a class="header" href="#设置-user-namespace">设置 User Namespace</a></h1>
<p>K8s <a target="_blank" rel="noopener noreferrer" href="https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/">User Namespaces</a> 功能将容器内运行的用户与主机中的用户隔离开来。</p>
<p>这是一个只对 Linux 有效的功能特性，且需要 Linux 支持在所用文件系统上挂载 idmap。这要求：</p>
<ul>
<li>在节点上，你用于 <code>/var/lib/kubelet/pods/</code> 的文件系统，或你为此配置的自定义目录， 需要支持 idmap 挂载</li>
<li>Pod 挂载的所有存储卷，其使用的文件系统支持 idmap 挂载</li>
</ul>
<p>在实践中，这意味着你需要 Linux Kernel 的版本不低于 6.3，因为 K8s 常用的 tmpfs 在该版本中开始支持 idmap 挂载。Linux 6.3 中支持 idmap 挂载的一些比较流行的文件系统是：btrfs、ext4、xfs、fat、 tmpfs、overlayfs。</p>
<p>容器运行时必须支持 User Namespace：</p>
<ul>
<li>CRI-O：1.25 或更高版本</li>
<li>containerd：2.0.0 或更高版本</li>
</ul>
<p>其底层 OCI 运行时必须支持 User Namespace：</p>
<ul>
<li>crun 1.9 或更高版本（推荐 1.13+ 版本）</li>
<li>runc 1.2.0 或更高版本</li>
</ul>
<p>满足上述条件后，你可以在 <code>group_vars/k8s_cluster/k8s-cluster.yml</code> 中添加以下变量，来启动 User Namespace 功能：</p>
<pre><code class="language-yaml">kube_apiserver_feature_gates:
  - &quot;UserNamespacesSupport=true&quot;

kubelet_feature_gates:
  - &quot;UserNamespacesSupport=true&quot;
</code></pre>
<h2 id="示例-1"><a class="header" href="#示例-1">示例</a></h2>
<p>下面是一个完整的说明，基于<a target="_blank" rel="noopener noreferrer" href="https://github.com/t9k/ks-clusters/tree/master/inventory/sample-multi-1.28.6-containerd">示例 inventory</a> 修改 containerd 和 runc 的版本，并启用 User Namespace 功能：</p>
<pre><code class="language-bash">diff -u -r sample-multi-1.28.6-containerd/group_vars/all/download.yml \
  sample-multi-1.28.6-containerd-userns/group_vars/all/download.yml
</code></pre>
<pre><code class="language-diff">--- sample-multi-1.28.6-containerd/group_vars/all/download.yml
+++ sample-multi-1.28.6-containerd-userns/group_vars/all/download.yml
@@ -101,6 +101,7 @@
 # cri_dockerd_download_url: &quot;{{ files_repo }}/github.com/Mirantis/cri-dockerd/releases/download/v{{ cri_dockerd_version }}/cri-dockerd-{{ cri_dockerd_version }}.{{ image_arch }}.tgz&quot;
 
 # [Optional] runc: if you set container_manager to containerd or crio
+runc_version: v1.2.0-rc.1
 runc_download_url: &quot;{{ files_repo }}/github.com/opencontainers/runc/releases/download/{{ runc_version }}/runc.{{ image_arch }}&quot;
 
 # [Optional] cri-o: only if you set container_manager: crio
@@ -110,6 +111,8 @@
 # skopeo_download_url: &quot;{{ files_repo }}/github.com/lework/skopeo-binary/releases/download/{{ skopeo_version }}/skopeo-linux-{{ image_arch }}&quot;
 
 # [Optional] containerd: only if you set container_runtime: containerd
+containerd_version: 2.0.0-rc.1
 containerd_download_url: &quot;{{ files_repo }}/github.com/containerd/containerd/releases/download/v{{ containerd_version }}/containerd-{{ containerd_version }}-linux-{{ image_arch }}.tar.gz&quot;
 nerdctl_download_url: &quot;{{ files_repo }}/github.com/containerd/nerdctl/releases/download/v{{ nerdctl_version }}/nerdctl-{{ nerdctl_version }}-{{ ansible_system | lower }}-{{ image_arch }}.tar.gz&quot;
</code></pre>
<pre><code class="language-bash">diff -u -r sample-multi-1.28.6-containerd/group_vars/k8s_cluster/k8s-cluster.yml \
  sample-multi-1.28.6-containerd-userns/group_vars/k8s_cluster/k8s-cluster.yml
</code></pre>
<pre><code class="language-diff">--- sample-multi-1.28.6-containerd/group_vars/k8s_cluster/k8s-cluster.yml	
+++ sample-multi-1.28.6-containerd-userns/group_vars/k8s_cluster/k8s-cluster.yml
@@ -19,6 +19,12 @@
## Change this to use another Kubernetes version, e.g. a current beta release
kube_version: v1.28.6

+kube_apiserver_feature_gates:
+  - &quot;UserNamespacesSupport=true&quot;
+
+kubelet_feature_gates:
+  - &quot;UserNamespacesSupport=true&quot;
+
# Where the binaries will be downloaded.
# Note: ensure that you've enough disk space (about 1G)
local_release_dir: &quot;/tmp/releases&quot;
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>截止文档撰写时，containerd 只发布了 2.0.0-rc.1，runc 只发布了 1.2.0-rc.1。如可行，建议使用正式发布版本。</p>
</aside>
<p>此外，由于自定义了容器运行时及其组件的版本，且该版本信息不包含在 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubespray-defaults/defaults/main/checksums.yml">checksums.yml</a> 中。还需要在此文件中补充 containerd 和 runc 文件的 checksum：</p>
<pre><code class="language-yaml">runc_checksums:
  amd64:
    v1.2.0-rc.1: 57fbfc33a20ca3ee13ec0f81b2e8798a59b3f2de5e0d703609f4eb165127f0c6

containerd_archive_checksums:
  amd64:
    2.0.0-rc.1: 2a56fe585f19bdb7254192304c0dbd92e36f2b3dc695afc2bd9a0bd9d1769ae9
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="设置集群存储"><a class="header" href="#设置集群存储">设置集群存储</a></h1>
<p>K8s 集群至少需要安装一个 StorageClass 来提供集群存储服务，即 “<a target="_blank" rel="noopener noreferrer" href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/">动态持久卷制备（Dynamic Volume Provisioning）</a>”。</p>
<p>我们可以使用多家存储系统产品，例如 NFS、Ceph、Lustre、GPFS 等。</p>
<h2 id="目的-3"><a class="header" href="#目的-3">目的</a></h2>
<p>为 K8s 集群设置至少一个 StorageClass，以支持动态持久卷制备。</p>
<h2 id="使用-nfs"><a class="header" href="#使用-nfs">使用 NFS</a></h2>
<p>NFS 适合小规模或者测试场景，可通过 ansible 方便的安装，详情见 <a href="online/./storage-service/nfs.html">安装 NFS 及 StorageClass</a>。</p>
<h2 id="使用-ceph"><a class="header" href="#使用-ceph">使用 Ceph</a></h2>
<ol>
<li>
<p>获得 Ceph 集群</p>
<p>Ceph 集群需要单独部署和管理，详情参考：<a target="_blank" rel="noopener noreferrer" href="https://t9k.github.io/ceph-admin-docs/overview.html">Ceph 存储集群管理员手册</a></p>
</li>
<li>
<p>配置 K8s 使用 Ceph 集群</p>
<p>运行脚本在 K8s  集群的节点上安装 Ceph packages：</p>
<pre><code class="language-bash">ansible-playbook ../ks-clusters/t9k-playbooks/11-install-ceph-package.yml \
  -i inventory/inventory.ini \
  --become -K
</code></pre>
</li>
<li>
<p>安装 CSI driver</p>
<p>查看 Ceph CSI Driver 的变量（在 <code>../ks-clusters/t9k-playbooks/roles/ceph-csi/defaults/main.yml</code> 中）：</p>
<pre><code class="language-yaml">ceph:
    manifests_dir: &quot;{{ kube_config_dir }}/addons/ceph&quot;
    set_default_storage_class: true
    namespace: cephfs-hdd
    storage_class_name: cephfs-hdd
    driver_name: cephfs-hdd.csi.ceph.com
    cluster_id: &lt;your-cluster-id&gt;
    fs_name: k8s_hdd
    admin_id: k8s_hdd
    admin_key: &lt;your-admin-key&gt;
    metrics_port: 8681
    monitors:
    - &quot;100.0.0.1:6789&quot;
    - &quot;100.0.0.2:6789&quot;
    ...
</code></pre>
<p>使用 ansible 安装 Ceph CSI Driver：</p>
<pre><code class="language-bash">ansible-playbook ../ks-clusters/t9k-playbooks/12-install-ceph-csi.yml \
    -i inventory/inventory.ini \
    --become -K \
    -e ceph.cluster_id=&lt;your-cluster-id&gt; \
    -e ceph.admin_id=&quot;k8s_hdd&quot; \
    -e ceph.admin_key=&lt;your-admin-key&gt; \
    -e '{&quot;ceph&quot;: {&quot;monitors&quot;: [&quot;100.0.0.1:6789&quot;, &quot;100.0.0.2:6789&quot;]}}' 
</code></pre>
</li>
</ol>
<h2 id="使用-lustre"><a class="header" href="#使用-lustre">使用 Lustre</a></h2>
<p>TODO: Add details.</p>
<h2 id="使用-gpfs"><a class="header" href="#使用-gpfs">使用 GPFS</a></h2>
<p>TODO: Add details.</p>
<h2 id="参考-6"><a class="header" href="#参考-6">参考</a></h2>
<p><a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/">https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/</a></p>
<p><a href="https://t9k.github.io/ceph-admin-docs/overview.html">https://t9k.github.io/ceph-admin-docs/overview.html</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="集群维护"><a class="header" href="#集群维护">集群维护</a></h1>
<h2 id="集群节点"><a class="header" href="#集群节点">集群节点</a></h2>
<h3 id="worker-节点"><a class="header" href="#worker-节点">Worker 节点</a></h3>
<blockquote>
<p>参考：<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/nodes.md#addingreplacing-a-worker-node">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/nodes.md#addingreplacing-a-worker-node</a></p>
</blockquote>
<h4 id="增加-worker-节点"><a class="header" href="#增加-worker-节点">增加 worker 节点</a></h4>
<ol>
<li>
<p>修改 inventory.ini，下面是一个增加节点（nc15，worker node）的示例：</p>
<details><summary><code class="hljs">diff -u inventory-old.ini inventory-new.ini</code></summary>
<pre><code class="language-diff">--- inventory-old.ini
+++ inventory-new.ini
@@ -1,6 +1,7 @@
[all]
nuc ansible_host=nuc
nc11 ansible_host=nc11
nc12 ansible_host=nc12
nc13 ansible_host=nc13
nc14 ansible_host=nc14
+nc15 ansible_host=nc15

@@ -18,6 +19,7 @@
[kube_node]
nuc
nc11
nc12
nc13
nc14
+nc15
</code></pre>
</details>
</li>
<li>
<p>更新 facts</p>
<pre><code class="language-bash">ansible-playbook ../kubespray/playbooks/facts.yml \
    -i inventory/inventory.ini \
    --become -K
</code></pre>
<aside class="note">
 <div class="title">注意</div>
<p>kubespray 1.24.10 及之前的版本，需要运行 kubespray/facts.yml。1.25.9 之后 kubespray 移除了 kubespray/facts.yml 文件，作为代替的是 playbooks 文件夹中的 facts.yml。</p>
</aside>
</li>
<li>
<p>运行 kubespray 脚本来添加节点：</p>
<pre><code class="language-bash"># add node nc15
ansible-playbook ../kubespray/scale.yml \
    -i inventory/inventory.ini \
    --become -K \
    --limit nc15
</code></pre>
<aside class="note">
 <div class="title">注意</div>
<p>使用命令行参数 <code>--limit nc15</code> 限制 playbook 的执行范围在 nc15 节点上，保障其他节点不受影响。如果有多个节点需要添加，使用例如 <code>--limit nc15,nc16</code> 的格式指定。</p>
</aside>
</li>
</ol>
<h4 id="移除-worker-节点"><a class="header" href="#移除-worker-节点">移除 worker 节点</a></h4>
<ol>
<li>
<p>更新 facts：</p>
<pre><code class="language-bash">ansible-playbook ../kubespray/playbooks/facts.yml \
    -i inventory/inventory.ini \
    --become -K
</code></pre>
</li>
<li>
<p>运行 kubespray 脚本来删除节点：</p>
<pre><code class="language-bash"># remove node nc12
ansible-playbook ../kubespray/remove-node.yml \
    -i inventory/inventory.ini \
    --become -K \
    -e node=nc12 --limit nc12 
</code></pre>
<aside class="note">
 <div class="title">注意</div>
<p>使用命令行参数 <code>-e</code> 设置 node 变量，指定要移除的节点。如果有多个节点需要移除，使用例如 <code>-e node=nc12,nc13 --limit nc12,nc13</code> 的格式指定。</p>
</aside>
</li>
<li>
<p>修改 inventory 文件，删去已经移除的节点。</p>
</li>
</ol>
<h3 id="control-plane-节点"><a class="header" href="#control-plane-节点">Control plane 节点</a></h3>
<p>对 control plane 节点的修改需要运行 cluster.yml，具体请参考文档：</p>
<ol>
<li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/nodes.md#addingreplacing-a-control-plane-node">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/nodes.md#addingreplacing-a-control-plane-node</a></li>
<li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/nodes.md#replacing-a-first-control-plane-node">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/nodes.md#replacing-a-first-control-plane-node</a></li>
</ol>
<h2 id="集群拆除"><a class="header" href="#集群拆除">集群拆除</a></h2>
<aside class="note warning">
<div class="title">警告</div>
<ol>
<li>集群的拆除是不可逆的，在运行之前请确认您已经备份了集群中的重要数据；</li>
<li>这里描述的方法仅限于使用 kubespray 部署的集群，并且要和集群部署时使用的 kubespray 版本和 inventory 一致。</li>
</ol>
</aside>
<p>拆除集群：</p>
<pre><code class="language-bash">ansible-playbook ../kubespray/reset.yml \
    -i inventory/inventory.ini \
    --become -K
</code></pre>
<h2 id="升级-k8s-版本"><a class="header" href="#升级-k8s-版本">升级 K8s 版本</a></h2>
<aside class="note">
<div class="title">注意</div>
<ol>
<li>在升级集群之前，查看 <a target="_blank" rel="noopener noreferrer" href="https://kubernetes.io/releases/#release-history">Release History</a> 的 Changelog 以了解 K8s 做了什么修改，并判断这些修改是否会影响您集群中的工作负载。</li>
<li>检查 kubespray 的 kubeadm_checksums 变量的值来确定目标 K8s 版本是否被支持。这个变量位于 role download 的 defaults 文件夹中，可能是 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/v2.22.1/roles/download/defaults/main.yml#L488">main.yml</a> 或者 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/master/roles/download/defaults/main/checksums.yml#L292">main/checksums.yml</a>。</li>
<li>升级时，请使用 T9k 提供的 kubespray 的相邻版本进行升级，不要跨多个版本升级。</li>
</ol>
</aside>
<p>步骤：</p>
<ol>
<li>将 kubespray 切换到合适的分支</li>
<li>修改 inventory，指定合适的 <code>kube_version</code>，<code>docker_version</code> 等计划升级的版本</li>
<li>运行升级脚本：</li>
</ol>
<pre><code class="language-bash">ansible-playbook ../kubespray/upgrade-cluster.yml \ 
    -i inventory/inventory.ini \
    --become \
    -e &quot;@~/nc15-1.25.9/vault.yml&quot; \
    --vault-password-file=~/ansible/.vault-password.txt
</code></pre>
<details><summary><code class="hljs">运行成功的 PLAY RECAP 示例</code></summary>
<pre><code>PLAY RECAP *********************************************
localhost                  : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
nc12                       : ok=483  changed=32   unreachable=0    failed=0    skipped=814  rescued=0    ignored=1   
nc14                       : ok=483  changed=32   unreachable=0    failed=0    skipped=814  rescued=0    ignored=1   
nc15                       : ok=742  changed=61   unreachable=0    failed=0    skipped=1561 rescued=0    ignored=1
</code></pre>
</details>
<h2 id="参考-7"><a class="header" href="#参考-7">参考</a></h2>
<p><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/nodes.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/nodes.md</a></p>
<p><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/upgrades.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/upgrades.md</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="常见问题"><a class="header" href="#常见问题">常见问题</a></h1>
<h2 id="大集群"><a class="header" href="#大集群">大集群</a></h2>
<p>如果集群节点数 &gt; 100，请阅读： <a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/large-deployments.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/large-deployments.md</a></p>
<h2 id="离线安装"><a class="header" href="#离线安装">离线安装</a></h2>
<p>请使用 <a href="online/../offline/index.html">离线安装</a> 文档。</p>
<p>kubespray 的离线安装文档：<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/offline-environment.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/offline-environment.md</a></p>
<h2 id="调整-lvm-的逻辑卷大小"><a class="header" href="#调整-lvm-的逻辑卷大小">调整 LVM 的逻辑卷大小</a></h2>
<p>Logical Volume Manager（LVM）是 Linux 环境下对磁盘分区进行管理的一种机制。</p>
<p>LVM的主要概念有：</p>
<ol>
<li>PV (Physical Volume)：物理卷，可以是整个物理硬盘或实际物理硬盘上的分区。</li>
<li>VG (Volume Group)：卷组，将数个PV进行整合，形成一个存储池。</li>
<li>LV (Logical Volume)：逻辑卷，由VG划分而来，LV的大小与PE的大小及PE的数量有关。</li>
<li>PE (Physical Extent)：物理区块，他是LVM中的最小存储单元。</li>
</ol>
<p>在准备节点的过程中，有时需要调整逻辑卷的大小，下面介绍具体的操作方式。</p>
<p>参考：</p>
<ol>
<li><a target="_blank" rel="noopener noreferrer" href="https://linux.die.net/man/8/lvextend">lvextend(8) - Linux man page</a></li>
<li><a target="_blank" rel="noopener noreferrer" href="https://www.redhat.com/sysadmin/resize-lvm-simple">How to resize a logical volume with 5 simple LVM commands</a></li>
</ol>
<p>查看文件系统和磁盘信息：</p>
<pre><code class="language-bash">lsblk
</code></pre>
<pre><code class="language-bash">NAME                      MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
loop0                       7:0    0  63.9M  1 loop /snap/core20/2318
loop1                       7:1    0  63.9M  1 loop /snap/core20/2105
loop3                       7:3    0    87M  1 loop /snap/lxd/28373
loop4                       7:4    0  38.7M  1 loop /snap/snapd/21465
loop5                       7:5    0  38.8M  1 loop /snap/snapd/21759
loop6                       7:6    0    87M  1 loop /snap/lxd/29351
sda                         8:0    0 447.1G  0 disk 
├─sda1                      8:1    0     1G  0 part /boot/efi
├─sda2                      8:2    0     2G  0 part /boot
└─sda3                      8:3    0 444.1G  0 part 
  └─ubuntu--vg-ubuntu--lv 252:0    0   440G  0 lvm  /var/lib/containers/storage/overlay
                                                    /
</code></pre>
<p>查看文件系统的信息：</p>
<pre><code class="language-bash">df -hl
</code></pre>
<pre><code class="language-bash">Filesystem                         Size  Used Avail Use% Mounted on
tmpfs                              1.6G   31M  1.5G   2% /run
efivarfs                           192K  108K   80K  58% /sys/firmware/efi/efivars
/dev/mapper/ubuntu--vg-ubuntu--lv  433G   18G  393G   5% /
tmpfs                              7.7G     0  7.7G   0% /dev/shm
tmpfs                              5.0M     0  5.0M   0% /run/lock
/dev/sda2                          2.0G  417M  1.4G  23% /boot
/dev/sda1                          1.1G  6.1M  1.1G   1% /boot/efi
tmpfs                              1.6G  4.0K  1.6G   1% /run/user/1000
</code></pre>
<p>查看卷组的信息：</p>
<pre><code class="language-bash">vgs # 如需更详细的信息，可以运行 vgdisplay
</code></pre>
<pre><code class="language-bash">  VG        #PV #LV #SN Attr   VSize    VFree 
  ubuntu-vg   1   1   0 wz--n- &lt;444.08g &lt;4.08g
</code></pre>
<p>查看逻辑卷的信息：</p>
<pre><code class="language-bash">lvs # 如需更详细的信息，可以运行 lvdisplay
</code></pre>
<pre><code class="language-bash">  LV        VG        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  ubuntu-lv ubuntu-vg -wi-ao---- 440.00g 
</code></pre>
<p>扩展逻辑卷，将它增加 2GB，并同时调整文件系统的大小：</p>
<pre><code class="language-bash">lvextend --resizefs --size +2GB /dev/ubuntu-vg/ubuntu-lv
</code></pre>
<pre><code class="language-bash">  Size of logical volume ubuntu-vg/ubuntu-lv changed from 440.00 GiB (112640 extents) to 442.00 GiB (113152 extents).
  Logical volume ubuntu-vg/ubuntu-lv successfully resized.
resize2fs 1.46.5 (30-Dec-2021)
Filesystem at /dev/mapper/ubuntu--vg-ubuntu--lv is mounted on /; on-line resizing required
old_desc_blocks = 55, new_desc_blocks = 56
The filesystem on /dev/mapper/ubuntu--vg-ubuntu--lv is now 115867648 (4k) blocks long.
</code></pre>
<aside class="note">
<div class="title">注意</div>
<ol>
<li><code>--resizefs</code> 参数仅适用于 ext2, ext3, ext4, ReiserFS, XFS 文件系统。</li>
<li>除了使用 <code>--size +2GB</code> 指定具体的大小外，也可以使用 <code>-l +100%FREE</code> 指定百分比。</li>
</ol>
</aside>
<p>验证文件系统已经扩大：</p>
<pre><code class="language-bash">df -hl
</code></pre>
<pre><code class="language-bash">Filesystem                         Size  Used Avail Use% Mounted on
tmpfs                              1.6G   31M  1.5G   2% /run
efivarfs                           192K  108K   80K  58% /sys/firmware/efi/efivars
/dev/mapper/ubuntu--vg-ubuntu--lv  434G   18G  394G   5% /
tmpfs                              7.7G     0  7.7G   0% /dev/shm
tmpfs                              5.0M     0  5.0M   0% /run/lock
/dev/sda2                          2.0G  417M  1.4G  23% /boot
/dev/sda1                          1.1G  6.1M  1.1G   1% /boot/efi
tmpfs                              1.6G  4.0K  1.6G   1% /run/user/1000
</code></pre>
<h2 id="日志收集不完整"><a class="header" href="#日志收集不完整">日志收集不完整</a></h2>
<p>问题表现为 kubelet cadvisor 收集的 <code>container_network_receive_bytes_total</code> 等 metrics 缺失 container 信息。</p>
<p>该问题在 issue 中有非常清晰的描述：</p>
<ul>
<li><a href="https://github.com/rancher/rancher/issues/38934#issuecomment-1294585708">https://github.com/rancher/rancher/issues/38934#issuecomment-1294585708</a></li>
<li><a href="https://github.com/kubernetes/website/issues/30681#issuecomment-1205677145">https://github.com/kubernetes/website/issues/30681#issuecomment-1205677145</a></li>
</ul>
<p>上述的两个 issue 中，前一个 issue 中的回答解释了问题的原因，并提供了一个 workaround。后一个 issue 简单介绍了这个问题的原因和现状，并附上了相关 KEP 的<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2371-cri-pod-container-stats/README.md">链接</a>。</p>
<h2 id="无法拉取-docker-hub-镜像"><a class="header" href="#无法拉取-docker-hub-镜像">无法拉取 Docker Hub 镜像</a></h2>
<p>使用下面的命令验证容器运行时可以从 Docker Hub 拉取镜像：</p>
<pre><code class="language-bash"># 容器运行时是 Docker
docker pull docker.io/t9kpublic/hello-world

# 容器运行时不是 Docker
# 注意 crictl 命令拉取镜像时不会显示进度条，因此需要耐心等待 1-2 分钟
crictl pull docker.io/t9kpublic/hello-world
</code></pre>
<p>如果节点无法顺利拉取 Docker Hub 的镜像，可以通过设置 Registry 镜像站或者设置代理来解决。下面提供具体的设置方法。</p>
<h3 id="未加入-k8s-集群"><a class="header" href="#未加入-k8s-集群">未加入 K8s 集群</a></h3>
<p>Kubespray 添加节点的过程会卸载原有的容器运行时，并自动安装规定版本的容器运行时。</p>
<p>通过修改 Kubespray inventory 的配置，可以使得节点安装的容器运行时进行相应的配置。你可以选择一种方式来设置：</p>
<ol>
<li>参考安装 K8s 文档的<a href="online/k8s-install.html#%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86">设置代理</a>，其中说明了如何为容器运行时设置代理。</li>
<li>参考 <a href="online/cri.html">CRI 配置</a>文档，找到你选择的容器运行时，在“可选操作”中说明了如何为容器运行时“设置 Registry 镜像站”。</li>
</ol>
<h3 id="已加入-k8s-集群---设置-registry-镜像站"><a class="header" href="#已加入-k8s-集群---设置-registry-镜像站">已加入 K8s 集群 - 设置 Registry 镜像站</a></h3>
<p>本章说明如何为一个已经加入 K8s 集群的节点设置容器运行时的代理。</p>
<p>因为修改设置需要重启节点的容器运行时服务，所以建议在开始操作前，先驱逐节点上的工作负载：</p>
<pre><code class="language-bash"># 在配置了 kubectl 的节点执行
kubectl drain &lt;node&gt; --ignore-daemonsets  --delete-emptydir-data --force
</code></pre>
<h4 id="docker-容器运行时"><a class="header" href="#docker-容器运行时">Docker 容器运行时</a></h4>
<p>修改 Docker 的配置文件：</p>
<pre><code class="language-bash">sudo vim /etc/systemd/system/docker.service.d/docker-options.conf
</code></pre>
<p>进行以下修改：</p>
<pre><code class="language-diff">diff -u docker-options.old.conf docker-options.new.conf 
--- ./docker-options.old.conf	2023-08-15 10:23:44.388444563 +0000
+++ ./docker-options.new.conf	2023-08-15 10:23:22.120168571 +0000
@@ -2,7 +2,7 @@
 Environment=&quot;DOCKER_OPTS= --iptables=false \
 --exec-opt native.cgroupdriver=systemd \
  \
- \
+--registry-mirror=https://registry.dockermirror.com/ \
 --data-root=/var/lib/docker \
 --log-opt max-size=50m --log-opt max-file=5&quot;
</code></pre>
<p>加载配置，并重启 Docker：</p>
<pre><code class="language-bash">sudo systemctl daemon-reload
sudo systemctl restart docker
</code></pre>
<p>验证生效：</p>
<pre><code class="language-bash">sudo docker info
</code></pre>
<p>输出：</p>
<pre><code>…
Server:
…
 Registry Mirrors:
  https://registry.dockermirror.com/
…
</code></pre>
<p>最后，去掉节点的 taint，允许工作负载被调度到节点上：</p>
<pre><code class="language-bash"># 在配置了 kubectl 的节点执行
kubectl uncordon &lt;node&gt;
</code></pre>
<h4 id="containerd-容器运行时"><a class="header" href="#containerd-容器运行时">containerd 容器运行时</a></h4>
<p>首先确认该文件夹存在，如果不存在则进行创建：</p>
<pre><code class="language-bash">sudo mkdir -p /etc/containerd/certs.d/docker.io
</code></pre>
<p>编辑配置文件：</p>
<pre><code class="language-bash">sudo vim /etc/containerd/certs.d/docker.io/hosts.toml
</code></pre>
<p>在配置文件中加入以下设置：</p>
<pre><code class="language-bash">server = &quot;https://docker.io&quot;
[host.&quot;https://registry.dockermirror.com&quot;]
  capabilities = [&quot;pull&quot;,&quot;resolve&quot;]
  skip_verify = false
  override_path = false
</code></pre>
<p>重启容器运行时：</p>
<pre><code class="language-bash">sudo systemctl restart containerd
</code></pre>
<p>验证：</p>
<pre><code class="language-bash"># 注意 crictl 命令拉取镜像时不会显示进度条，因此需要耐心等待 1-2 分钟
crictl pull docker.io/t9kpublic/hello-world
</code></pre>
<p>最后，去掉节点的 taint，允许工作负载被调度到节点上：</p>
<pre><code class="language-bash"># 在配置了 kubectl 的节点执行
kubectl uncordon &lt;node&gt;
</code></pre>
<h4 id="cri-o-容器运行时"><a class="header" href="#cri-o-容器运行时">CRI-O 容器运行时</a></h4>
<p>编辑 CRI-O 的配置文件：</p>
<pre><code class="language-bash">vim /etc/containers/registries.conf.d/10-docker.io.conf
</code></pre>
<p>加入 Registry 镜像站的设置：</p>
<pre><code class="language-bash">[[registry]]
prefix = &quot;docker.io&quot;
insecure = false
blocked = false
location = &quot;docker.io&quot;

[[registry.mirror]]
location = &quot;registry.dockermirror.com&quot;
insecure = false
</code></pre>
<p>重启容器运行时：</p>
<pre><code class="language-bash">sudo systemctl restart crio
</code></pre>
<p>验证：</p>
<pre><code class="language-bash"># 注意 crictl 命令拉取镜像时不会显示进度条，因此需要耐心等待 1-2 分钟
crictl pull docker.io/t9kpublic/hello-world
</code></pre>
<p>最后，去掉节点的 taint，允许工作负载被调度到节点上：</p>
<pre><code class="language-bash"># 在配置了 kubectl 的节点执行
kubectl uncordon &lt;node&gt;
</code></pre>
<h3 id="已加入-k8s-集群---设置代理"><a class="header" href="#已加入-k8s-集群---设置代理">已加入 K8s 集群 - 设置代理</a></h3>
<p>本章说明如何为一个已经加入 K8s 集群的节点设置容器运行时的代理。</p>
<p>由于需要重启节点的容器运行时服务，建议在开始操作前，先驱逐节点上的工作负载：</p>
<pre><code class="language-bash"># 在配置了 kubectl 的节点执行
kubectl drain &lt;node&gt; --ignore-daemonsets  --delete-emptydir-data --force
</code></pre>
<p>准备一个保存了代理信息的文件，文件内容为：</p>
<pre><code class="language-bash">[Service]
Environment=&quot;HTTP_PROXY=&lt;proxy-server&gt;&quot; &quot;HTTPS_PROXY=&lt;proxy-server&gt;&quot; &quot;NO_PROXY=localhost,&lt;no-proxy-IP&gt;&quot;
</code></pre>
<p>将该文件存放到指定路径下：</p>
<ul>
<li>Docker 容器运行时：<code>/etc/systemd/system/docker.service.d/http-proxy.conf</code></li>
<li>containerd 容器运行时：<code>/etc/systemd/system/containerd.service.d/http-proxy.conf</code></li>
<li>CRI-O 容器运行时：<code>/etc/systemd/system/crio.service.d/http-proxy.conf</code></li>
</ul>
<aside class="note">
<div class="title">注意</div>
<p>如果指定路径中不存在该文件，直接创建即可；如果指定文件中已经存在配置文件，则需要在原配置文件的基础上添加上述文件内容。</p>
</aside>
<p>然后重启容器运行时，这里以 Docker 为例，其他容器运行时同理：</p>
<pre><code class="language-bash">systemctl restart docker
</code></pre>
<p>最后，去掉节点的 taint，允许工作负载被调度到节点上：</p>
<pre><code class="language-bash"># 在配置了 kubectl 的节点执行
kubectl uncordon &lt;node&gt;
</code></pre>
<h2 id="升级-linux-kernel-后无法找到网卡"><a class="header" href="#升级-linux-kernel-后无法找到网卡">升级 linux kernel 后无法找到网卡</a></h2>
<p>通过以下命令可以升级 ubuntu 系统的 linux kernel：</p>
<pre><code class="language-bash">sudo apt --fix-broken install linux-{image,headers}-5.15.0-107-generic
sudo reboot
</code></pre>
<p>如果升级 kernel 后，节点无法 ssh 连接，物理连接后显示没有可用的网卡，根据<a href="https://askubuntu.com/questions/1307447/network-not-working-updating-to-kernel-5-8-ubuntu-20-04">讨论</a>，原因是升级 kernel 时 <code>linux-modules-extra-*</code> 包缺失，安装对应版本的包并重启即可：</p>
<pre><code class="language-bash">dpkg -l | grep linux- | grep 5.15.0
</code></pre>
<pre><code>ii  linux-headers-5.15.0-107-generic      5.15.0-107.117~20.04.1            amd64        Linux kernel headers for version 5.15.0 on 64 bit x86 SMP
ii  linux-hwe-5.15-headers-5.15.0-107     5.15.0-107.117~20.04.1            all          Header files related to Linux kernel version 5.15.0
ii  linux-image-5.15.0-107-generic        5.15.0-107.117~20.04.1            amd64        Signed kernel image generic
ii  linux-modules-5.15.0-107-generic      5.15.0-107.117~20.04.1            amd64        Linux kernel extra modules for version 5.15.0 on 64 bit x86 SMP
</code></pre>
<pre><code class="language-bash">sudo apt install linux-modules-extra-5.15.0-107-generic

sudo reboot
</code></pre>
<h2 id="参考-8"><a class="header" href="#参考-8">参考</a></h2>
<p><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/large-deployments.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/large-deployments.md</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装后配置"><a class="header" href="#安装后配置">安装后配置</a></h1>
<h2 id="加固集群安全"><a class="header" href="#加固集群安全">加固集群安全</a></h2>
<p><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/hardening.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/hardening.md</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装-k8s-组件"><a class="header" href="#安装-k8s-组件">安装 K8s 组件</a></h1>
<p>在最基本的 K8s 安装完成之后，我们需要在 K8s 集群上安装一些额外的组件，以支持 TensorStack AI 计算平台的安装。</p>
<p>本部分安装的模块如下：</p>
<ul>
<li><a href="online/k8s-components/./istio.html">Istio</a> - Service Mesh 和 Gateway API</li>
<li><a href="online/k8s-components/./knative.html">Knative</a> - Serverles 框架</li>
<li><a href="online/k8s-components/./metrics-server.html">Metrics Server</a> - 确保其设置正确</li>
<li><a href="online/k8s-components/./elastic-search.html">Elastic Search</a> - 存储集群的 log</li>
<li><a href="online/k8s-components/./monitoring.html">监控相关</a> - 一些设置</li>
<li><a href="online/k8s-components/./gatekeeper.html">Gatekeeper</a> - 准入控制</li>
</ul>
<h2 id="下一步-6"><a class="header" href="#下一步-6">下一步</a></h2>
<p>完成 K8s 组件安装之后，根据需要 <a href="online/k8s-components/../../hardware/index.html">安装硬件支持</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="istio"><a class="header" href="#istio">Istio</a></h1>
<p>T9k 产品需要使用 Istio 的 routing API (e.g. Gateway, VirtualService) 以及 Knative 也依赖 Istio。</p>
<h2 id="安装"><a class="header" href="#安装">安装</a></h2>
<p>文档 <a href="https://istio.io/latest/docs/releases/supported-releases/#support-status-of-istio-releases">Support status of Istio releases</a> 记录了各个 Istio 版本兼容的 K8s 版本。</p>
<p>结合上述文档和之前的使用经验，我们提供以下安装建议：</p>
<ul>
<li>Kubernetes v1.22 到 v1.25，安装 Istio 1.15.2。</li>
<li>Kubernetes v1.26 到 v1.28，安装 Istio 1.20.6。</li>
<li>Kubernetes v1.29 到 v1.30，安装 Istio 1.23.0。</li>
</ul>
<h3 id="下载-istio"><a class="header" href="#下载-istio">下载 istio</a></h3>
<p>以安装 Istio 1.15.2 为例，其他版本的 Istio 安装只需要修改下载的 istioctl 版本即可。</p>
<pre><code class="language-bash">cd ~/ansible/$T9K_CLUSTER

# online install, istio-1.15.2
curl -LO https://github.com/istio/istio/releases/download/1.15.2/istio-1.15.2-linux-amd64.tar.gz

# offline install, istio-1.15.2
cp ../ks-clusters/tools/offline-additionals/misc/istio-1.15.2-linux-amd64.tar.gz ./

tar zxvf istio-1.15.2-linux-amd64.tar.gz
cd istio-1.15.2
export PATH=$PWD/bin:$PATH
cd ..
</code></pre>
<p>再提供一个 Istio 1.20.6 的例子：</p>
<pre><code class="language-bash">cd ~/ansible/$T9K_CLUSTER

# online install, istio-1.20.6
curl -LO https://github.com/istio/istio/releases/download/1.20.6/istio-1.20.6-linux-amd64.tar.gz

# offline install, istio-1.20.6
cp ../ks-clusters/tools/offline-additionals/misc/istio-1.20.6-linux-amd64.tar.gz ./

tar zxvf istio-1.20.6-linux-amd64.tar.gz
cd istio-1.20.6
export PATH=$PWD/bin:$PATH
cd ..
</code></pre>
<p>后续操作适用于不同版本的 Istio，不需要额外的修改。</p>
<h3 id="修改配置"><a class="header" href="#修改配置">修改配置</a></h3>
<pre><code class="language-bash">vim ../ks-clusters/additionals/istio/config.yaml
</code></pre>
<p>文件中的配置解释，参考 <a target="_blank" rel="noopener noreferrer" href="https://istio.io/latest/docs/reference/config/istio.operator.v1alpha1/">IstioOperator Options</a>：</p>
<ol>
<li><code>autoInject: disabled</code> 默认禁止自动注入 sidecar，仅在 Pod 或者 Pod 所在的 namespace 配置中要求了注入。我们不希望使用 istio 的 sidecar 注入功能。</li>
<li><code>sidecarInjectorWebhook.enableNamespacesByDefault: false</code> 新创建的命名空间默认禁用自动注入。同上，我们不希望使用 istio 的 sidecar 注入功能。</li>
<li><code>hub: docker.io/t9kpublic</code> 用于指定 docker image 的前缀。</li>
<li><code>addonComponents.pilot.enabled: true</code> 启用 Pilot 组件。Pilot 负责管理和配置所有的 Istio 服务网格。</li>
<li><code>components.ingressGateways</code> 定义了一个名为 istio-ingressgateway 的 ingress gateway，用于从服务网格外部访问内部服务。</li>
</ol>
<aside class="note">
<div class="title">离线安装</div>
<p>如果采用本地容器镜像服务器，需要修改镜像仓库的设置：</p>
<pre><code class="language-bash"># for example, using 192.168.101.159:5000/t9kpublic as registry
sed -i &quot;s|hub: docker.io/t9kpublic|hub: 192.168.101.159:5000/t9kpublic|g&quot; \
  ../ks-clusters/additionals/istio/config.yaml
</code></pre>
</aside>
<h3 id="启动安装"><a class="header" href="#启动安装">启动安装</a></h3>
<pre><code class="language-bash">istioctl install -f ../ks-clusters/additionals/istio/config.yaml
</code></pre>
<p>istioctl install 过程中会自动检查对应的 Pod 存在，且处于正常的 running 状态：</p>
<pre><code class="language-console">This will install the Istio 1.15.2 default profile with [&quot;Istio core&quot; &quot;Istiod&quot; &quot;Ingress gateways&quot;] components into the cluster. Proceed? (y/N) y
✔ Istio core installed
✔ Istiod installed
✔ Ingress gateways installed
✔ Installation complete
</code></pre>
<h2 id="验证"><a class="header" href="#验证">验证</a></h2>
<p>进一步检查 logs：</p>
<pre><code class="language-bash"># inspect logs
# warning: logs can be very long
kubectl -n istio-system logs -l app=istiod --tail=-1 -f

# warning: logs can be very long
kubectl -n istio-system logs -l app=istio-ingressgateway --tail=-1 -f
</code></pre>
<p>检查 injection policy 配置：</p>
<pre><code class="language-bash"># default injection policy 
kubectl -n istio-system get configmap istio-sidecar-injector \
  -o jsonpath='{.data.config}' | grep policy:
</code></pre>
<p>输出：</p>
<pre><code class="language-console">policy: disabled
</code></pre>
<p>检查 <code>rewriteAppHTTPProbe</code> 设置：</p>
<pre><code class="language-bash"># side-car injector
kubectl -n istio-system get cm istio-sidecar-injector -o jsonpath='{.data.values}' \
  | grep rewriteAppHTTPProbe
</code></pre>
<p>输出：</p>
<pre><code class="language-console">    &quot;rewriteAppHTTPProbe&quot;: true,
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>在 ConfigMap istio-sidecar-injector 中，我们关心下面这两个配置参数：</p>
<ol>
<li><code>.data.config</code> 中的 <code>policy</code>：
<ol>
<li>值应该是 <code>disabled；</code></li>
<li>意义：控制 sidecar injection 的 default policy，影响 istio sidecar-injection-webhook 是否为 Pod 注入 sidecar container；</li>
<li>参考 <a target="_blank" rel="noopener noreferrer" href="https://istio.io/v1.1/help/ops/setup/injection/#:~:text=Check%20default%20policy">Check default policy</a>；</li>
</ol>
</li>
<li><code>.data.values</code> 中的 <code>rewriteAppHTTPProbe</code>：
<ol>
<li>值应该是 <code>true</code></li>
<li>意义：<code>rewriteAppHTTPProbe</code> 是 <code>true</code> 时，被注入 sidecar container 的 Pod 的 readiness/liveness probe 字段会被 istio 重写，使得 probe request 被发送到 sidecar agent。</li>
<li>参考：<a target="_blank" rel="noopener noreferrer" href="https://istio.io/latest/docs/ops/configuration/mesh/app-health-check/">Health Checking of Istio Services</a></li>
</ol>
</li>
</ol>
</aside>
<pre><code class="language-bash"># TODO: 其他 cm，命令行参数等
kubectl -n istio-system  get cm
</code></pre>
<p>输出：</p>
<pre><code class="language-console">NAME                                  DATA   AGE
istio                                 2      225d
istio-ca-root-cert                    1      225d
istio-gateway-deployment-leader       0      225d
istio-gateway-leader                  0      225d
istio-gateway-status-leader           0      225d
istio-leader                          0      225d
istio-namespace-controller-election   0      225d
istio-sidecar-injector                2      225d
kube-root-ca.crt                      1      225d
</code></pre>
<h2 id="参考-9"><a class="header" href="#参考-9">参考</a></h2>
<p><a href="https://istio.io/latest/docs/reference/config/istio.operator.v1alpha1/">https://istio.io/latest/docs/reference/config/istio.operator.v1alpha1/</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="knative"><a class="header" href="#knative">Knative</a></h1>
<h2 id="目的-4"><a class="header" href="#目的-4">目的</a></h2>
<p>安装 Knative。</p>
<h2 id="前置条件"><a class="header" href="#前置条件">前置条件</a></h2>
<p>Knative 依赖 K8s 集群、<a target="_blank" rel="noopener noreferrer" href="https://istio.io/">Istio</a> 以及 <a target="_blank" rel="noopener noreferrer" href="https://kubernetes.github.io/ingress-nginx/deploy/">NGINX Ingress Controller</a>。其中 NGINX Ingress Controller 默认会在<a href="online/k8s-components/../k8s-install.html#%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4">安装集群</a>时一同被安装，选择与 K8s 兼容的版本即可。</p>
<p>安装的 Knative 版本需要与 K8s、Istio 版本相适应的，参考文档是：<a href="https://github.com/knative/community/blob/main/mechanics/RELEASE-SCHEDULE.md">RELEASE-SCHEDULE</a>。</p>
<p>我们根据实际使用的经验，给出以下版本建议：</p>
<ul>
<li>Kubernetes 版本 v1.22.0，Istio 版本 1.15.2，适用 Knative v1.7.1</li>
<li>Kubernetes 版本 v1.25.9，Istio 版本 1.15.2，适用 Knative v1.9.0</li>
<li>Kubernetes 版本 v1.28.6，Istio 版本 1.20.6，适用 Knative v1.13.1</li>
<li>Kubernetes 版本 v1.30.4，Istio 版本 1.23.0，适用 Knative v1.15.1</li>
</ul>
<h2 id="安装-1"><a class="header" href="#安装-1">安装</a></h2>
<p>下面以安装 Knative v1.13.1 为例。如需安装其他版本，可以选择相应的文件夹进行安装。</p>
<aside class="note">
<div class="title">离线安装</div>
<p>如需要使用本地镜像仓库（替换 <code>192.168.101.159:5000</code> 为实际的地址）：</p>
<pre><code class="language-bash"># verify t9kpublic is only used in image name
grep t9kpublic ../ks-clusters/additionals/knative/v1.13.1/*

# replace image registry
sed -i &quot;s|docker.io/t9kpublic|192.168.101.159:5000/t9kpublic|g&quot; \
    ../ks-clusters/additionals/knative/v1.13.1/*
</code></pre>
</aside>
<p>运行以下命令在 K8s 集群中安装 Knative：</p>
<pre><code class="language-bash">kubectl apply -f ../ks-clusters/additionals/knative/v1.13.1/serving-crds.yaml
kubectl apply -f ../ks-clusters/additionals/knative/v1.13.1/serving-core.yaml
kubectl apply -f ../ks-clusters/additionals/knative/v1.13.1/net-istio.yaml
</code></pre>
<h2 id="安装后配置-1"><a class="header" href="#安装后配置-1">安装后配置</a></h2>
<h3 id="修改-config"><a class="header" href="#修改-config">修改 config</a></h3>
<p>修改 knative config-domain 来配置用于推理服务的 domain suffix。</p>
<p>下面的示例会将 domain suffix 设置为 <code>ksvc.sample.t9kcloud.cn</code>：</p>
<pre><code class="language-bash">kubectl patch configmap/config-domain \
  --namespace knative-serving \
  --type merge \
  --patch '{&quot;data&quot;:{&quot;ksvc.sample.t9kcloud.cn&quot;:&quot;&quot;}}'
</code></pre>
<aside class="note">
<div class="title">离线安装</div>
<p>如果你使用的是基于 HTTP 的镜像仓库，则还需要添加额外设置：</p>
<pre><code class="language-bash">kubectl patch configmap/config-deployment \
  --namespace knative-serving \
  --type merge \
  --patch '{&quot;data&quot;:{&quot;registries-skipping-tag-resolving&quot;:&quot;192.168.101.159:5000&quot;}}'
</code></pre>
</aside>
<h3 id="配置-ingress"><a class="header" href="#配置-ingress">配置 Ingress</a></h3>
<p>创建如下所示的 Ingress，需要注意：</p>
<ol>
<li><code>spec.rules[0].host</code> 应该与前述步骤的 domain suffix 一致。</li>
<li>需要配置 DNS，让 <code>*.ksvc.sample.t9kcloud.cn</code> 能够映射到 ingress 所在的节点的 IP。</li>
</ol>
<pre><code class="language-bash">kubectl -n istio-system create -f ../ks-clusters/additionals/knative/v1.13.1/ingress.yaml

# verify it's there
kubectl -n istio-system get ing/t9k.serving
</code></pre>
<h2 id="验证-1"><a class="header" href="#验证-1">验证</a></h2>
<p>确认 pods running：</p>
<pre><code class="language-bash">kubectl -n knative-serving get pods
</code></pre>
<p>输出：</p>
<pre><code class="language-console">NAME                                     READY   STATUS    RESTARTS   AGE
activator-5cc89f4c4d-w6hdz               1/1     Running   0          6m44s
autoscaler-6fb596f4bb-vw4q8              1/1     Running   0          6m44s
controller-6b5874c54-j5gc4               1/1     Running   0          6m44s
net-istio-controller-777b6b4d89-j7qg4    1/1     Running   0          6m33s
net-istio-webhook-78665d59fd-86kxq       1/1     Running   0          6m33s
webhook-79f8449d8f-8cdc7                 1/1     Running   0          6m40s
</code></pre>
<p>创建一个 knative service 进行测试：</p>
<pre><code class="language-bash">kubectl -n default create -f ../ks-clusters/additionals/knative/v1.13.1/hello-ksvc.yaml
</code></pre>
<p>等待 knative service 就绪：</p>
<pre><code class="language-bash">kubectl -n default get ksvc
</code></pre>
<p>输出：</p>
<pre><code class="language-console">NAME         URL                                                 LATESTCREATED      LATESTREADY        READY   REASON
helloworld   http://helloworld.default.ksvc.sample.t9kcloud.cn   helloworld-00001   helloworld-00001   True 
</code></pre>
<p>使用 curl 进行测试：</p>
<pre><code class="language-bash"># 如果已经创建了域名解析
curl helloworld.default.ksvc.sample.t9kcloud.cn

# 如果尚未创建域名解析
curl -H &quot;Host: helloworld.default.ksvc.sample.t9kcloud.cn&quot; &lt;ingress-nginx-ip&gt;
</code></pre>
<p>输出：</p>
<pre><code class="language-console">Hello World!
</code></pre>
<h2 id="参考-10"><a class="header" href="#参考-10">参考</a></h2>
<p><a href="https://knative.dev/docs/install/yaml-install/serving/install-serving-with-yaml/">https://knative.dev/docs/install/yaml-install/serving/install-serving-with-yaml/</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="metrics-server"><a class="header" href="#metrics-server">Metrics Server</a></h1>
<p>Istio 依赖于  <a target="_blank" rel="noopener noreferrer" href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"> HPA(Horizontal Pod Autoscaler)</a>，而 HPA 依赖 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/metrics-server#kubernetes-metrics-server">kubernetes metrics server</a>，所以需要部署 metrics server。</p>
<h2 id="目的-5"><a class="header" href="#目的-5">目的</a></h2>
<p>确保集群中 Metrics Server 安装正确。</p>
<h2 id="正确性检查"><a class="header" href="#正确性检查">正确性检查</a></h2>
<p>查看 Istio 的 HPA：</p>
<pre><code class="language-bash">kubectl -n istio-system get hpa
</code></pre>
<p>输出：</p>
<pre><code class="language-console">NAME                   REFERENCE                         TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
istio-ingressgateway   Deployment/istio-ingressgateway   67%/80%   1         5         5          221d
istiod                 Deployment/istiod                 13%/80%   1         5         1          221d
</code></pre>
<p>查看 APIService：</p>
<pre><code class="language-bash"># 查看当前版本
kubectl get APIService v1beta1.metrics.k8s.io
</code></pre>
<p>确认 metrics server 可用：</p>
<pre><code class="language-bash">kubectl top node
</code></pre>
<p>输出：</p>
<pre><code>NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
node01    1113m        13%    14691Mi         46%       
node02    1336m        11%    13375Mi         42%       
</code></pre>
<h2 id="参考-11"><a class="header" href="#参考-11">参考</a></h2>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</a></li>
<li><a href="https://github.com/kubernetes-sigs/metrics-server#kubernetes-metrics-server">https://github.com/kubernetes-sigs/metrics-server#kubernetes-metrics-server</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="可选-elastic-search"><a class="header" href="#可选-elastic-search">[可选] Elastic Search</a></h1>
<p>如果使用 Elastic Search 保存集群日志，则需要安装此组件。</p>
<aside class="note">
<div class="title">注意</div>
<p>使用 Elastic Search 保存集群日志的方式已经过时，请安装并使用 <a href="online/k8s-components/./loki.html">Loki</a>。</p>
</aside>
<h2 id="目的-6"><a class="header" href="#目的-6">目的</a></h2>
<p>在 namespace <code>t9k-monitoring</code> 中安装 Elastic Search 以存储集群日志。</p>
<h2 id="安装-2"><a class="header" href="#安装-2">安装</a></h2>
<p>如果 namespace <code>t9k-monitoring</code> 不存在，则需创建：</p>
<pre><code class="language-bash">kubectl create ns t9k-monitoring
</code></pre>
<aside class="note">
<div class="title">离线安装</div>
<p>修改镜像仓库的设置：</p>
<pre><code class="language-bash">cat &gt;&gt; ../ks-clusters/additionals/elasticsearch/master.yaml &lt;&lt; EOF
image: &quot;192.168.101.159:5000/t9kpublic/elasticsearch&quot;
EOF

cat &gt;&gt; ../ks-clusters/additionals/elasticsearch/client.yaml &lt;&lt; EOF
image: &quot;192.168.101.159:5000/t9kpublic/elasticsearch&quot;
EOF

cat &gt;&gt; ../ks-clusters/additionals/elasticsearch/data.yaml &lt;&lt; EOF
image: &quot;192.168.101.159:5000/t9kpublic/elasticsearch&quot;
EOF

cat &gt;&gt; ../ks-clusters/additionals/elasticsearch/single.yaml &lt;&lt; EOF
image: &quot;192.168.101.159:5000/t9kpublic/elasticsearch&quot;
EOF
</code></pre>
</aside>
<h3 id="多节点集群"><a class="header" href="#多节点集群">多节点集群</a></h3>
<p>多节点 K8s 集群中的安装，选择下列一种方式。</p>
<p>在线安装：</p>
<pre><code class="language-bash"># online installation
helm install elasticsearch-master \
  oci://tsz.io/t9kcharts/elasticsearch \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/elasticsearch/master.yaml

helm install elasticsearch-client \
  oci://tsz.io/t9kcharts/elasticsearch \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/elasticsearch/client.yaml

helm install elasticsearch-data \
  oci://tsz.io/t9kcharts/elasticsearch \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/elasticsearch/data.yaml

</code></pre>
<p>离线安装：</p>
<pre><code class="language-bash"># offline install
helm install elasticsearch-master \
  ../ks-clusters/tools/offline-additionals/charts/elasticsearch-7.13.4.tgz \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/elasticsearch/master.yaml

helm install elasticsearch-client \
  ../ks-clusters/tools/offline-additionals/charts/elasticsearch-7.13.4.tgz \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/elasticsearch/client.yaml

helm install elasticsearch-data \
  ../ks-clusters/tools/offline-additionals/charts/elasticsearch-7.13.4.tgz \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/elasticsearch/data.yaml
</code></pre>
<p>其中 Helm Chart 的来源参考：<a href="online/k8s-components/../../appendix/modify-helm-chart.html#elastic-search">Elastic Search 的 Helm Chart 修改</a></p>
<h3 id="单节点安装"><a class="header" href="#单节点安装">单节点安装</a></h3>
<p>选择下列一种方式。</p>
<p>在线安装：</p>
<pre><code class="language-bash"># online installation
helm install elasticsearch-single \
  oci://tsz.io/t9kcharts/elasticsearch \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/elasticsearch/single.yaml
</code></pre>
<p>离线安装：</p>
<pre><code># offline install
helm install elasticsearch-single \
  ../ks-clusters/tools/offline-additionals/charts/elasticsearch-7.13.4.tgz \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/elasticsearch/single.yaml
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>单节点安装方式仅在只有一个 K8s 节点的测试场景中适用。</p>
</aside>
<h2 id="验证-2"><a class="header" href="#验证-2">验证</a></h2>
<p>以多节点安装为例，查看 helm status：</p>
<pre><code class="language-bash">helm status -n t9k-monitoring elasticsearch-master

helm status -n t9k-monitoring elasticsearch-client

helm status -n t9k-monitoring elasticsearch-data
</code></pre>
<p>以多节点安装为例，确认 elasticsearch Pod 正常运行：</p>
<pre><code class="language-bash">kubectl get pods --namespace=t9k-monitoring -l app=elasticsearch-master
</code></pre>
<p>输出：</p>
<pre><code>NAME                     READY   STATUS    RESTARTS   AGE
elasticsearch-master-0   1/1     Running   0          64d
elasticsearch-master-1   1/1     Running   0          105d
elasticsearch-master-2   1/1     Running   0          11d
</code></pre>
<pre><code class="language-bash">kubectl get pods --namespace=t9k-monitoring -l app=elasticsearch-client
</code></pre>
<p>输出：</p>
<pre><code>NAME                     READY   STATUS    RESTARTS   AGE
elasticsearch-client-0   1/1     Running   0          11d
elasticsearch-client-1   1/1     Running   0          64d
</code></pre>
<pre><code class="language-bash">kubectl get pods --namespace=t9k-monitoring -l app=elasticsearch-data
</code></pre>
<p>输出：</p>
<pre><code>NAME                   READY   STATUS    RESTARTS   AGE
elasticsearch-data-0   1/1     Running   0          132d
elasticsearch-data-1   1/1     Running   0          11d
elasticsearch-data-2   1/1     Running   0          64d
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>在 Post Install 流程中，我们还需要为 Elasticsearch 配置 Index。</p>
</aside>
<h2 id="参考-12"><a class="header" href="#参考-12">参考</a></h2>
<p><a href="https://github.com/elastic/elasticsearch">https://github.com/elastic/elasticsearch</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="loki"><a class="header" href="#loki">Loki</a></h1>
<p>如果使用 Loki 保存集群日志，则需要安装此组件。</p>
<h2 id="目的-7"><a class="header" href="#目的-7">目的</a></h2>
<p>在 namespace <code>t9k-monitoring</code> 中安装 Loki 以存储集群日志。</p>
<h2 id="安装-3"><a class="header" href="#安装-3">安装</a></h2>
<p>如果 namespace <code>t9k-monitoring</code> 不存在，则需创建：</p>
<pre><code class="language-bash">kubectl create ns t9k-monitoring
</code></pre>
<aside class="note">
<div class="title">离线安装</div>
<p>修改镜像仓库的设置：</p>
<pre><code class="language-bash">sed -i -e 's/docker.io/192.168.101.159:5000/' ../ks-clusters/additionals/loki/loki.yaml
sed -i -e 's/docker.io/192.168.101.159:5000/' ../ks-clusters/additionals/loki/loki-single.yaml
sed -i -e 's/docker.io/192.168.101.159:5000/' ../ks-clusters/additionals/loki/promtail.yaml
</code></pre>
</aside>
<h3 id="values-配置"><a class="header" href="#values-配置">Values 配置</a></h3>
<h4 id="存储"><a class="header" href="#存储">存储</a></h4>
<p>参考 <a href="https://github.com/grafana/loki/blob/v3.0.0/production/helm/loki/values.yaml#L272">Loki Values</a>，Loki 支持 s3、gcs 等多种日志存储方式，我们一般使用 S3 存储。</p>
<p>在使用 S3 数据库存储时，需要提前创建好 Loki 所需 Bucket：loki-chunks、loki-ruler 和 loki-admin。如果需要修改 Bucket 名称，请修改 <code>../ks-clusters/additionals/loki/loki.yaml</code> 中的 <code>loki.storage.bucketName</code> 字段。同时，据实填写 <code>loki.storage.s3</code> 中的字段。</p>
<p>Loki 支持在未提前创建数据库的情况下部署，Loki 会自动部署 Minio 并在其中创建好对应的 Bucket（参考 <code>../ks-clusters/additionals/loki/loki-single.yaml</code> 中的 <code>minio</code> 字段）。</p>
<h4 id="t9k-审计日志"><a class="header" href="#t9k-审计日志">T9k 审计日志</a></h4>
<p>如果想启用 <a href="online/k8s-components/../products/pre-install/t9k-monitoring.html#%E5%90%AF%E7%94%A8-t9k-%E5%AE%A1%E8%AE%A1%E6%97%A5%E5%BF%97">T9k 审计日志</a>，请确保 <code>../ks-clusters/additionals/loki/promtail.yaml</code> 文件的 <code>config.snippets.scrapeConfigs</code> 字段中包含下列内容：</p>
<pre><code class="language-yaml">      # ----------------------------------------------------
      # Collect auditing logs.
      # ----------------------------------------------------
      - job_name: t9k-auditing-logs
        static_configs:
          - targets:
              - localhost
            labels:
              __path__: /var/log/kubernetes/audit/*log       
        pipeline_stages:
          - json:
              expressions:
                object_resources: objectRef.resource
                requestURI: requestURI
          - drop:
              source: requestURI
              expression: &quot;.*dryRun=.*&quot;
          - labels:
              object_resources:
          - match:
              selector: '{object_resources=&quot;proxyoperations&quot;}'
              stages:
                - json:
                    expressions:
                      requestObject: requestObject
                      spec: requestObject.spec
                      verb: requestObject.spec.verb
                      involvedObject: requestObject.spec.involvedObject
                      timestamp: requestReceivedTimestamp
                - labels:
                    involvedObject:
                    verb:
                - static_labels:
                    type: t9k_operations
                    level: info
                - timestamp:
                    source: timestamp
                    format: RFC3339 
                - output:
                    source: spec
          - match:
              selector: '{object_resources!=&quot;proxyoperations&quot;}'
              stages:
                - json:
                    expressions:
                      verb: verb
                      object_apiGroup: objectRef.apiGroup
                      object_resources: objectRef.resource
                      object_namespace: objectRef.namespace
                      response_code: responseStatus.code
                      timestamp: requestReceivedTimestamp
                - labels:
                    object_namespace:
                    object_apiGroup:
                    object_resources:
                    response_code:
                    verb:
                - static_labels:
                    type: k8s_objects
                - timestamp:
                    source: timestamp
                    format: RFC3339
                - match:
                    selector: '{response_code=~&quot;1.+|2.+|3.+&quot;}'
                    stages:
                      - static_labels:
                          level: info
                - match:
                    selector: '{response_code!~&quot;1.+|2.+|3.+&quot;}'
                    stages:
                      - static_labels:
                          level: warning
                - labeldrop:
                    - response_code
          - tenant:
              value: t9k-auditing-logs
</code></pre>
<h3 id="多节点集群-1"><a class="header" href="#多节点集群-1">多节点集群</a></h3>
<p>多节点 K8s 集群中的安装，选择下列一种方式。</p>
<p>在线安装：</p>
<pre><code class="language-bash">helm install loki \
  oci://tsz.io/t9kcharts/loki --version 6.6.4 \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/loki/loki.yaml 

helm install promtail \
  oci://tsz.io/t9kcharts/promtail --version 6.16.2 \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/loki/promtail.yaml
</code></pre>
<p>离线安装：</p>
<pre><code class="language-bash">helm install loki \
  ../ks-clusters/tools/offline-additionals/charts/loki-6.6.4.tgz \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/loki/loki.yaml 

helm install promtail \
  ../ks-clusters/tools/offline-additionals/charts/promtail-6.16.2.tgz \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/loki/promtail.yaml
</code></pre>
<h3 id="单节点安装-1"><a class="header" href="#单节点安装-1">单节点安装</a></h3>
<p>选择下列一种方式。</p>
<p>在线安装：</p>
<pre><code class="language-bash">helm install loki \
  oci://tsz.io/t9kcharts/loki --version 6.6.4 \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/loki/loki-single.yaml 

helm install promtail \
  oci://tsz.io/t9kcharts/promtail --version 6.16.2 \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/loki/promtail.yaml
</code></pre>
<p>离线安装：</p>
<pre><code class="language-bash">helm install loki \
  ../ks-clusters/tools/offline-additionals/charts/loki-6.6.4.tgz \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/loki/loki-single.yaml 

helm install promtail \
  ../ks-clusters/tools/offline-additionals/charts/promtail-6.16.2.tgz \
  -n t9k-monitoring \
  -f ../ks-clusters/additionals/loki/promtail.yaml
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>单节点安装方式仅在只有一个 K8s 节点的测试场景中适用。</p>
</aside>
<h2 id="验证-3"><a class="header" href="#验证-3">验证</a></h2>
<p>以多节点安装为例，查看 helm status：</p>
<pre><code class="language-bash">helm status -n t9k-monitoring loki

helm status -n t9k-monitoring promtail
</code></pre>
<p>以多节点安装为例，确认 Pod 正常运行：</p>
<pre><code class="language-bash">% kubectl get pods -n t9k-monitoring
</code></pre>
<p>输出：</p>
<pre><code>NAME                            READY   STATUS    RESTARTS   AGE
loki-0                          1/1     Running   0          156m
loki-canary-jjz5d               1/1     Running   0          156m
loki-chunks-cache-0             2/2     Running   0          156m
loki-gateway-59b665996c-xf4c9   1/1     Running   0          156m
loki-minio-0                    1/1     Running   0          156m
loki-results-cache-0            2/2     Running   0          156m
promtail-76cpg                  1/1     Running   0          3h56m
</code></pre>
<h2 id="参考-13"><a class="header" href="#参考-13">参考</a></h2>
<p><a href="https://grafana.com/docs/loki/latest/get-started/overview/">https://grafana.com/docs/loki/latest/get-started/overview/</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="监控相关"><a class="header" href="#监控相关">监控相关</a></h1>
<p>为了使监控系统正常工作，还需要创建额外的 K8s 资源。</p>
<h2 id="目的-8"><a class="header" href="#目的-8">目的</a></h2>
<p>确保系统 namespace <code>kube-system</code> 中存在 <code>svc/kube-scheduler svc/kube-controller-manager</code>。 </p>
<h2 id="kube-system-service"><a class="header" href="#kube-system-service">kube-system service</a></h2>
<p>在创建之前，请先确认系统中是否已经存在相应的 service。以下展示的 k8s cluster，由于已经创建了相应的 Service，则无需创建。</p>
<pre><code class="language-bash">kubectl -n kube-system get svc/kube-scheduler svc/kube-controller-manager
</code></pre>
<pre><code class="language-console">NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE
kube-scheduler            ClusterIP   10.233.18.162   &lt;none&gt;        10259/TCP   39m
kube-controller-manager   ClusterIP   10.233.19.17    &lt;none&gt;        10257/TCP   32m
</code></pre>
<p>如果并无上述 service， 则可手工创建：</p>
<pre><code class="language-bash">kubectl apply -n kube-system -f ../ks-clusters/additionals/monitoring/kube-system-svc.yaml
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="gatekeeper"><a class="header" href="#gatekeeper">Gatekeeper</a></h1>
<p>集群管理模块（<code>t9k-cluster-admin</code>） 依赖于 Gatekeeper，因此在安装 <code>t9k-cluster-admin</code> 之前，应当预先安装 Gatekeeper。</p>
<h2 id="目的-9"><a class="header" href="#目的-9">目的</a></h2>
<p>安装 Gatekeeper 于 namespace <code>t9k-system</code> 中。</p>
<h2 id="安装-4"><a class="header" href="#安装-4">安装</a></h2>
<p>创建 namespace：</p>
<pre><code class="language-bash">kubectl create ns t9k-system
</code></pre>
<p>确保 namespace <code>t9k-system </code> 的 label <code>kubernetes.io/metadata.name</code> 存在：</p>
<pre><code class="language-bash">kubectl get ns t9k-system -o jsonpath='{.metadata.labels}'
</code></pre>
<pre><code>{&quot;kubernetes.io/metadata.name&quot;:&quot;t9k-system&quot;}
</code></pre>
<p>如不存在此 label，需要手工创建它：</p>
<pre><code class="language-bash">kubectl label ns  t9k-system kubernetes.io/metadata.name=t9k-system
</code></pre>
<p>运行以下命令安装 gatekeeper，2 选 1。</p>
<ol>
<li>
<p>可直接访问 registry</p>
<pre><code class="language-bash"># For K8s v1.24 or v1.25
helm -n t9k-system install t9k-gatekeeper oci://tsz.io/t9kcharts/gatekeeper \
  --version 3.11.0 \
  -f ../ks-clusters/additionals/gatekeeper/values.yaml

# For K8s v1.22
helm -n t9k-system install t9k-gatekeeper oci://tsz.io/t9kcharts/gatekeeper \
  --version 3.11.0-1 \
  -f ../ks-clusters/additionals/gatekeeper/values.yaml
</code></pre>
</li>
<li>
<p>离线安装</p>
<aside class="note">
 <div class="title">离线安装</div>
<p>修改镜像仓库的设置，示例为 <code>192.168.101.159:5000</code>：</p>
<pre><code class="language-bash">sed -i &quot;s|docker.io/t9kpublic|192.168.101.159:5000/t9kpublic|g&quot; \
  ../ks-clusters/additionals/gatekeeper/values.yaml
</code></pre>
</aside>
<pre><code class="language-bash"># offline install for K8s v1.24 or v1.25 
helm -n t9k-system install t9k-gatekeeper \
  ../ks-clusters/tools/offline-additionals/charts/gatekeeper-3.11.0.tgz \
  -f ../ks-clusters/additionals/gatekeeper/values.yaml

# offline install for K8s v1.22
helm -n t9k-system install t9k-gatekeeper \
  ../ks-clusters/tools/offline-additionals/charts/gatekeeper-3.11.0-1.tgz \
  -f ../ks-clusters/additionals/gatekeeper/values.yaml
</code></pre>
</li>
</ol>
<p>等待约 1-3 分钟，gatekeeper 安装完成后会返回信息。</p>
<h2 id="验证-4"><a class="header" href="#验证-4">验证</a></h2>
<p>查看状态：</p>
<pre><code class="language-bash">helm status -n t9k-system t9k-gatekeeper 
</code></pre>
<pre><code class="language-console">NAME: t9k-gatekeeper
LAST DEPLOYED: Tue Nov  7 13:05:55 2023
NAMESPACE: t9k-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
</code></pre>
<p>确认 Gatekeeper Pod 运行正常：</p>
<pre><code class="language-bash">kubectl -n t9k-system get pod -l app=gatekeeper
</code></pre>
<pre><code class="language-console">NAME                                           READY STATUS RESTARTS AGE
gatekeeper-audit-549bcc6775-2fcd8              1/1 Running 1 (2m22s ago) 2m29s
gatekeeper-controller-manager-7997dc9df8-kmnlk 1/1 Running 0 2m29s
gatekeeper-controller-manager-7997dc9df8-phx8s 1/1 Running 0 2m29s
gatekeeper-controller-manager-7997dc9df8-snr7g 1/1 Running 0 2m29s
</code></pre>
<p>查看日志：</p>
<pre><code class="language-bash"># audit controller
kubectl -n t9k-system logs -l app=gatekeeper,control-plane=audit-controller --tail=50

# controller-manager, multiple pods
kubectl -n t9k-system logs deploy/gatekeeper-controller-manager --tail=50
</code></pre>
<h2 id="参考-14"><a class="header" href="#参考-14">参考</a></h2>
<p><a href="https://github.com/open-policy-agent/gatekeeper">https://github.com/open-policy-agent/gatekeeper</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装硬件支持"><a class="header" href="#安装硬件支持">安装硬件支持</a></h1>
<p>这里提供针对不同硬件厂商的设置/安装信息。</p>
<ul>
<li><a href="hardware/./nvidia/index.html">NVIDIA</a></li>
<li><a href="hardware/./amd/index.html">AMD</a></li>
<li><a href="hardware/./enflame/index.html">燧原 Enflame</a></li>
<li><a href="hardware/./hygon/index.html">海光 Hygon</a></li>
<li><a href="hardware/./huawei/index.html">华为</a></li>
<li><a href="hardware/./iluvatar/index.html">天数智芯 iluvatar</a></li>
<li><a href="hardware/./metax/index.html">沐曦 MetaX</a></li>
</ul>
<h2 id="下一步-7"><a class="header" href="#下一步-7">下一步</a></h2>
<p>完成硬件支持之后，就可以 <a href="hardware/../online/products/index.html">安装 TensorStack AI 计算平台</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="nvidia-硬件支持"><a class="header" href="#nvidia-硬件支持">NVIDIA 硬件支持</a></h1>
<p>针对 NVIDIA 的硬件包括如下组件：</p>
<ul>
<li><a href="hardware/nvidia/./gpu-operator.html">NVIDIA GPU Operator</a> -  GPU 驱动程序和 K8s 支持</li>
<li><a href="hardware/nvidia/./network-operator.html">NVIDIA Network Operator</a> - IB 网卡驱动程序和 K8s 支持</li>
</ul>
<h2 id="参考-15"><a class="header" href="#参考-15">参考</a></h2>
<p><a href="https://docs.nvidia.com/datacenter/cloud-native/">https://docs.nvidia.com/datacenter/cloud-native/</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装-nvidia-gpu-operator"><a class="header" href="#安装-nvidia-gpu-operator">安装 NVIDIA GPU Operator</a></h1>
<pre><code>TODO: 1. 支持更多 OS/Kernel 版本组合
</code></pre>
<h2 id="目标"><a class="header" href="#目标">目标</a></h2>
<p>在集群中安装 NVIDIA GPU Operator v24.3.0<sup><a href="hardware/nvidia/gpu-operator.html#参考">[1]</a></sup>，以支持在集群内使用 NVIDIA GPU。</p>
<h2 id="前置条件-1"><a class="header" href="#前置条件-1">前置条件</a></h2>
<p>节点需要满足以下条件：</p>
<ol>
<li>已安装 K8s 集群</li>
<li>集群中含有安装了 NVIDIA GPU 硬件的节点</li>
</ol>
<h2 id="兼容性"><a class="header" href="#兼容性">兼容性</a></h2>
<h3 id="gpu-operator-v2430"><a class="header" href="#gpu-operator-v2430">GPU Operator v24.3.0</a></h3>
<p>GPU Operator v24.3.0 兼容性<sup><a href="hardware/nvidia/gpu-operator.html#参考">[2]</a></sup>如下所示：</p>
<div class="table-wrapper"><table><thead><tr><th>Operating System</th><th>Kubernetes</th><th>Red Hat OpenShift</th><th>VMWare vSphere with Tanzu</th><th>Rancher Kubernetes Engine2</th><th>HPE Ezmeral Runtime Enterprise</th><th>Canonical MicroK8s</th></tr></thead><tbody>
<tr><td>Ubuntu 20.04 LTS</td><td>1.22—1.30</td><td></td><td>7.0 U3c, 8.0 U2</td><td>1.22—1.30</td><td></td><td></td></tr>
<tr><td>Ubuntu 22.04 LTS</td><td>1.22—1.30</td><td></td><td>8.0 U2</td><td>1.22—1.30</td><td></td><td>1.26</td></tr>
<tr><td>Red Hat Core OS</td><td></td><td>4.12—4.15</td><td></td><td></td><td></td><td></td></tr>
<tr><td>Red Hat Enterprise Linux 8.4,8.6—8.9</td><td>1.22—1.30</td><td></td><td></td><td>1.22—1.30</td><td></td><td></td></tr>
<tr><td>Red Hat Enterprise Linux 8.4, 8.5</td><td></td><td></td><td></td><td></td><td>5.5</td><td></td></tr>
</tbody></table>
</div>
<h3 id="驱动兼容性"><a class="header" href="#驱动兼容性">驱动兼容性</a></h3>
<p>GPU Operator v24.3.0 可以通过在节点上部署 GPU 驱动容器来安装 GPU 驱动， 这种方式安装的驱动版本<sup><a href="hardware/nvidia/gpu-operator.html#参考">[3]</a></sup>有：</p>
<ol>
<li><a href="https://docs.nvidia.com/datacenter/tesla/tesla-release-notes-550-90-07/index.html">550.90.07</a> (推荐)</li>
<li><a href="https://docs.nvidia.com/datacenter/tesla/tesla-release-notes-550-54-15/index.html">550.54.15</a> (默认)</li>
<li><a href="https://docs.nvidia.com/datacenter/tesla/tesla-release-notes-535-183-01/index.html">535.183.01</a></li>
<li><a href="https://docs.nvidia.com/datacenter/tesla/tesla-release-notes-470-256-02/index.html">470.256.02</a></li>
</ol>
<p>目前的 GPU 驱动容器兼容下列系统<sup><a href="hardware/nvidia/gpu-operator.html#参考">[2]</a></sup>：</p>
<ul>
<li>Ubuntu 22.04 LTS, 内核版本 5.15</li>
<li>Ubuntu 20.04 LTS, 内核版本 5.4 和 5.15
如果 GPU 驱动容器无法兼容你的系统，请在节点上<a href="hardware/nvidia/gpu-operator.html#可选-nvidia-驱动">手动安装 GPU 驱动</a>：</li>
</ul>
<h2 id="ansible-脚本安装"><a class="header" href="#ansible-脚本安装">ansible 脚本安装</a></h2>
<h3 id="nvidia-驱动"><a class="header" href="#nvidia-驱动">NVIDIA 驱动</a></h3>
<p>使用 ansible 简化 NVIDIA 驱动的安装。</p>
<p>首先进入 inventory 所在的目录：</p>
<pre><code class="language-bash">cd ~/ansible/$T9K_CLUSTER
</code></pre>
<p>运行以下命令安装 GPU 驱动：</p>
<pre><code class="language-bash">ansible-playbook ../ks-clusters/t9k-playbooks/3-install-gpu-driver.yml \
    -i inventory/inventory.ini \
    --become -K \
    -e nvidia_driver_skip_reboot=false \
    --limit node01,node02
</code></pre>
<aside class="note info">
<div class="title">命令行参数说明</div>
<ul>
<li><code>-e nvidia_driver_skip_reboot=false</code> 参数的作用是在安装驱动完成后，重启安装了 GPU 驱动的节点（这也是默认设置）。</li>
<li><code>--limit node01,node02</code> 参数的作用是限制只在 node01 和 node02 节点上安装 GPU 驱动。</li>
</ul>
</aside>
<p>这个 Playbook 执行的任务包括<a href="hardware/nvidia/gpu-operator.html#%E5%AE%89%E8%A3%85">安装 GPU 驱动</a>，<a href="hardware/nvidia/gpu-operator.html#%E5%85%B3%E9%97%AD-gsp">关闭 GSP</a> 和重启节点。</p>
<h3 id="gpu-operator"><a class="header" href="#gpu-operator">GPU Operator</a></h3>
<p>使用 ansible 简化 GPU Operator 的安装。</p>
<p>首先进入 inventory 所在的目录：</p>
<pre><code class="language-bash">cd ~/ansible/$T9K_CLUSTER
</code></pre>
<p>查看预设的变量：</p>
<pre><code class="language-bash">cat ../ks-clusters/t9k-playbooks/roles/gpu-operator/defaults/main.yml
</code></pre>
<aside class="note info">
<div class="title">自定义版本</div>
<p>请参考 <a target="_blank" rel="noopener noreferrer" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/release-notes.html">GPU Operator Release Notes</a> 来设置变量 <code>nvidia_gpu_operator_version</code>。你需要确保相应版本的 Helm Chart 存在于 <code>nvidia_gpu_operator_charts</code> 中，且相应版本的镜像存在于 <code>nvidia_gpu_operator_image_registry</code> 中。</p>
<p>GPU Operator 会用到许多镜像，你可以通过命令行参数指定这些镜像的版本（后面有实际例子）。部分镜像名称中带有操作系统的后缀，常见的有 <code>ubi8</code> 和 <code>ubuntu20.04</code>。其中 ubi 是 <a target="_blank" rel="noopener noreferrer" href="https://www.redhat.com/en/blog/introducing-red-hat-universal-base-image">Red Hat Universal Base Image</a> 的缩写，ubi8 是 RHEL 8 的基础镜像。我们推荐使用与实际操作系统一致的镜像。</p>
</aside>
<p>通过命令行参数 &quot;-e&quot; 来指定需要修改的变量，运行以下命令安装 GPU Operator：</p>
<pre><code class="language-bash">ansible-playbook ../ks-clusters/t9k-playbooks/3-install-gpu-operator.yml \
    -i inventory/inventory.ini \
    --become -K \
    -e nvidia_gpu_operator_charts=&quot;oci://tsz.io/t9kcharts/gpu-operator&quot;
    -e nvidia_gpu_operator_image_registry=&quot;docker.io/t9kpublic&quot; \
    -e nvidia_node_feature_discovery_repo=&quot;docker.io/t9kpublic/node-feature-discovery&quot; \
    -e nvidia_gpu_operator_version=&quot;v24.3.0&quot; \
    -e nvidia_node_feature_discovery_tag=&quot;v0.15.4&quot; \
    -e device_plugin_version=&quot;v0.15.0&quot; \
    -e enable_install_gpu_driver=false
</code></pre>
<p>这个 Playbook 执行的任务包括 <a href="hardware/nvidia/gpu-operator.html#helm-template">helm template</a>，<a href="hardware/nvidia/gpu-operator.html#%E5%AE%89%E8%A3%85-1">安装 GPU Operator</a>，以及<a href="hardware/nvidia/gpu-operator.html#%E9%85%8D%E7%BD%AE-prometheus">配置 Prometheus</a>。</p>
<aside class="note">
<div class="title">离线安装</div>
<p>修改命令行参数，可以基于本地 Helm Chart 和镜像仓库来安装 GPU Operator：</p>
<pre><code class="language-bash">ansible-playbook ../ks-clusters/t9k-playbooks/3-install-gpu-operator.yml \
    -i inventory/inventory.ini \
    --become -K \
    -e nvidia_gpu_operator_charts=&quot;&lt;path/to/helm-chart&gt;&quot;
    -e nvidia_gpu_operator_image_registry=&quot;&lt;registry&gt;&quot; \
    -e nvidia_node_feature_discovery_repo=&quot;&lt;registry&gt;/node-feature-discovery&quot;
    -e nvidia_gpu_operator_version=&quot;v24.3.0&quot; \
    -e nvidia_node_feature_discovery_tag=&quot;v0.15.4&quot; \
    -e device_plugin_version=&quot;v0.15.0&quot; \
    -e enable_install_gpu_driver=false
</code></pre>
</aside>
<h2 id="手动安装"><a class="header" href="#手动安装">手动安装</a></h2>
<h3 id="可选-nvidia-驱动"><a class="header" href="#可选-nvidia-驱动">[可选] NVIDIA 驱动</a></h3>
<p>你可以选择在节点上手动安装 NVIDIA 驱动，然后再安装 GPU Operator。在下面的演示中，安装的驱动版本是 <code>nvidia-driver-525-server</code>，你可以根据系统兼容性、GPU 硬件兼容性自行选择驱动版本。</p>
<h4 id="安装-5"><a class="header" href="#安装-5">安装</a></h4>
<p>查看节点上 NVIDIA GPU 硬件：</p>
<pre><code class="language-bash">sudo lshw -C display
</code></pre>
<pre><code class="language-console">  *-display                 
       description: VGA compatible controller
       product: GP102 [TITAN X]
       vendor: NVIDIA Corporation
       physical id: 0
       bus info: pci@0000:41:00.0
       version: a1
       width: 64 bits
       clock: 33MHz
       capabilities: pm msi pciexpress vga_controller bus_master cap_list rom
       configuration: driver=nouveau latency=0
       resources: irq:58 memory:ea000000-eaffffff memory:d0000000-dfffffff memory:e0000000-e1ffffff ioport:a000(size=128) memory:c0000-dffff
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>如果计划安装 CUDA Toolkit，也可以使用 <a target="_blank" rel="noopener noreferrer" href="https://developer.nvidia.com/cuda-downloads">CUDA Toolkit 的安装包</a>同时完成 NVIDIA 驱动和 CUDA Toolkit 的安装。</p>
</aside>
<p>安装 NVIDIA 驱动：</p>
<pre><code class="language-bash">sudo apt update
sudo apt list nvidia-driver-*
</code></pre>
<pre><code class="language-console">...
nvidia-driver-515-server/focal-updates,focal-security 515.86.01-0ubuntu0.20.04.2 amd64
nvidia-driver-515/focal-updates,focal-security 515.86.01-0ubuntu0.20.04.1 amd64
nvidia-driver-520-open/focal-updates,focal-security 525.60.11-0ubuntu0.20.04.2 amd64
nvidia-driver-520/focal-updates,focal-security 525.60.11-0ubuntu0.20.04.2 amd64
nvidia-driver-525-open/focal-updates,focal-security 525.60.11-0ubuntu0.20.04.2 amd64
nvidia-driver-525-server/focal-updates,focal-security 525.60.13-0ubuntu0.20.04.1 amd64
...
</code></pre>
<pre><code class="language-bash">sudo apt install -y nvidia-driver-525-server
sudo apt-hold mark nvidia-driver-525-server
sudo reboot
</code></pre>
<p>开启 nvidia persistenced mode：</p>
<blockquote>
<p>NVIDIA 驱动安装后，会在集群内添加 system unit <code>nvidia-persistenced.service</code>，我们需要修改这个 unit 以启用 persistenced mode。</p>
</blockquote>
<pre><code class="language-bash">sudo systemctl status nvidia-persistenced.service
</code></pre>
<pre><code class="language-console">● nvidia-persistenced.service - NVIDIA Persistence Daemon
     Loaded: loaded (/lib/systemd/system/nvidia-persistenced.service; static; vendor preset: enabled)
     Active: active (running) since Wed 2023-08-02 05:11:57 UTC; 1h 33min ago
   Main PID: 2748 (nvidia-persiste)
      Tasks: 1 (limit: 618539)
     Memory: 1.0M
     CGroup: /system.slice/nvidia-persistenced.service
             └─2748 /usr/bin/nvidia-persistenced --user nvidia-persistenced --no-persistence-mode --verbose
</code></pre>
<p>修改 <code>nvidia-persistenced.service</code> 的启动命令，删除 --no-persistence-mode 参数。</p>
<pre><code class="language-bash">cat /lib/systemd/system/nvidia-persistenced.service
</code></pre>
<p>修改后的文件内容：</p>
<pre><code class="language-console">[Unit]
Description=NVIDIA Persistence Daemon
Wants=syslog.target
StopWhenUnneeded=true
Before=systemd-backlight@backlight:nvidia_0.service
[Service]
Type=forking
ExecStart=/usr/bin/nvidia-persistenced --user nvidia-persistenced --verbose
ExecStopPost=/bin/rm -rf /var/run/nvidia-persistenced
</code></pre>
<p>重启 nvidia-persistenced.service，重启之后运行 nvidia-smi 可以发现已经开启 nvidia persistenced mode：</p>
<pre><code class="language-bash">sudo systemctl daemon-reload 
sudo systemctl restart nvidia-persistenced.service

nvidia-smi
</code></pre>
<details><summary><code class="hljs">nvidia-smi output</code></summary>
<pre><code class="language-console">+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A40          On   | 00000000:19:00.0 Off |                    0 |
|  0%   21C    P8    12W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A40          On   | 00000000:1A:00.0 Off |                    0 |
|  0%   22C    P8    20W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A40          On   | 00000000:1B:00.0 Off |                    0 |
|  0%   22C    P8    19W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A40          On   | 00000000:1C:00.0 Off |                    0 |
|  0%   23C    P8    18W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  NVIDIA A40          On   | 00000000:B3:00.0 Off |                    0 |
|  0%   22C    P8    20W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  NVIDIA A40          On   | 00000000:B4:00.0 Off |                    0 |
|  0%   22C    P8    18W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  NVIDIA A40          On   | 00000000:B5:00.0 Off |                    0 |
|  0%   22C    P8    19W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  NVIDIA A40          On   | 00000000:B6:00.0 Off |                    0 |
|  0%   22C    P8    18W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
</details>
<h4 id="关闭-gsp"><a class="header" href="#关闭-gsp">关闭 GSP</a></h4>
<aside class="note">
<div class="title">注意</div>
<p>510.x.x 及之后的 driver，在 nvidia driver bug 未修复前，还需要 <a href="hardware/nvidia/gpu-operator.html#disable-gsp">Disable GSP</a>。</p>
</aside>
<h4 id="验证-5"><a class="header" href="#验证-5">验证</a></h4>
<p>Driver 安装后，可使用 nvidia-smi 查看 GPU 信息：</p>
<pre><code class="language-bash">nvidia-smi -L
</code></pre>
<pre><code class="language-console">GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-2032b4e2-30cb-f9e6-6a8a-fb0204e5b966)
GPU 1: NVIDIA A100-SXM4-80GB (UUID: GPU-656b12e4-119e-322d-3133-8a9c8e0cce83)
GPU 2: NVIDIA A100-SXM4-80GB (UUID: GPU-2c855dbe-1b55-094c-52e0-7382cfa3ea1e)
GPU 3: NVIDIA A100-SXM4-80GB (UUID: GPU-71761943-ea45-5827-0031-02c86c0c8b43)
GPU 4: NVIDIA A100-SXM4-80GB (UUID: GPU-53ddb134-1b6e-9978-e593-c3c7ff844768)
GPU 5: NVIDIA A100-SXM4-80GB (UUID: GPU-28146b32-c3b4-3184-ceb6-57690d90e386)
GPU 6: NVIDIA A100-SXM4-80GB (UUID: GPU-37809c96-b96e-5889-203a-38c24bde66d0)
GPU 7: NVIDIA A100-SXM4-80GB (UUID: GPU-95977b14-3e55-936e-b275-bb0f5bc60b39)
</code></pre>
<p>运行 <a target="_blank" rel="noopener noreferrer" href="https://github.com/NVIDIA/cuda-samples/tree/v12.2/Samples/5_Domain_Specific/p2pBandwidthLatencyTest">P2P Bandwidth Latency Test</a> 以进行进一步的测试。</p>
<p>首先安装 <a target="_blank" rel="noopener noreferrer" href="https://developer.nvidia.com/cuda-downloads">CUDA Toolkit</a>。如果你已经安装了 NVIDIA 驱动，建议根据 <code>nvidia-smi</code> 的结果选择相同的 CUDA Toolkit 版本，例如 <a target="_blank" rel="noopener noreferrer" href="https://developer.nvidia.com/cuda-12-2-0-download-archive">https://developer.nvidia.com/cuda-12-2-0-download-archive</a>，并且在安装过程中不要再次安装 NVIDIA Driver。根据安装后的提示信息设置适当的环境变量。</p>
<p>验证：</p>
<pre><code class="language-bash">nvcc --version
</code></pre>
<pre><code class="language-console">nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Tue_Jun_13_19:16:58_PDT_2023
Cuda compilation tools, release 12.2, V12.2.91
Build cuda_12.2.r12.2/compiler.32965470_0
</code></pre>
<p>然后下载 <a target="_blank" rel="noopener noreferrer" href="https://github.com/NVIDIA/cuda-samples/tree/master">CUDA Samples</a>，并切换到与 CUDA 版本一致的 tag：</p>
<pre><code class="language-bash">git clone https://github.com/NVIDIA/cuda-samples.git &amp;&amp; cd cuda-samples
git checkout tags/v12.2
</code></pre>
<p>安装必要的 Packages：</p>
<pre><code class="language-bash">sudo apt update &amp;&amp; sudo apt-get install -y \
    freeglut3-dev \
    build-essential \
    libx11-dev \
    libxmu-dev \
    libxi-dev \
    libgl1-mesa-glx \
    libglu1-mesa \
    libglu1-mesa-dev \
    libglfw3-dev \
    libgles2-mesa-dev
</code></pre>
<p>编译可执行文件：</p>
<pre><code class="language-bash">cd Samples/5_Domain_Specific/p2pBandwidthLatencyTest
make
</code></pre>
<p>运行测试：</p>
<pre><code class="language-bash">./p2pBandwidthLatencyTest
</code></pre>
<p>输出结果的示例如下：</p>
<pre><code class="language-console">[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]
Device: 0, NVIDIA A100-SXM4-80GB, pciBusID: 4f, pciDeviceID: 0, pciDomainID:0
Device: 1, NVIDIA A100-SXM4-80GB, pciBusID: 52, pciDeviceID: 0, pciDomainID:0
Device: 2, NVIDIA A100-SXM4-80GB, pciBusID: 56, pciDeviceID: 0, pciDomainID:0
Device: 3, NVIDIA A100-SXM4-80GB, pciBusID: 57, pciDeviceID: 0, pciDomainID:0
Device: 4, NVIDIA A100-SXM4-80GB, pciBusID: ce, pciDeviceID: 0, pciDomainID:0
Device: 5, NVIDIA A100-SXM4-80GB, pciBusID: d1, pciDeviceID: 0, pciDomainID:0
Device: 6, NVIDIA A100-SXM4-80GB, pciBusID: d5, pciDeviceID: 0, pciDomainID:0
Device: 7, NVIDIA A100-SXM4-80GB, pciBusID: d6, pciDeviceID: 0, pciDomainID:0
Device=0 CAN Access Peer Device=1
Device=0 CAN Access Peer Device=2
Device=0 CAN Access Peer Device=3
Device=0 CAN Access Peer Device=4
Device=0 CAN Access Peer Device=5
Device=0 CAN Access Peer Device=6
Device=0 CAN Access Peer Device=7
Device=1 CAN Access Peer Device=0
Device=1 CAN Access Peer Device=2
Device=1 CAN Access Peer Device=3
Device=1 CAN Access Peer Device=4
Device=1 CAN Access Peer Device=5
Device=1 CAN Access Peer Device=6
Device=1 CAN Access Peer Device=7
Device=2 CAN Access Peer Device=0
Device=2 CAN Access Peer Device=1
Device=2 CAN Access Peer Device=3
Device=2 CAN Access Peer Device=4
Device=2 CAN Access Peer Device=5
Device=2 CAN Access Peer Device=6
Device=2 CAN Access Peer Device=7
Device=3 CAN Access Peer Device=0
Device=3 CAN Access Peer Device=1
Device=3 CAN Access Peer Device=2
Device=3 CAN Access Peer Device=4
Device=3 CAN Access Peer Device=5
Device=3 CAN Access Peer Device=6
Device=3 CAN Access Peer Device=7
Device=4 CAN Access Peer Device=0
Device=4 CAN Access Peer Device=1
Device=4 CAN Access Peer Device=2
Device=4 CAN Access Peer Device=3
Device=4 CAN Access Peer Device=5
Device=4 CAN Access Peer Device=6
Device=4 CAN Access Peer Device=7
Device=5 CAN Access Peer Device=0
Device=5 CAN Access Peer Device=1
Device=5 CAN Access Peer Device=2
Device=5 CAN Access Peer Device=3
Device=5 CAN Access Peer Device=4
Device=5 CAN Access Peer Device=6
Device=5 CAN Access Peer Device=7
Device=6 CAN Access Peer Device=0
Device=6 CAN Access Peer Device=1
Device=6 CAN Access Peer Device=2
Device=6 CAN Access Peer Device=3
Device=6 CAN Access Peer Device=4
Device=6 CAN Access Peer Device=5
Device=6 CAN Access Peer Device=7
Device=7 CAN Access Peer Device=0
Device=7 CAN Access Peer Device=1
Device=7 CAN Access Peer Device=2
Device=7 CAN Access Peer Device=3
Device=7 CAN Access Peer Device=4
Device=7 CAN Access Peer Device=5
Device=7 CAN Access Peer Device=6

***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.
So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.

P2P Connectivity Matrix
     D\D     0     1     2     3     4     5     6     7
     0	     1     1     1     1     1     1     1     1
     1	     1     1     1     1     1     1     1     1
     2	     1     1     1     1     1     1     1     1
     3	     1     1     1     1     1     1     1     1
     4	     1     1     1     1     1     1     1     1
     5	     1     1     1     1     1     1     1     1
     6	     1     1     1     1     1     1     1     1
     7	     1     1     1     1     1     1     1     1
Unidirectional P2P=Disabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3      4      5      6      7 
     0 1540.93  17.55  18.02  18.12  19.68  21.03  21.01  21.00 
     1  18.17 1539.41  18.01  18.15  19.70  21.02  21.03  21.02 
     2  18.04  18.34 1548.56  18.24  20.26  21.02  20.96  21.00 
     3  18.22  18.38  18.05 1540.93  19.65  19.73  20.96  20.97 
     4  19.77  19.77  19.77  19.74 1386.42  18.14  18.14  18.14 
     5  19.81  19.80  21.01  21.05  18.13 1568.78  18.06  18.12 
     6  19.80  19.77  19.80  20.85  18.15  18.17 1575.10  18.17 
     7  19.68  19.80  19.80  19.76  18.07  18.19  18.17 1579.88 
Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)
   D\D     0      1      2      3      4      5      6      7 
     0 1536.38  20.56  24.18  24.18  18.46  18.55  18.60  18.60 
     1  24.18 1550.10  20.56  24.18  18.59  18.59  18.52  18.42 
     2  24.18  24.18 1550.10  20.56  18.60  18.60  18.60  18.60 
     3  24.18  24.18  24.18 1543.97  18.51  18.54  17.32  18.59 
     4  18.57  18.60  18.58  18.60 1393.84  20.56  24.18  25.01 
     5  18.60  18.58  18.59  18.60  24.53 1587.91  20.56  25.21 
     6  18.59  18.60  18.60  18.60  25.22  25.22 1586.29  20.56 
     7  18.54  18.48  18.34  18.55  24.18  24.18  25.15 1587.91 
Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3      4      5      6      7 
     0 1560.16  19.96  20.23  20.07  29.62  29.64  29.64  29.68 
     1  20.04 1563.28  20.20  20.26  29.74  29.64  29.66  29.67 
     2  20.28  20.31 1602.56  20.25  28.36  28.36  29.60  29.66 
     3  20.16  20.09  20.11 1564.06  28.32  28.34  28.33  28.34 
     4  28.47  28.43  28.45  28.42 1414.03  20.08  20.05  20.05 
     5  27.56  28.46  29.14  29.64  19.93 1601.74  20.11  20.02 
     6  27.52  28.41  29.59  29.58  20.09  20.12 1605.86  20.04 
     7  27.53  27.44  28.12  28.30  20.07  20.12  20.12 1606.68 
Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3      4      5      6      7 
     0 1562.50  41.11  41.11  41.11  37.19  36.79  37.16  37.14 
     1  41.10 1563.28  41.10  41.10  37.15  37.19  37.18  37.12 
     2  41.11  41.10 1564.06  41.11  37.19  37.18  37.14  37.17 
     3  41.10  41.11  41.11 1560.16  37.16  37.17  36.99  37.18 
     4  37.19  37.18  37.19  37.18 1444.75  50.43  50.41  50.26 
     5  37.17  37.15  37.19  37.19  50.40 1595.20  50.43  50.41 
     6  37.16  37.19  37.17  37.19  50.42  50.40 1602.56  41.12 
     7  37.17  37.17  37.18  37.00  50.12  50.42  50.42 1596.83 
P2P=Disabled Latency Matrix (us)
   GPU     0      1      2      3      4      5      6      7 
     0   2.84  20.50  20.47  20.54  21.28  21.49  21.29  20.48 
     1  20.37   2.48  20.54  20.54  21.44  12.71  13.13  21.43 
     2  20.49  17.73   2.33  20.53  20.18  13.08  17.06  15.57 
     3  19.78  20.31  20.47   2.37  20.04  14.41  21.45  21.38 
     4  21.25  21.38  18.55  21.44   2.43  15.51  15.46  17.60 
     5  21.15  12.89  17.77  16.36  18.93   2.29  12.54  19.32 
     6  21.48  14.99  17.08  21.06  17.89  14.23   2.25  18.75 
     7  21.48  21.01  19.98  21.47  20.09  14.09  17.62   2.51 

   CPU     0      1      2      3      4      5      6      7 
     0   2.38   5.89   5.65   5.55   5.96   5.93   5.89   5.73 
     1   5.73   2.24   5.43   5.30   5.69   5.78   5.75   5.65 
     2   5.41   5.32   2.24   5.28   5.64   5.66   5.69   5.58 
     3   5.40   5.29   5.22   2.24   5.68   5.72   5.75   5.57 
     4   5.66   5.49   5.48   5.43   2.36   5.86   5.97   5.89 
     5   5.60   5.49   5.46   5.41   5.77   2.34   5.94   5.79 
     6   5.59   5.50   5.48   5.44   5.82   5.89   2.33   5.84 
     7   5.53   5.45   5.39   5.37   5.77   5.83   5.92   2.31 
P2P=Enabled Latency (P2P Writes) Matrix (us)
   GPU     0      1      2      3      4      5      6      7 
     0   2.85   1.99   1.96   1.99   2.48   2.49   2.48   2.49 
     1   1.74   2.51   1.69   1.67   2.25   2.25   2.23   2.25 
     2   1.79   1.78   2.32   1.79   2.25   2.25   2.31   2.25 
     3   1.78   1.79   1.84   2.38   2.25   2.24   2.24   2.25 
     4   2.30   2.25   2.30   2.26   2.43   1.70   1.70   1.68 
     5   2.26   2.26   2.25   2.25   1.70   2.29   1.70   1.70 
     6   2.27   2.25   2.27   2.26   1.70   1.73   2.24   1.73 
     7   2.27   2.27   2.25   2.30   1.72   1.76   1.72   2.49 

   CPU     0      1      2      3      4      5      6      7 
     0   2.26   1.59   1.63   1.60   1.62   1.60   1.63   1.56 
     1   1.74   2.30   1.65   1.66   1.68   1.69   1.65   1.63 
     2   1.76   1.68   2.34   1.67   1.69   1.64   1.59   1.59 
     3   1.68   1.63   1.65   2.30   1.67   1.64   1.69   1.66 
     4   1.88   1.76   1.79   1.77   2.39   1.75   1.76   1.77 
     5   1.91   1.80   1.80   1.80   1.77   2.38   1.71   1.72 
     6   1.84   1.75   1.76   1.83   1.81   1.79   2.40   1.75 
     7   1.92   1.79   1.79   1.77   1.76   1.79   1.78   2.44 

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
</code></pre>
<h4 id="其他"><a class="header" href="#其他">其他</a></h4>
<pre><code class="language-bash"># 检查安装状态
dpkg -l nvidia-driver-525-server
</code></pre>
<pre><code>Desired=Unknown/Install/Remove/Purge/Hold
| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)
||/ Name                     Version                     Architecture Description
+++-========================-===========================-============-=================================
hi  nvidia-driver-525-server 525.125.06-0ubuntu0.20.04.2 amd64        NVIDIA Server Driver metapackage
</code></pre>
<h3 id="gpu-operator-1"><a class="header" href="#gpu-operator-1">GPU Operator</a></h3>
<h4 id="安装-6"><a class="header" href="#安装-6">安装</a></h4>
<p>运行下列命令即可通过 Helm Chart 安装 GPU Operator</p>
<pre><code class="language-bash">helm repo add nvidia https://helm.ngc.nvidia.com/nvidia 
helm repo update
helm install --wait --generate-name \
    --version v24.3.0 \
    -n gpu-operator --create-namespace \
    nvidia/gpu-operator
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>GPU Operator 安装的组件使用的镜像无法从国内直接访问，如果你的集群节点无法访问外网，请参考<a href="hardware/nvidia/gpu-operator.html#集群节点无法访问外网
">附录-&gt;集群节点无法访问外网</a>，将这些镜像拷贝到国内容器镜像服务中，然后再安装 GPU Operator</p>
</aside>
<h4 id="验证-6"><a class="header" href="#验证-6">验证</a></h4>
<p>GPU Operator 安装完成后，运行下列命令查看安装的组件：</p>
<pre><code class="language-bash">$ kubectl -n gpu-operator get deploy
NAME                                         READY   UP-TO-DATE   AVAILABLE   AGE
gpu-operator                                 1/1     1            1           37d
release-name-node-feature-discovery-gc       1/1     1            1           37d
release-name-node-feature-discovery-master   1/1     1            1           37d
$ kubectl -n gpu-operator get ds
NAME                                         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                                          AGE
gpu-feature-discovery                        2         2         2       2            2           nvidia.com/gpu.deploy.gpu-feature-discovery=true                       37d
nvidia-container-toolkit-daemonset           2         2         2       2            2           nvidia.com/gpu.deploy.container-toolkit=true                           37d
nvidia-dcgm-exporter                         2         2         2       2            2           nvidia.com/gpu.deploy.dcgm-exporter=true                               37d
nvidia-device-plugin-daemonset               2         2         2       2            2           nvidia.com/gpu.deploy.device-plugin=true                               37d
nvidia-driver-daemonset                      0         0         0       0            0           nvidia.com/gpu.deploy.driver=true                                      37d
nvidia-mig-manager                           1         1         1       1            1           nvidia.com/gpu.deploy.mig-manager=true                                 37d
nvidia-operator-validator                    2         2         2       2            2           nvidia.com/gpu.deploy.operator-validator=true                          37d
release-name-node-feature-discovery-worker   10        10        10      10           10          &lt;none&gt;                                                                 37d
</code></pre>
<p>查看 GPU Operator 的配置<sup><a href="hardware/nvidia/gpu-operator.html#参考">[4]</a></sup>：</p>
<pre><code class="language-bash">$ kubectl -n gpu-operator get clusterpolicy cluster-policy  
NAME             STATUS   AGE
cluster-policy   ready    2024-05-21T07:00:15Z
</code></pre>
<h4 id="组件"><a class="header" href="#组件">组件</a></h4>
<p>GPU Operator 会在集群内安装的多个组件<sup><a href="hardware/nvidia/gpu-operator.html#参考">[3]</a></sup>，下面对一些重要的组件进行说明。</p>
<h5 id="全局组件"><a class="header" href="#全局组件">全局组件</a></h5>
<p>Deployment gpu-operator：</p>
<ul>
<li>GPU Operator 的运行主体，他会在集群中部署与 NVIDIA GPU 相关的组件。</li>
<li>如何确认正常工作？Pod 运行正常，并且 NVIDIA GPU 相关的组件已经被部署在集群中。</li>
</ul>
<p>node-feature-discovery（master &amp; worker）：</p>
<ul>
<li>GPU Operator 依赖的第三方组件。运行在所有节点上，检测集群节点的硬件信息、系统信息，并将这些信息记录在节点标签上，这些标签前缀是 feature.node.kubernetes.io/。GPU Operator 依赖 node feature discovery 添加的节点标签。</li>
<li>如何确认正常工作？Pod 运行正常，并且可以在节点上查看到相关的节点标签。</li>
</ul>
<h5 id="nvidia-gpu-节点"><a class="header" href="#nvidia-gpu-节点">NVIDIA GPU 节点</a></h5>
<p>下面的组件只能运行在含有 NVIDIA GPU 的节点上</p>
<p><a href="https://github.com/NVIDIA/gpu-feature-discovery">gpu-feature-discovery</a>：</p>
<ul>
<li>根据节点上的 GPU 信息来生成节点标签，标签前缀是 nvidia.com/</li>
<li>如何确认正常工作？Pod 运行正常，并且可以在节点上查看到相关的节点标签。</li>
</ul>
<p><a href="https://github.com/NVIDIA/nvidia-container-toolkit">nvidia-container-toolkit</a>:</p>
<ul>
<li>运行在含有 NVIDIA GPU 的节点上，在节点上安装 nvidia container toolkit</li>
<li>如何确认正常工作？Pod 运行正常，并且可以在节点主机上查看到安装的 nvidia container toolkit</li>
</ul>
<p><a href="https://github.com/NVIDIA/dcgm-exporter">nvidia-dcgm-exporter</a>：</p>
<ul>
<li>在节点上安装 dcgm-exporter，将 GPU 的监控数据以 <a href="https://prometheus.io/">Prometheus</a> metrics 形式暴露出来。</li>
<li>如何确认正常工作？Pod 运行正常，并且可以通过 Pod 上 dcgm exporter 服务查询 GPU metrics。</li>
</ul>
<p><a href="https://github.com/NVIDIA/k8s-device-plugin">nvidia-device-plugin</a>：</p>
<ul>
<li>将 NVIDIA GPU 注册为 K8s 扩展资源。</li>
<li>如何确认正常工作？Pod 运行正常，可以在含有 GPU 的节点上查看到 NVIDIA GPU 扩展资源。</li>
</ul>
<pre><code class="language-bash">$ kubectl get node z02 -o json | jq .status.capacity
{
  &quot;cpu&quot;: &quot;32&quot;,
  &quot;ephemeral-storage&quot;: &quot;59643812Ki&quot;,
  &quot;hugepages-1Gi&quot;: &quot;0&quot;,
  &quot;hugepages-2Mi&quot;: &quot;0&quot;,
  &quot;memory&quot;: &quot;131942876Ki&quot;,
  &quot;nvidia.com/gpu&quot;: &quot;1&quot;,
  &quot;pods&quot;: &quot;110&quot;,
  &quot;tensorstack.dev/test&quot;: &quot;100&quot;
}
</code></pre>
<p>nvidia-driver-daemonset</p>
<ul>
<li>nvidia-driver-daemonset 只会运行在没有安装 GPU 驱动的节点上，作用是为节点安装 GPU 驱动容器。nvidia-driver-daemonset 中运行了下列两个组件：
<ul>
<li><a href="https://github.com/NVIDIA/k8s-driver-manager">k8s-driver-manager</a>：为 GPU Driver Container 的安装做准备工作。</li>
<li><a href="https://github.com/NVIDIA/gpu-driver-container">GPU Driver Container</a>：通过容器提供 NVIDIA GPU 驱动。</li>
</ul>
</li>
<li>如何确认正常工作？GPU 驱动容器可以正常运行在未安装 GPU 驱动的节点上。</li>
</ul>
<p><a href="https://github.com/NVIDIA/mig-parted">nvidia-mig-manager</a></p>
<ul>
<li>只会运行在 GPU 支持 <a href="https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html">MIG</a> 模式的节点上，作用是支持以 MIG 形式共享 GPU。具体地，当节点启用 MIG GPU 共享模式时，nvidia-mig-manager 会根据配置将一个 MIG GPU 划分为多个 <a href="https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#concepts:~:text=A%20GPU%20Instance,number%20of%20SMs.">MIG GPU 实例</a>。</li>
<li>如何确认正常工作？Pod 运行正常</li>
</ul>
<p><a href="https://github.com/NVIDIA/gpu-operator/tree/v24.3.0/validator">nvidia-operator-validator</a></p>
<ul>
<li>验证 GPU Operator 的多个组件是否正常工作。</li>
<li>如何确认正常工作？Pod 运行正常，Pod 日志显示 all validations are successful。</li>
</ul>
<h2 id="安装后配置-2"><a class="header" href="#安装后配置-2">安装后配置</a></h2>
<h3 id="设置-time-slicing"><a class="header" href="#设置-time-slicing">设置 time-slicing</a></h3>
<p>GPU Operator 安装完成后，可以通过以下设置，让 GPU 以 <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/gpu-sharing.html#configuration-for-shared-access-to-gpus-with-gpu-time-slicing">time-slicing 方式</a>被共享使用。</p>
<h4 id="gpu-operator-配置"><a class="header" href="#gpu-operator-配置">GPU Operator 配置</a></h4>
<p>首先需要配置 GPU Operator 启用 time-slicing。</p>
<p>创建 <code>config.yaml</code> 文件来定义 time-slicing config。</p>
<details><summary><code class="hljs">config.yaml</code></summary>
<pre><code class="language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
 name: time-slicing-config
 namespace: gpu-operator
data:
   a100-40gb: |-
       version: v1
       sharing:
         timeSlicing:
           renameByDefault: true
           resources:
           - name: nvidia.com/gpu
             replicas: 8
           - name: nvidia.com/mig-1g.5gb
             replicas: 2
           - name: nvidia.com/mig-2g.10gb
             replicas: 2
           - name: nvidia.com/mig-3g.20gb
             replicas: 3
           - name: nvidia.com/mig-7g.40gb
             replicas: 7
   common: |-
       version: v1
       sharing:
         timeSlicing:
           renameByDefault: true
           resources:
           - name: nvidia.com/gpu
             replicas: 4
</code></pre>
</details>
<p>在本示例中，ConfigMap 定义了 2 个 time-slicing config（config 设置<a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/gpu-sharing.html#configuration-for-shared-access-to-gpus-with-gpu-time-slicing">参考</a>）：<code>a100-40gb</code> 和 <code>common</code>。</p>
<p>创建 ConfigMap time-slicing-config：</p>
<pre><code class="language-bash">kubectl create -f config.yaml
</code></pre>
<p>然后修改 GPU Operator 配置：</p>
<pre><code class="language-bash">kubectl patch clusterpolicy/cluster-policy \
  -n gpu-operator --type merge \
  -p '{&quot;spec&quot;: {&quot;devicePlugin&quot;: {&quot;config&quot;: {&quot;name&quot;: &quot;time-slicing-config&quot;}}}}'
</code></pre>
<h4 id="节点设置-time-slicing"><a class="header" href="#节点设置-time-slicing">节点设置 time-slicing</a></h4>
<p>GPU Operator 启用 time-slicing 后，你可以在 GPU 节点上添加标签 <code>nvidia.com/device-plugin.config=&lt;config-name&gt;</code> 来表明想要将这个节点上的 GPU 以共享形式提供给 K8s Pod 使用，<config-name> 表明节点使用的 time-slicing config，对应 ConfigMap time-slicing-config 中定义的 time-slicing config 的名称（a100-40gb 或 common）。</p>
<p>例如：</p>
<pre><code class="language-bash">kubectl label node z02 nvidia.com/device-plugin.config=common
</code></pre>
<p>z02 上只有一个物理 GPU，在节点上设置了 time-slicing config common 后，你可以看见 4 个 K8s GPU extended-resources:</p>
<pre><code class="language-bash">$ kubectl get node z02 -o json | jq .status.capacity
{
  …
  &quot;nvidia.com/gpu.shared&quot;: &quot;4&quot;,
  …
}
</code></pre>
<h4 id="optional-设置-t9k-scheduler-queue"><a class="header" href="#optional-设置-t9k-scheduler-queue">(optional) 设置 T9k Scheduler Queue</a></h4>
<p>设置了共享 GPU 的节点后，你可以创建 T9k Scheduler Queue <code>shared-gpu</code>，让 Queue 中的 Pod 只运行在共享 GPU 节点上。如果你想要创建使用共享 GPU 的 Pod，你只需要在 Pod 中设置扩展资源 <code>nvidia.com/gpu.shared</code>，并设置 Pod 使用 <code>t9k-scheduler</code>，同时指定 Pod Queue 为 <code>shared-gpu</code> 即可。</p>
<p>Queue 的 YAML 示例如下：</p>
<pre><code class="language-yaml">apiVersion: scheduler.tensorstack.dev/v1beta1
kind: Queue
metadata:
 name: shared-gpu
 namespace: t9k-system
spec:
 closed: false
 nodeSelector:
   matchExpressions:
   - key: nvidia.com/device-plugin.config
     operator: Exists
 preemptible: false
 priority: 80
 quota:
   requests:
     cpu: &quot;400&quot;
     memory: 800Gi
     nvidia.com/gpu.shared: &quot;12&quot;
</code></pre>
<h3 id="节点禁用-gpu-operator"><a class="header" href="#节点禁用-gpu-operator">节点禁用 GPU Operator</a></h3>
<p>如果不想让 GPU Operator 运行在某个节点上，可运行下列命令：</p>
<pre><code class="language-bash">kubectl label nodes $NODE nvidia.com/gpu.deploy.operands=false
</code></pre>
<p>参考：<a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operands">https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operands</a></p>
<h2 id="附录"><a class="header" href="#附录">附录</a></h2>
<h3 id="disable-gsp"><a class="header" href="#disable-gsp">Disable GSP</a></h3>
<p>在 NVIDIA Driver 510.x.x 版本之后，会有一个 Bug。当 Driver 产生 &quot;Timeout waiting for RPC from GSP!&quot; 错误时，通过 Disable GSP 可能解决这个错误。</p>
<h4 id="什么是-gsp"><a class="header" href="#什么是-gsp">什么是 GSP？</a></h4>
<blockquote>
<p>Some GPUs include a GPU System Processor (GSP) which can be used to offload GPU initialization and management tasks. This processor is driven by the firmware file <code>/lib/firmware/nvidia/510.39.01/gsp.bin</code>. A few select products currently use GSP by default, and more products will take advantage of GSP in future driver releases.
Offloading tasks which were traditionally performed by the driver on the CPU can improve performance due to lower latency access to GPU hardware internals.</p>
</blockquote>
<h4 id="why-disable-gsp"><a class="header" href="#why-disable-gsp">Why disable GSP</a></h4>
<p>从 510 版本开始，NVIDIA Driver 引入了 GSP Feature，但是他有 Bug。这个 Bug 可能会导致在使用/查询 GPU 时产生错误：&quot;Timeout waiting for RPC from GSP!&quot;（详情：<a href="https://github.com/NVIDIA/open-gpu-kernel-modules/issues/446">https://github.com/NVIDIA/open-gpu-kernel-modules/issues/446</a>）。</p>
<p>关闭 GSP 可以解决上述 Bug。</p>
<h4 id="how-to-disable-gsp"><a class="header" href="#how-to-disable-gsp">How to disable GSP</a></h4>
<p>命令如下：</p>
<pre><code class="language-bash">sudo su -c 'echo options nvidia NVreg_EnableGpuFirmware=0 &gt; /etc/modprobe.d/nvidia-gsp.conf'
sudo update-initramfs -u
sudo reboot
</code></pre>
<p>检查：</p>
<pre><code class="language-bash"># EnableGpuFirmware is 0 means GSP feature is disabled
cat /proc/driver/nvidia/params | grep EnableGpuFirmware
</code></pre>
<pre><code>EnableGpuFirmware: 0
EnableGpuFirmwareLogs: 2
</code></pre>
<h3 id="集群节点无法访问外网"><a class="header" href="#集群节点无法访问外网">集群节点无法访问外网</a></h3>
<p>当你的集群节点无法下载外网的镜像时，你可以参考下面的示例，先将镜像拷贝到国内的容器镜像服务中，然后再安装 gpu operator。下面的示例使用的国内镜像仓库是 tsz.io，请将其替换为你的镜像仓库。</p>
<p>将下列的镜像列表放入文件 <code>image.mirror.txt</code> 中，每一行 # 前面是 GPU Operator 使用的镜像名称，# 后面是你想要复制的容器镜像名称：</p>
<pre><code class="language-text">registry.k8s.io/nfd/node-feature-discovery:v0.15.4#tsz.io/t9kmirror/node-feature-discovery:v0.15.4
nvcr.io/nvidia/gpu-operator:v24.3.0#tsz.io/t9kmirror/gpu-operator:v24.3.0
nvcr.io/nvidia/cuda:12.4.1-base-ubi8#tsz.io/t9kmirror/cuda:12.4.1-base-ubi8
nvcr.io/nvidia/cloud-native/gpu-operator-validator:v24.3.0#tsz.io/t9kmirror/cloud-native/gpu-operator-validator:v24.3.0
nvcr.io/nvidia/driver:550.54.15-ubuntu20.04#tsz.io/t9kmirror/driver:550.54.15-ubuntu20.04
nvcr.io/nvidia/driver:550.54.15-ubuntu22.04#tsz.io/t9kmirror/driver:550.54.15-ubuntu22.04
nvcr.io/nvidia/cloud-native/k8s-driver-manager:v0.6.8#tsz.io/t9kmirror/cloud-native/k8s-driver-manager:v0.6.8
nvcr.io/nvidia/cloud-native/k8s-kata-manager:v0.2.0#tsz.io/t9kmirror/cloud-native/k8s-kata-manager:v0.2.0
nvcr.io/nvidia/cloud-native/vgpu-device-manager:v0.2.6#tsz.io/t9kmirror/cloud-native/vgpu-device-manager:v0.2.6
nvcr.io/nvidia/cloud-native/k8s-cc-manager:v0.1.1#tsz.io/t9kmirror/cloud-native/k8s-cc-manager:v0.1.1
nvcr.io/nvidia/k8s/container-toolkit:v1.15.0-ubuntu20.04#tsz.io/t9kmirror/k8s/container-toolkit:v1.15.0-ubuntu20.04
nvcr.io/nvidia/k8s-device-plugin:v0.15.0#tsz.io/t9kmirror/k8s-device-plugin:v0.15.0
nvcr.io/nvidia/cloud-native/dcgm:3.3.5-1-ubuntu22.04#tsz.io/t9kmirror/cloud-native/dcgm:3.3.5-1-ubuntu22.04
nvcr.io/nvidia/k8s/dcgm-exporter:3.3.5-3.4.1-ubuntu22.04#tsz.io/t9kmirror/k8s/dcgm-exporter:3.3.5-3.4.1-ubuntu22.04
nvcr.io/nvidia/cloud-native/k8s-mig-manager:v0.7.0-ubuntu20.04#tsz.io/t9kmirror/cloud-native/k8s-mig-manager:v0.7.0-ubuntu20.04
nvcr.io/nvidia/kubevirt-gpu-device-plugin:v1.2.7#tsz.io/t9kmirror/kubevirt-gpu-device-plugin:v1.2.7
</code></pre>
<p>然后运行下列脚本完成镜像拷贝</p>
<pre><code class="language-bash">#!/bin/bash

# Specify the file to read
file=&quot;image.mirror.txt&quot;

# Check if the file exists
if [[ -f &quot;$file&quot; ]]; then
   # Read the file line by line
   while IFS= read -r line
   do
       # Print each line
       oldImage=&quot;${line%%#*}&quot;
       newImage=&quot;${line##*#}&quot;
       docker pull $oldImage
       docker tag $oldImage $newImage
       docker push $newImage
   done &lt; &quot;$file&quot;
else
   echo &quot;$file not found.&quot;
fi
</code></pre>
<p>最后运行下列命令安装 GPU Operator：</p>
<pre><code class="language-bash">helm repo add nvidia https://helm.ngc.nvidia.com/nvidia 
helm repo update
helm install --wait --generate-name \
    --version v24.3.0 \
    -n gpu-operator --create-namespace \
 --set &quot;node-feature-discovery.image.repository=tsz.io/t9kmirror/node-feature-discovery&quot;,&quot;node-feature-discovery.image.tag=v0.15.4&quot; \
 --set &quot;validator.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;operator.repository=tsz.io/t9kmirror&quot; \
 --set &quot;driver.repository=tsz.io/t9kmirror&quot; \
 --set &quot;driver.manager.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;toolkit.repository=tsz.io/t9kmirror/k8s&quot; \
 --set &quot;devicePlugin.repository=tsz.io/t9kmirror&quot;,&quot;devicePlugin.version=v0.15.0&quot; \
 --set &quot;dcgm.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;dcgmExporter.repository=tsz.io/t9kmirror/k8s&quot; \
 --set &quot;gfd.repository=tsz.io/t9kmirror&quot; \
 --set &quot;migManager.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;nodeStatusExporter.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;gds.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;vgpuManager.driverManager.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;vgpuDeviceManager.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;vfioManager.repository=tsz.io/t9kmirror&quot; \
 --set &quot;vfioManager.driverManager.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;kataManager.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;sandboxDevicePlugin.repository=tsz.io/t9kmirror&quot; \
 --set &quot;ccManager.repository=tsz.io/t9kmirror/cloud-native&quot; \
    nvidia/gpu-operator
</code></pre>
<h3 id="安装-gpu-operator-其他版本"><a class="header" href="#安装-gpu-operator-其他版本">安装 GPU Operator 其他版本</a></h3>
<p>如果你想安装其他版本的 GPU Operator，运行 helm install 命令时，添加命令行参数 --version 来指定你想要安装的版本。</p>
<pre><code class="language-bash">helm install --wait --generate-name \
    --version &lt;version&gt;\
    -n gpu-operator --create-namespace \
    nvidia/gpu-operator
</code></pre>
<h3 id="升级-gpu-operator"><a class="header" href="#升级-gpu-operator">升级 GPU Operator</a></h3>
<p>参考 NVIDIA GPU Operator <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/upgrade.html#option-1-manually-upgrading-crds">官方文档</a></p>
<aside class="note">
<div class="title">注意</div>
<p>更新可能会导致正在使用 GPU 的工作负载出错。</p>
</aside>
<h3 id="修改组件版本"><a class="header" href="#修改组件版本">修改组件版本</a></h3>
<p>你可以通过 ClusterPolicy 来修改 GPU Operator 组件的版本，但请先确保组件版本与 GPU Operator 版本兼容。</p>
<p>下面是修改 Device Plugin 版本的示例：
首先运行下列命令查看当前的 Device Plugin 版本</p>
<pre><code class="language-bash">$ k get clusterpolicy cluster-policy  -o yaml
spec:
  devicePlugin:
    image: k8s-device-plugin
    imagePullPolicy: IfNotPresent
    repository: tsz.io/t9kmirror
    version: v0.15.0
</code></pre>
<p>然后使用 <code>kubectl edit clusterpolicy cluster-policy</code> 来修改 <code>spec.devicePlugin.version</code> 字段。</p>
<h2 id="参考-16"><a class="header" href="#参考-16">参考</a></h2>
<ul>
<li><a href="https://github.com/NVIDIA/gpu-operator/tree/v24.3.0">NVIDIA GPU Operator/v24.3.0 Github</a></li>
<li><a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-operating-systems-and-kubernetes-platforms">GPU Operator 兼容平台</a></li>
<li><a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#gpu-operator-component-matrix">GPU Operator Component Matrix</a></li>
<li><a href="https://github.com/NVIDIA/gpu-operator/blob/v24.3.0/api/v1/clusterpolicy_types.go#L1669">ClusterPolicy 定义</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装-nvidia-network-operator"><a class="header" href="#安装-nvidia-network-operator">安装 NVIDIA Network Operator</a></h1>
<pre><code>TODO:
    1. GPU operator 的 version 当提供灵活性。
</code></pre>
<h2 id="前提条件-3"><a class="header" href="#前提条件-3">前提条件</a></h2>
<p>硬件：</p>
<ol>
<li>节点上安装了支持 RDMA 的硬件设备，并正确连接了网线。</li>
<li>节点上的 GPU 支持 GPUDirect 功能。</li>
</ol>
<p>软件：</p>
<ol>
<li>节点加载了 kernel module <code>nvidia_peermem</code>。</li>
<li>节点已经加入到 K8s 集群中。</li>
<li>安装了 <a href="hardware/nvidia/./gpu-operator.html">GPU Operator v22.9.2</a>。</li>
<li>安装了 Node Feature Discovery v0.10.1（安装 GPU Operator 时启用了该功能）。</li>
</ol>
<h3 id="验证-7"><a class="header" href="#验证-7">验证</a></h3>
<p>下面提供部分前提条件的验证方式。</p>
<p>节点上安装了支持 RDMA 的硬件设备，这里以 InfiniBand 为例：</p>
<pre><code class="language-bash">lspci -nn | grep -i infini
</code></pre>
<pre><code>98:00.0 Infiniband controller [0207]: Mellanox Technologies MT28908 Family [ConnectX-6] [15b3:101b]
</code></pre>
<p>需要注意这里的 <code>[15b3:101b]</code>，他们分别代表设备供应商代码和设备 ID。</p>
<p>节点正确连接了 IB 网线，例如：</p>
<pre><code class="language-bash">ip a | grep -i ib
</code></pre>
<pre><code>5: ibs102: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 2044 qdisc mq state UP group default qlen 256
    link/infiniband 00:00:0d:86:fe:80:00:00:00:00:00:00:e8:eb:d3:03:00:a6:19:aa brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
    inet 10.20.65.13/24 brd 10.20.65.255 scope global ibs102
</code></pre>
<p>这里的状态可以是 UP 或者 DOWN，但不能有 NO-CARRIER。</p>
<p>节点启用了 kernel module <code>nvidia_peermem</code>：</p>
<pre><code class="language-bash">lsmod | grep nvidia_peermem
</code></pre>
<p>参考输出：</p>
<pre><code>nvidia_peermem         16384  0
ib_core               348160  9 rdma_cm,ib_ipoib,nvidia_peermem,iw_cm,ib_umad,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm
nvidia              56344576  492 nvidia_uvm,nvidia_peermem,nvidia_modeset
</code></pre>
<p>如果未启动，可手动启用：</p>
<pre><code>sudo modprobe nvidia_peermem
</code></pre>
<p>通过供应商代码 (15b3) 查看含有 Mellanox Infinity Band NIC 的节点：</p>
<pre><code class="language-bash">kubectl get nodes  -l feature.node.kubernetes.io/pci-15b3.present
</code></pre>
<pre><code>NAME    STATUS    ROLES                          AGE    VERSION
a101    Ready     compute                        16d    v1.24.10
a102    Ready     compute                        16d    v1.24.10
a31     Ready     compute                        45d    v1.24.10
a41     Ready     compute,ingress                92d    v1.24.10
a42     Ready     compute                        177d   v1.24.10
a43     Ready     compute                        177d   v1.24.10
a44     Ready     compute                        16d    v1.24.10
a45     Ready     compute                        7d3h   v1.24.10
login   Ready     control-plane,ingress,master   178d   v1.24.10
</code></pre>
<p>查看含有 NVIDIA GPU (供应商代码 10de) 的节点：</p>
<pre><code class="language-bash">kubectl get nodes  -l feature.node.kubernetes.io/pci-10de.present
</code></pre>
<pre><code>NAME    STATUS    ROLES             AGE    VERSION
a101    Ready     compute           16d    v1.24.10
a102    Ready     compute           16d    v1.24.10
a31     Ready     compute           45d    v1.24.10
a41     Ready     compute,ingress   92d    v1.24.10
a42     Ready     compute           177d   v1.24.10
a43     Ready     compute           177d   v1.24.10
a44     Ready     compute           16d    v1.24.10
a45     Ready     compute           7d3h   v1.24.10
</code></pre>
<p>确认节点上安装了 GPU Operator v22.9.2，并确认 NFD （GPU Operator 部署的）可以正常运行：</p>
<pre><code class="language-bash">kubectl -n gpu-operator get ds
</code></pre>
<pre><code>NAME                                         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                      AGE
gpu-feature-discovery                        10        10        10      10           10          nvidia.com/gpu.deploy.gpu-feature-discovery=true   173d
nvidia-container-toolkit-daemonset           10        10        10      10           10          nvidia.com/gpu.deploy.container-toolkit=true       173d
nvidia-dcgm-exporter                         10        10        10      10           10          nvidia.com/gpu.deploy.dcgm-exporter=true           173d
nvidia-device-plugin-daemonset               10        10        10      10           10          nvidia.com/gpu.deploy.device-plugin=true           173d
nvidia-mig-manager                           5         5         5       5            5           nvidia.com/gpu.deploy.mig-manager=true             173d
nvidia-operator-validator                    10        10        10      10           10          nvidia.com/gpu.deploy.operator-validator=true      173d
release-name-node-feature-discovery-worker   11        11        11      11           11          &lt;none&gt;                                             173d
</code></pre>
<pre><code class="language-bash">k -n gpu-operator get ds nvidia-operator-validator  -o yaml | grep image:
</code></pre>
<pre><code>                f:image: {}
                f:image: {}
                f:image: {}
                f:image: {}
                f:image: {}
        image: nvcr.io/nvidia/cloud-native/gpu-operator-validator:v22.9.2
        image: nvcr.io/nvidia/cloud-native/gpu-operator-validator:v22.9.2
        image: nvcr.io/nvidia/cloud-native/gpu-operator-validator:v22.9.2
        image: nvcr.io/nvidia/cloud-native/gpu-operator-validator:v22.9.2
        image: nvcr.io/nvidia/cloud-native/gpu-operator-validator:v22.9.2
</code></pre>
<h2 id="安装-7"><a class="header" href="#安装-7">安装</a></h2>
<h3 id="mlnx_ofed-驱动"><a class="header" href="#mlnx_ofed-驱动">MLNX_OFED 驱动</a></h3>
<pre><code class="language-bash"># 进入为此次安装准备的 inventory 目录
cd ~/ansible/$T9K_CLUSTER
</code></pre>
<p>根据以下格式设置 group <code>ib_node</code>，将需要安装 IB 驱动的节点都添加到这个 group 中。</p>
<pre><code class="language-YAML">## install ib driver on ib_node
[ib_node]
a31 ansible_host=x.x.x.x
a41 ansible_host=x.x.x.x
a42 ansible_host=x.x.x.x
</code></pre>
<p>运行脚本安装驱动：</p>
<pre><code class="language-bash">ansible-playbook ../ks-clusters/t9k-playbooks/4-install-ib-driver.yml \
  -i inventory/inventory.ini \
  --become \
  -e &quot;@~/ansible/$T9K_CLUSTER/vault.yml&quot; \
  --vault-password-file=~/ansible/.vault-password.txt
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>本脚本暂不支持离线安装，可以参考 <a href="hardware/nvidia/../../appendix/manually-install-mlnx-ofed-driver.html">附录：手动安装 MLNX_OFED 驱动</a> 来手动安装 MLNX_OFED 驱动。</p>
</aside>
<p>安装后，查看 OFED driver 信息的命令：</p>
<pre><code class="language-bash">ofed_info -s
</code></pre>
<pre><code>MLNX_OFED_LINUX-5.9-0.5.6.0:
</code></pre>
<p>查看节点上的设备信息：</p>
<pre><code class="language-bash">ls /dev/infiniband
</code></pre>
<pre><code>issm0  rdma_cm  umad0  uverbs0
</code></pre>
<h3 id="network-operator"><a class="header" href="#network-operator">Network Operator</a></h3>
<p>运行脚本在 K8s 集群中创建 Network Operator， 选择一种方式：</p>
<pre><code class="language-bash"># 1. 通常的在线安装方式
ansible-playbook ks-clusters/t9k-playbooks/4-install-network-operator.yml \
  -i inventory/inventory.ini \
  --become \
  -e &quot;@~/ansible/$T9K_CLUSTER/vault.yml&quot; \
  --vault-password-file=~/ansible/.vault-password.txt \
  -e rdma_shared_device_name=rdma_shared_device_a \
  -e rdma_shared_device_vendor=15b3 \
  -e rdma_shared_device_id=101b,101d \
  -e network_operator_version=&quot;23.10.0&quot;

# 2. 不使用 ansible vault，而是交互式输入 become password
ansible-playbook ks-clusters/t9k-playbooks/4-install-network-operator.yml \
  -i inventory/inventory.ini \
  --become --ask-become-pass
  -e rdma_shared_device_name=rdma_shared_device_a \
  -e rdma_shared_device_vendor=15b3 \
  -e rdma_shared_device_id=101b \
  -e network_operator_version=&quot;23.10.0&quot;

# 3. 离线安装时，需要根据实际情况
# 设置 network_operator_charts 参数和 network_operator_image_registry 参数
ansible-playbook ks-clusters/t9k-playbooks/4-install-network-operator.yml \
  -i inventory/inventory.ini \
  --become \
  -e &quot;@~/ansible/$T9K_CLUSTER/vault.yml&quot; \
  --vault-password-file=~/ansible/.vault-password.txt \
  -e network_operator_charts=../ks-clusters/tools/offline-additionals/charts/network-operator-23.10.0.tgz \
  -e network_operator_image_registry=192.168.101.159:5000/t9kpublic

</code></pre>
<aside class="note info">
<div class="title">参数说明</div>
<p>命令行中设置的 vars：</p>
<ul>
<li><code>rdma_shared_device_name</code>: Network Operator 在集群中注册的扩展资源的名称，通常不需要修改。</li>
<li><code>rdma_shared_device_vendor</code>: 设备供应商代码，可以通过 <code>lspci -nn | grep -i infini</code> 命令获得。</li>
<li><code>rdma_shared_device_id</code>: 设备 ID，可以通过 <code>lspci -nn | grep -i infini</code> 命令获得。如果集群中的节点中有不同的设备 ID，需要将所有的设备 ID 都添加到设置中（逗号分隔），例如：<code>rdma_shared_device_id=101b,101c,101d</code></li>
<li><code>network_operator_version</code>: 安装的 Network Operator 版本。</li>
<li><code>network_operator_charts</code>: 安装 Network Operator 时使用的 Helm Chart 来源。在离线安装时必须设置。</li>
<li><code>network_operator_image_registry</code>: 安装 Network Operator 时使用的镜像仓库。在离线安装时必须设置。</li>
</ul>
</aside>
<p>安装完成后，通过下述命令查看安装的产品组件：</p>
<pre><code class="language-bash">kubectl -n network-operator get pods -o wide
</code></pre>
<p>通过下述命令查看 Network Operator 配置：</p>
<pre><code class="language-bash">kubectl get  NicClusterPolicy  nic-cluster-policy -o yaml
</code></pre>
<h2 id="验证-8"><a class="header" href="#验证-8">验证</a></h2>
<h3 id="rdma"><a class="header" href="#rdma">RDMA</a></h3>
<p>运行下列命令来创建两个 Pod rdma-test-pod-1 和 rdma-test-pod-2，通过 nodeSelector 来保证他们运行在两个不同的含有 IB NIC 的节点上（需要将 a31 和 a42 替换为你的集群节点）：</p>
<pre><code class="language-bash">kubectl create -f - &lt;&lt; EOF
apiVersion: v1
kind: Pod
metadata:
  name: rdma-test-pod-1
spec:
  nodeSelector:
    # Note: Replace hostname or remove selector altogether
    kubernetes.io/hostname: a31
  restartPolicy: OnFailure
  containers:
  - image: mellanox/rping-test
    name: rdma-test-ctr
    securityContext:
      capabilities:
        add: [ &quot;IPC_LOCK&quot; ]
    resources:
      limits:
        rdma/rdma_shared_device_a: 1
    command:
    - sh
    - -c
    - |
      sleep infinity
EOF
</code></pre>
<pre><code class="language-bash">kubectl create -f - &lt;&lt; EOF
apiVersion: v1
kind: Pod
metadata:
  name: rdma-test-pod-2
spec:
  nodeSelector:
    # Note: Replace hostname or remove selector altogether
    kubernetes.io/hostname: a42
  restartPolicy: OnFailure
  containers:
  - image: mellanox/rping-test
    name: rdma-test-ctr
    securityContext:
      capabilities:
        add: [ &quot;IPC_LOCK&quot; ]
    resources:
      limits:
        rdma/rdma_shared_device_a: 1
    command:
    - sh
    - -c
    - |
      sleep infinity
EOF
</code></pre>
<p>创建完成后，查看 Pod 状态，等 Pod Ready 后进入下一步：</p>
<pre><code class="language-bash">kubectl get pod -o wide | grep rdma-test
</code></pre>
<pre><code>rdma-test-pod-1       1/1     Running     0          69s     10.233.84.218   a31    &lt;none&gt;           &lt;none&gt;
rdma-test-pod-2       1/1     Running     0          31s     10.233.118.5    a42    &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>进入 pod rdma-test-pod-1 和 rdma-test-pod-2 中查看 infiniband 设备文件：</p>
<pre><code class="language-bash">kubectl exec -ti pod/rdma-test-pod-1  -- bash
</code></pre>
<pre><code>[root@rdma-test-pod-1 /]# ls -al /sys/class/infiniband
total 0
drwxr-xr-x  2 root root 0 Aug  9 11:00 .
drwxr-xr-x 84 root root 0 Aug  9 11:00 ..
lrwxrwxrwx  1 root root 0 Aug  9 11:00 mlx5_0 -&gt; ../../devices/pci0000:16/0000:16:02.0/0000:17:00.0/0000:18:04.0/0000:1d:00.0/infiniband/mlx5_0
</code></pre>
<pre><code class="language-bash">kubectl exec -ti pod/rdma-test-pod-2  -- bash
</code></pre>
<pre><code>[root@rdma-test-pod-2 /]# ls -al /sys/class/infiniband
total 0
drwxr-xr-x  2 root root 0 Aug  9 11:02 .
drwxr-xr-x 83 root root 0 Aug  9 11:02 ..
lrwxrwxrwx  1 root root 0 Aug  9 11:02 mlx5_0 -&gt; ../../devices/pci0000:16/0000:16:02.0/0000:17:00.0/0000:18:04.0/0000:1d:00.0/infiniband/mlx5_0
</code></pre>
<p>在 Pod rdma-test-pod-1 和 rdma-test-pod-2 中进行 RDMA 测试。</p>
<p>在 Pod rdma-test-pod-1 中运行 test server：</p>
<pre><code class="language-bash">kubectl exec -ti pod/rdma-test-pod-1  -- bash
</code></pre>
<pre><code>[root@rdma-test-pod-1 /]# ip a show eth0
3: eth0@if5331: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default
    link/ether c6:be:41:50:d5:65 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.233.84.218/32 scope global eth0
       valid_lft forever preferred_lft forever
[root@rdma-test-pod-1 /]# ib_write_bw -a -F --report_gbits
************************************
* Waiting for client to connect... *
************************************
---------------------------------------------------------------------------------------
                    RDMA_Write BW Test
 Dual-port       : OFF		Device         : mlx5_0
 Number of qps   : 1		Transport type : IB
 Connection type : RC		Using SRQ      : OFF
 CQ Moderation   : 100
 Mtu             : 4096[B]
 Link type       : IB
 Max inline data : 0[B]
 rdma_cm QPs	 : OFF
 Data ex. method : Ethernet
---------------------------------------------------------------------------------------
 local address: LID 0x04 QPN 0x01df PSN 0x8627f0 RKey 0x1802e2 VAddr 0x007f6c3c08f000
 remote address: LID 0x03 QPN 0x0027 PSN 0x28ccf6 RKey 0x1805e5 VAddr 0x007fe46b172000
---------------------------------------------------------------------------------------
 #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]
 8388608    5000             98.38              98.38  		   0.001466
</code></pre>
<p>在 Pod rdma-test-pod-2 中运行 test client：</p>
<pre><code class="language-bash">kubectl exec -ti pod/rdma-test-pod-2  -- bash
</code></pre>
<pre><code>[root@rdma-test-pod-2 /]# ib_write_bw -a -F --report_gbits 10.233.84.218
---------------------------------------------------------------------------------------
                    RDMA_Write BW Test
 Dual-port       : OFF		Device         : mlx5_0
 Number of qps   : 1		Transport type : IB
 Connection type : RC		Using SRQ      : OFF
 TX depth        : 128
 CQ Moderation   : 100
 Mtu             : 4096[B]
 Link type       : IB
 Max inline data : 0[B]
 rdma_cm QPs	 : OFF
 Data ex. method : Ethernet
---------------------------------------------------------------------------------------
 local address: LID 0x03 QPN 0x0027 PSN 0x28ccf6 RKey 0x1805e5 VAddr 0x007fe46b172000
 remote address: LID 0x04 QPN 0x01df PSN 0x8627f0 RKey 0x1802e2 VAddr 0x007f6c3c08f000
---------------------------------------------------------------------------------------
 #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]
 2          5000           0.060665            0.059098            3.693631
 4          5000             0.11               0.11   		   3.449869
 8          5000             0.19               0.19   		   2.891324
 16         5000             0.50               0.50   		   3.878476
 32         5000             0.94               0.83   		   3.259774
 64         5000             1.86               1.83   		   3.565836
 128        5000             4.04               3.98   		   3.883562
 256        5000             8.04               7.51   		   3.668285
 512        5000             16.01              15.78  		   3.851526
 1024       5000             31.60              30.90  		   3.772522
 2048       5000             54.11              53.34  		   3.255410
 4096       5000             87.54              86.35  		   2.635233
 8192       5000             98.19              98.01  		   1.495541
 16384      5000             98.29              98.20  		   0.749223
 32768      5000             98.28              98.24  		   0.374774
 65536      5000             98.33              98.32  		   0.187536
 131072     5000             98.36              98.36  		   0.093805
 262144     5000             98.35              98.35  		   0.046895
 524288     5000             98.37              98.37  		   0.023453
 1048576    5000             98.38              98.38  		   0.011728
 2097152    5000             98.35              98.35  		   0.005862
 4194304    5000             98.38              98.38  		   0.002932
 8388608    5000             98.38              98.38  		   0.001466
---------------------------------------------------------------------------------------
</code></pre>
<p>最后删除上述用于测试的 Pods：</p>
<pre><code class="language-bash">kubectl delete pod rdma-test-pod-1 rdma-test-pod-2
</code></pre>
<pre><code>pod &quot;rdma-test-pod-1&quot; deleted
pod &quot;rdma-test-pod-2&quot; deleted
</code></pre>
<h3 id="测试-gpudirect-rdma"><a class="header" href="#测试-gpudirect-rdma">测试 GPUDirect RDMA</a></h3>
<p>运行下列命令来创建两个 Pod rdma-gpu-test-pod-1 和 rdma-gpu-test-pod-2，通过 nodeSelector 来保证他们运行在两个不同的且含有 IB NIC、NVIDIA GPU(GPU 需支持 RDMA) 的节点上（需要将 a101 和 a102 替换为你的集群节点）：</p>
<pre><code class="language-bash">$ kubectl create -f - &lt;&lt; EOF
apiVersion: v1
kind: Pod
metadata:
  name: rdma-gpu-test-pod-1
spec:
  nodeSelector:
    # Note: Replace hostname or remove selector altogether
    kubernetes.io/hostname: a101
  restartPolicy: OnFailure
  containers:
  - image: mellanox/cuda-perftest
    name: rdma-gpu-test-ctr
    securityContext:
      capabilities:
        add: [ &quot;IPC_LOCK&quot; ]
    resources:
      limits:
        nvidia.com/gpu: 1
        rdma/rdma_shared_device_a: 1
EOF
</code></pre>
<pre><code class="language-bash">$ kubectl create -f - &lt;&lt; EOF
apiVersion: v1
kind: Pod
metadata:
  name: rdma-gpu-test-pod-2
spec:
  nodeSelector:
    # Note: Replace hostname or remove selector altogether
    kubernetes.io/hostname: a102
  restartPolicy: OnFailure
  containers:
  - image: mellanox/cuda-perftest
    name: rdma-gpu-test-ctr
    securityContext:
      capabilities:
        add: [ &quot;IPC_LOCK&quot; ]
    resources:
      limits:
        nvidia.com/gpu: 1
        rdma/rdma_shared_device_a: 1
EOF
</code></pre>
<p>创建完成后，查看 Pod 状态，等 Pod Ready 后进入下一步：</p>
<pre><code class="language-bash">kubectl get pod -o wide | grep rdma-gpu
</code></pre>
<pre><code>rdma-gpu-test-pod-1   1/1     Running     0          15s     10.233.120.114   a101    &lt;none&gt;           &lt;none&gt;
rdma-gpu-test-pod-2   1/1     Running     0          12s     10.233.71.205    a102    &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>进入 pod rdma-gpu-test-pod-1 和 rdma-gpu-test-pod-2 中查看 infiniband 设备文件、GPU 设备文件：</p>
<pre><code class="language-bash">kubectl exec -ti rdma-gpu-test-pod-1 -- bash
</code></pre>
<pre><code>root@rdma-gpu-test-pod-1:~# ls -al /sys/class/infiniband
total 0
drwxr-xr-x  2 root root 0 Aug  9 11:18 .
drwxr-xr-x 75 root root 0 Aug  9 11:18 ..
lrwxrwxrwx  1 root root 0 Aug  9 11:18 mlx5_0 -&gt; ../../devices/pci0000:4a/0000:4a:02.0/0000:4b:00.0/0000:4c:04.0/0000:50:00.0/0000:51:10.0/0000:53:00.0/infiniband/mlx5_0
root@rdma-gpu-test-pod-1:~# ls /dev | grep nvidia
nvidia-modeset
nvidia-uvm
nvidia-uvm-tools
nvidia3
nvidiactl
</code></pre>
<pre><code class="language-bash">kubectl exec -ti rdma-gpu-test-pod-2 -- bash
</code></pre>
<pre><code>root@rdma-gpu-test-pod-2:~# ls -al /sys/class/infiniband
total 0
drwxr-xr-x  2 root root 0 Aug  9 11:21 .
drwxr-xr-x 76 root root 0 Aug  9 11:21 ..
lrwxrwxrwx  1 root root 0 Aug  9 11:21 mlx5_0 -&gt; ../../devices/pci0000:4a/0000:4a:02.0/0000:4b:00.0/0000:4c:04.0/0000:50:00.0/0000:51:10.0/0000:53:00.0/infiniband/mlx5_0
root@rdma-gpu-test-pod-2:~# ls /dev | grep nvidia
nvidia-modeset
nvidia-uvm
nvidia-uvm-tools
nvidia6
nvidiactl
</code></pre>
<p>在 Pod rdma-gpu-test-pod-1 和 rdma-gpu-test-pod-2 中进行 GPUDirect RDMA 测试。</p>
<p>在 Pod rdma-gpu-test-pod-1 中运行 test server：</p>
<pre><code class="language-bash">kubectl exec -ti rdma-gpu-test-pod-1 -- bash
</code></pre>
<pre><code>root@rdma-gpu-test-pod-1:~# ip a show eth0
3: eth0@if29: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default
    link/ether 6a:99:3a:2e:82:c7 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.233.120.114/32 scope global eth0
       valid_lft forever preferred_lft forever
root@rdma-gpu-test-pod-1:~# ib_write_bw -a -F --report_gbits -q 2 --use_cuda 0
************************************
* Waiting for client to connect... *
************************************
initializing CUDA
Listing all CUDA devices in system:
CUDA device 0: PCIe address is 57:00
Picking device No. 0
[pid = 69, dev = 0] device name = [NVIDIA A100-SXM4-80GB]
creating CUDA Ctx
making it the current CUDA Ctx
cuMemAlloc() of a 33554432 bytes GPU buffer
allocated GPU buffer address at 00007f2418000000 pointer=0x7f2418000000
---------------------------------------------------------------------------------------
                    RDMA_Write BW Test
 Dual-port       : OFF		Device         : mlx5_0
 Number of qps   : 2		Transport type : IB
 Connection type : RC		Using SRQ      : OFF
 PCIe relax order: ON
 ibv_wr* API     : ON
 CQ Moderation   : 100
 Mtu             : 4096[B]
 Link type       : IB
 Max inline data : 0[B]
 rdma_cm QPs	 : OFF
 Data ex. method : Ethernet
---------------------------------------------------------------------------------------
 local address: LID 0x08 QPN 0x00dc PSN 0xf0a0c8 RKey 0x1fdfd1 VAddr 0x007f2419000000
 local address: LID 0x08 QPN 0x00dd PSN 0x860249 RKey 0x1fdfd1 VAddr 0x007f2419800000
 remote address: LID 0x02 QPN 0x064b PSN 0x21791 RKey 0x1fdfff VAddr 0x007fb29d000000
 remote address: LID 0x02 QPN 0x064c PSN 0x83b8a7 RKey 0x1fdfff VAddr 0x007fb29d800000
---------------------------------------------------------------------------------------
 #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]
 8388608    10000            81.02              80.96  		   0.001206
---------------------------------------------------------------------------------------
deallocating RX GPU buffer 00007f2418000000
destroying current CUDA Ctx
</code></pre>
<p>在 Pod rdma-gpu-test-pod-2 中运行 test client：</p>
<pre><code class="language-bash">kubectl exec -ti rdma-gpu-test-pod-2 -- bash
</code></pre>
<pre><code>root@rdma-gpu-test-pod-2:~# ib_write_bw -a -F --report_gbits -q 2 --use_cuda 0 10.233.120.114
initializing CUDA
Listing all CUDA devices in system:
CUDA device 0: PCIe address is D5:00
Picking device No. 0
[pid = 38, dev = 0] device name = [NVIDIA A100-SXM4-80GB]
creating CUDA Ctx
making it the current CUDA Ctx
cuMemAlloc() of a 33554432 bytes GPU buffer
allocated GPU buffer address at 00007fb29c000000 pointer=0x7fb29c000000
---------------------------------------------------------------------------------------
                    RDMA_Write BW Test
 Dual-port       : OFF		Device         : mlx5_0
 Number of qps   : 2		Transport type : IB
 Connection type : RC		Using SRQ      : OFF
 PCIe relax order: ON
 ibv_wr* API     : ON
 TX depth        : 128
 CQ Moderation   : 100
 Mtu             : 4096[B]
 Link type       : IB
 Max inline data : 0[B]
 rdma_cm QPs	 : OFF
 Data ex. method : Ethernet
---------------------------------------------------------------------------------------
 local address: LID 0x02 QPN 0x064b PSN 0x21791 RKey 0x1fdfff VAddr 0x007fb29d000000
 local address: LID 0x02 QPN 0x064c PSN 0x83b8a7 RKey 0x1fdfff VAddr 0x007fb29d800000
 remote address: LID 0x08 QPN 0x00dc PSN 0xf0a0c8 RKey 0x1fdfd1 VAddr 0x007f2419000000
 remote address: LID 0x08 QPN 0x00dd PSN 0x860249 RKey 0x1fdfd1 VAddr 0x007f2419800000
---------------------------------------------------------------------------------------
 #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]
 2          10000           0.061867            0.060962            3.810117
 4          10000            0.13               0.13   		   3.953929
 8          10000            0.26               0.26   		   4.034568
 16         10000            0.52               0.52   		   4.029406
 32         10000            1.03               1.02   		   4.003723
 64         10000            2.07               2.06   		   4.015258
 128        10000            4.04               3.86   		   3.774170
 256        10000            8.17               8.03   		   3.921990
 512        10000            15.94              15.20  		   3.712094
 1024       10000            32.68              32.54  		   3.972166
 2048       10000            78.28              67.50  		   4.119992
 4096       10000            80.53              74.83  		   2.283757
 8192       10000            80.23              71.90  		   1.097180
 16384      10000            81.66              81.00  		   0.617974
 32768      10000            81.76              81.01  		   0.309037
 65536      10000            81.79              81.24  		   0.154945
 131072     10000            81.66              81.06  		   0.077303
 262144     10000            81.79              80.96  		   0.038603
 524288     10000            81.74              80.98  		   0.019308
 1048576    10000            81.13              80.94  		   0.009649
 2097152    10000            81.11              80.96  		   0.004825
 4194304    10000            81.01              80.97  		   0.002413
 8388608    10000            81.02              80.96  		   0.001206
---------------------------------------------------------------------------------------
deallocating RX GPU buffer 00007fb29c000000
destroying current CUDA Ctx
</code></pre>
<p>最后删除上述用于测试的 Pods：</p>
<pre><code class="language-bash">kubectl delete pod rdma-gpu-test-pod-1 rdma-gpu-test-pod-2
</code></pre>
<pre><code>pod &quot;rdma-gpu-test-pod-1&quot; deleted
pod &quot;rdma-gpu-test-pod-2&quot; deleted
</code></pre>
<h2 id="参考-17"><a class="header" href="#参考-17">参考</a></h2>
<p><a href="https://github.com/Mellanox/network-operator">https://github.com/Mellanox/network-operator</a></p>
<p><a href="https://docs.nvidia.com/networking/display/cokan10/network+operator">https://docs.nvidia.com/networking/display/cokan10/network+operator</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="amd"><a class="header" href="#amd">AMD</a></h1>
<h2 id="参考-18"><a class="header" href="#参考-18">参考</a></h2>
<p><a href="https://github.com/ROCm/k8s-device-plugin">https://github.com/ROCm/k8s-device-plugin</a></p>
<p><a href="https://www.amd.com/en/support/linux-drivers">https://www.amd.com/en/support/linux-drivers</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="燧原"><a class="header" href="#燧原">燧原</a></h1>
<h2 id="简介"><a class="header" href="#简介">简介</a></h2>
<p>在集群内部署下列内容即可实现在 Kubernetes 中使用燧原 GPU。</p>
<h3 id="燧原驱动"><a class="header" href="#燧原驱动">燧原驱动</a></h3>
<p>详情参考 <a href="https://support.enflame-tech.com/onlinedoc_dev_2.5.115/4-develop/kmd/kernel_module_guide/content/source/kernel_module_guide.html#kmd" target="_blank">KMD 用户使用手册</a>。</p>
<h3 id="topscloud-产品"><a class="header" href="#topscloud-产品">TopsCloud 产品</a></h3>
<p>燧原 TopsCloud 用于提供在 K8s 上使用 GCU 的解决方案，包括下面几个组件：</p>
<ul>
<li>资源管理：
<ul>
<li><a href="https://support.enflame-tech.com/onlinedoc_dev_2.5.115/6-k8s/k8s/k8s-device-plugin/content/source/enflame_gcu_k8s_plugin_user_guide.html#k8s-device-plugin" target="_blank">K8s Device Plugin</a>：将节点上的 GCU 硬件注册为 K8s 扩展资源。</li>
<li><a href="https://support.enflame-tech.com/onlinedoc_dev_2.5.115/6-k8s/k8s/GCUshare/content/source/index.html" target="_blank">GCUShare</a> 相关的组件：GCUShare 用于支持多个 Pod 共享 GCU，主要包含下列组件：
<ul>
<li>gcushare-scheduler-extender：kube-scheduler 插件，增加调度器功能，使得调度器可以为使用 share GCU 的 Pod 提供调度支持。</li>
<li>gcushare-device-plugin：将节点上的 GCU 硬件注册为 K8s 扩展资源。</li>
</ul>
</li>
<li><a href="https://support.enflame-tech.com/onlinedoc_dev_2.5.115/6-k8s/k8s/Container-toolkit/content/source/index.html" target="_blank">Container Toolkit</a>：使得容器可以使用 GCU 卡。主要包含一个 container-runtime，可以自动为容器挂载 GCU 设备、注入运行环境。</li>
<li><a href="https://support.enflame-tech.com/onlinedoc_dev_2.5.115/6-k8s/k8s/GCU-feature-discovery/content/source/index.html" target="_blank">GCU Feature Discovery</a>：给 GCU 节点添加设备属性标签。</li>
<li>[third-party] <a href="https://support.enflame-tech.com/onlinedoc_dev_2.5.115/6-k8s/k8s/Node-feature-discovery/content/source/index.html" target="_blank">Node Feature Discovery</a>：给节点添加硬件属性标签的 Kubernetes 插件。</li>
</ul>
</li>
<li>监控管理：
<ul>
<li><a href="https://support.enflame-tech.com/onlinedoc_dev_2.5.115/6-k8s/k8s/GCU-Exporter/content/source/index.html" target="_blank">GCU-Exporter</a>：采集 GCU 运行指标，并以 Prometheus metrics 形式暴露出来。</li>
</ul>
</li>
<li>部署运维：
<ul>
<li><a href="https://support.enflame-tech.com/onlinedoc_dev_2.5.115/6-k8s/k8s/GCU-operator/content/source/index.html" target="_blank">GCU-Operator</a>：可以自动化部署上述组件 + GCU 驱动的 Operator。</li>
<li><a href="https://support.enflame-tech.com/onlinedoc_dev_2.5.115/6-k8s/k8s/KubeOne/content/source/index.html" target="_blank">KubeOne</a>：基于 sealer 进行定制二次开发的 K8s 集群部署工具。</li>
</ul>
</li>
<li>二次开发库：
<ul>
<li><a href="https://support.enflame-tech.com/onlinedoc_dev_2.5.115/6-k8s/k8s/Go-eflib/content/source/index.html" target="_blank">Go-Eflib</a>：支持 GCU 设备管理的 Golang API</li>
</ul>
</li>
<li>设备管理工具：
<ul>
<li><a href="https://support.enflame-tech.com/onlinedoc_dev_2.5.115/6-k8s/efml/EFSMI/content/source/index.html" target="_blank">EFSMI</a>：管理 GCU 设置的命令行工具。</li>
</ul>
</li>
</ul>
<h2 id="安装部署"><a class="header" href="#安装部署">安装部署</a></h2>
<p>参考<a href="https://support.enflame-tech.com/onlinedoc_dev_2.5.115/6-k8s/k8s/GCU-operator/content/source/enflame_gcu_operator_2_0_user_guide.html#gcu-operator" target="_blank">燧原官方文档</a>，使用 GCU Operator 安装部署燧原驱动和 TopsCloud 产品。</p>
<h2 id="参考-19"><a class="header" href="#参考-19">参考</a></h2>
<p><a href="https://support.enflame-tech.com/onlinedoc_dev_2.5.115/6-k8s/index.html">TopsCloud 用户使用指南</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="海光-dcu"><a class="header" href="#海光-dcu">海光 DCU</a></h1>
<h2 id="参考-20"><a class="header" href="#参考-20">参考</a></h2>
<p><a href="https://cancon.hpccube.com:65024/5/main/Kubernetes%E6%8F%92%E4%BB%B6">https://cancon.hpccube.com:65024/5/main/Kubernetes插件</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="华为"><a class="header" href="#华为">华为</a></h1>
<h2 id="参考-21"><a class="header" href="#参考-21">参考</a></h2>
<p><a href="https://gitee.com/ascend/ascend-device-plugin">https://gitee.com/ascend/ascend-device-plugin</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="天数智芯"><a class="header" href="#天数智芯">天数智芯</a></h1>
<h2 id="参考-22"><a class="header" href="#参考-22">参考</a></h2>
<p><a href="https://www.iluvatar.com/docs/">https://www.iluvatar.com/docs/</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="沐曦"><a class="header" href="#沐曦">沐曦</a></h1>
<blockquote>
<p>MetaX 的驱动和软件栈目前没有公开发布，需要注册其支持账户，从其它渠道获得。</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装-tensorstack-ai-计算平台"><a class="header" href="#安装-tensorstack-ai-计算平台">安装 TensorStack AI 计算平台</a></h1>
<p>安装 TensorStack AI 计算平台分为以下三个阶段：</p>
<ol>
<li><a href="online/products/./pre-install.html">安装前的准备工作</a></li>
<li>安装产品，你可以选择<a href="online/products/./install-uc-mode.html">安装产品-User Console 模式</a>或者 <a href="online/products/./install-traditional-mode.html">安装产品-传统模式</a></li>
<li><a href="online/products/./post-install.html">安装后的配置工作</a>以及<a href="online/products/./post-install-optional.html">安装后可选配置</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装前准备"><a class="header" href="#安装前准备">安装前准备</a></h1>
<h2 id="目的-10"><a class="header" href="#目的-10">目的</a></h2>
<p>为安装 T9k 产品做安装前的准备工作，包括获取域名及证书、预先创建一些 K8s 资源等。</p>
<h2 id="前提条件-4"><a class="header" href="#前提条件-4">前提条件</a></h2>
<p>执行安装命令的环境要求：</p>
<ul>
<li>可用的 K8s 集群
<ul>
<li>已安装前述的各种 <a href="online/products/../k8s-components/index.html">k8s 组件</a>;</li>
<li>可使用 <code>kubectl</code>，具备 cluster-admin 权限；</li>
<li>可访问安装过程中使用的容器镜像服务（一般在公网上，可支持本地 mirror）；</li>
</ul>
</li>
<li>能够访问存放安装包的网络服务（一般在公网上，可支持本地 mirror）；</li>
<li><code>kubectl</code> 版本 &gt;= v1.25.x；<code>helm</code> 版本 &gt;= v3.9.x。</li>
</ul>
<h2 id="域名及证书"><a class="header" href="#域名及证书">域名及证书</a></h2>
<p>本节准备产品的域名、设置域名解析、获取域名证书。</p>
<h3 id="获取域名"><a class="header" href="#获取域名">获取域名</a></h3>
<blockquote>
<p>应当以合适的途径获得域名，并配置其解析。</p>
</blockquote>
<p>下文假设用户选择使用域名 <code>sample.t9kcloud.cn</code> 部署产品，各个具体的模块的子域名如下表。</p>
<div class="table-wrapper"><table><thead><tr><th>域名</th><th>说明</th></tr></thead><tbody>
<tr><td><code>home.sample.t9kcloud.cn</code></td><td>平台主入口</td></tr>
<tr><td><code>auth.sample.t9kcloud.cn</code></td><td>安全系统</td></tr>
<tr><td><code>s3.sample.t9kcloud.cn</code></td><td>AI 资产和实验管理服务的 S3 接口地址</td></tr>
<tr><td><code>*.ksvc.sample.t9kcloud.cn</code></td><td>模型推理服务</td></tr>
</tbody></table>
</div>
<p>注意事项：</p>
<ol>
<li>具体安装时，应当替换 <code>sample.t9kcloud.cn</code> 为实际的名称；</li>
<li>可使用 <code>*.sample.t9kcloud.cn</code> 的域名证书简化流程；</li>
<li>如果要在公网使用 TensorStack AI 平台，域名一般需要备案才能使用。</li>
</ol>
<h3 id="设置解析"><a class="header" href="#设置解析">设置解析</a></h3>
<p>为 <code>*.sample.t9kcloud.cn</code> 域名添加一条 A （或者 CNAME）记录，使其最终正确指向 K8s 集群的 <a target="_blank" rel="noopener noreferrer" href="https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress">Ingress</a> 服务的 IP 地址。</p>
<p>验证：</p>
<pre><code class="language-bash"># 注意：需要使用实际的域名
dig home.sample.t9kcloud.cn +short
</code></pre>
<h3 id="获取证书"><a class="header" href="#获取证书">获取证书</a></h3>
<aside class="note info">
<div class="title">获取 TLS 证书</div>
<p>用于支持 HTTPS 的证书可以在服务商处购买，也可以使用免费证书服务（如 <a href="https://freessl.cn/">https://freessl.cn/</a>， <a href="https://letsencrypt.org/">https://letsencrypt.org/</a>，<a href="https://zerossl.com/">https://zerossl.com/</a>）。</p>
<p><a href="online/products/../../appendix/manage-domain-certificate.html">管理域名证书</a> 中提供了获取证书的更多详情。</p>
</aside>
<p>证书为 2 个文件，分别为 ：</p>
<ul>
<li><code>server.crt</code>，公钥证书（public key certificate）；</li>
<li><code>server.key</code>，私钥（private key）。</li>
</ul>
<p>查看证书内容：</p>
<pre><code class="language-bash"># 验证公钥证书有效期：
cat server.crt | openssl x509 -noout -enddate

# 确认公钥证书对应的域名：
cat server.crt | openssl x509 -noout -text \
  | grep -A 1 -i &quot;Subject Alternative Name&quot;

# 输出公钥证书所有内容：
cat server.crt | openssl x509 -noout -text

# 确认私钥的 RSA 格式正确：
cat server.key | openssl rsa -check
</code></pre>
<h2 id="创建-k8s-资源"><a class="header" href="#创建-k8s-资源">创建 K8s 资源</a></h2>
<h3 id="namespace"><a class="header" href="#namespace">namespace</a></h3>
<p>需要创建以下 namespace：</p>
<div class="table-wrapper"><table><thead><tr><th>名称</th><th>说明</th></tr></thead><tbody>
<tr><td>t9k-system</td><td>TensorStack AI 平台控制平面</td></tr>
<tr><td>t9k-syspub</td><td>存储公共配置</td></tr>
<tr><td>t9k-monitoring</td><td>监控及告警系统</td></tr>
</tbody></table>
</div>
<p>确认以下 namespace 是否存在：</p>
<pre><code class="language-bash">kubectl get ns t9k-system
kubectl get ns t9k-syspub
kubectl get ns t9k-monitoring
</code></pre>
<p>如果不存在则创建：</p>
<pre><code>for ns in &quot;t9k-system&quot; &quot;t9k-syspub&quot; &quot;t9k-monitoring&quot;; do
  kubectl create ns &quot;$ns&quot;
done
</code></pre>
<h3 id="设置-label"><a class="header" href="#设置-label">设置 label</a></h3>
<blockquote>
<p>说明：没有 <code>control-plane</code> label 的 namespace 会受到系统 Admission Control 模块的检查，但运行 TensorStack AI 平台服务（系统控制平面）的 namespace 中的工作负载不应当接受这些检查。</p>
</blockquote>
<p>为运行 TensorStack AI 平台系统功能的 namespace 设置 label: <code>control-plane=&quot;true&quot;</code>：</p>
<pre><code class="language-bash">namespaces=(
  &quot;ingress-nginx&quot;
  &quot;istio-system&quot;
  &quot;knative-serving&quot;
  &quot;kube-system&quot;
  &quot;t9k-system&quot;
  &quot;t9k-syspub&quot;
  &quot;t9k-monitoring&quot;
)

for ns in &quot;${namespaces[@]}&quot;; do
  kubectl label ns &quot;$ns&quot; control-plane=&quot;true&quot;
done
</code></pre>
<h3 id="证书-secret"><a class="header" href="#证书-secret">证书 Secret</a></h3>
<p>创建以下 Secret 资源以存储 Ingress 的 TLS 证书：</p>
<div class="table-wrapper"><table><thead><tr><th>Name</th><th>Namespace</th><th>Host</th><th>说明</th></tr></thead><tbody>
<tr><td><code>cert.home</code></td><td>istio-system</td><td>home.sample.t9kcloud.cn</td><td>平台主入口</td></tr>
<tr><td><code>cert.auth</code></td><td>t9k-system</td><td>auth.sample.t9kcloud.cn</td><td>安全系统入口</td></tr>
<tr><td><code>cert.s3</code></td><td>t9k-system</td><td>s3.sample.t9kcloud.cn</td><td>AI 资产和实验管理服务的 S3 接口地址</td></tr>
</tbody></table>
</div><aside class="note">
<div class="title">注意</div>
<p>上表中的 <code>Host</code> 字段为示意，部署时应当使用实际的域名，而不是 <code>*.sample.t9kcloud.cn</code>。</p>
</aside>
<p>如果我们使用多域名证书，可以使用同一份 cert 文件创建这些 secret：</p>
<pre><code class="language-bash">kubectl create secret tls cert.home \
    --cert='server.crt' \
    --key='server.key' \
    -n istio-system

kubectl create secret tls cert.auth \
    --cert='server.crt' \
    --key='server.key' \
    -n t9k-system

kubectl create secret tls cert.s3 \
    --cert='server.crt' \
    --key='server.key' \
    -n t9k-system
</code></pre>
<p>说明：</p>
<ol>
<li>如果使用单独的证书，需要在上面的命令中使用不同的文件分别创建 Secret。</li>
<li>目前模型推理服务的 Ingress (*.ksvc.sample.t9kcloud.cn) 使用 HTTP 协议，不需要配置 Cert/Secret。</li>
</ol>
<h2 id="准备配置"><a class="header" href="#准备配置">准备配置</a></h2>
<h3 id="设置-valuesyaml"><a class="header" href="#设置-valuesyaml">设置 values.yaml</a></h3>
<p>从 github 上获取与产品版本对应的 <a target="_blank" rel="noopener noreferrer" href="https://github.com/t9k/ks-clusters/tree/master/values">示例 values.yaml 文件</a>，其中名称带有 “uc” 的适用于 User Console 安装模式，其他的适用于传统安装模式。</p>
<aside class="note">
<div class="title">注意</div>
<ol>
<li>带有注释 MUST 的设置必须检查。</li>
<li>如需自定义安装安装功能模块（例如不安装集群管理的部分功能），需要参考<a href="online/products/../../appendix/cluster-admin-installation-configuration.html">集群管理安装配置</a>进一步调整 <code>values.yaml</code> 的设置。</li>
</ol>
</aside>
<p>参考 <code>values.yaml</code> 的注释修改此文件中的相应字段。</p>
<h2 id="预先拉取镜像"><a class="header" href="#预先拉取镜像">预先拉取镜像</a></h2>
<p>可选，预先拉取 T9k 产品需要的所有镜像。</p>
<p>预先拉取镜像需要在所有加入了 K8s 集群的节点上进行，有以下好处：</p>
<ol>
<li>加快部署速度，减少部署过程中等待 Pod 就绪的时间；</li>
<li>减少 Pod 因为其依赖项尚未就绪，导致 Pod 出错、重启的风险；</li>
<li>可以较快地判断已经部署的产品是否正常运行，并及时处理潜在的错误。</li>
</ol>
<p>从 github 上获取与产品对应的<a target="_blank" rel="noopener noreferrer" href="https://github.com/t9k/ks-clusters/tree/master/tools/offline-t9k/imagelist">镜像列表</a>，保存到本地，然后运行以下命令拉取列表中的镜像：</p>
<pre><code class="language-bash">ansible-playbook ../ks-clusters/t9k-playbooks/22-pre-pull-images.yml \
  -i inventory/inventory.ini \
  --become -K \
  -e path_to_image_list=&lt;PATH&gt;
</code></pre>
<p>其中，<code>&lt;PATH&gt;</code> 需要替换为镜像列表文件的路径，可以是绝对路径或者相对于 playbook 文件 <code>22-pre-pull-images.yml</code> 的相对路径。</p>
<blockquote>
<p>如果计划安装的产品尚未生成镜像列表，则需要参考文档 <a href="online/products/../appendix/generate-t9k-product-image-list.html">生成 T9k 产品镜像列表</a>。</p>
</blockquote>
<h2 id="下一步-8"><a class="header" href="#下一步-8">下一步</a></h2>
<p>完成本文档的准备工作后，可进行实际的 <a href="online/products/./install-uc-mode.html">产品安装</a>。</p>
<h2 id="参考-23"><a class="header" href="#参考-23">参考</a></h2>
<p><a href="https://freessl.cn/">https://freessl.cn/</a></p>
<p><a href="https://letsencrypt.org/">https://letsencrypt.org/</a></p>
<p><a href="https://zerossl.com/">https://zerossl.com/</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="审计日志"><a class="header" href="#审计日志">审计日志</a></h1>
<p>T9k 审计日志记录下列两种类型的操作：</p>
<ol>
<li>平台管理：记录 T9k 管理员的操作行为，例如创建用户、修改资源价格等。</li>
<li>资源对象：记录系统 namespace （如 kube-system，t9k-system）及集群范围（cluster-wide） 资源对象发生的变化。例如，管理员修改了 kube-sytstem 中 ConfigMap coredns，或者创建了新的 CRD <code>workflowtemplates.argoproj.io</code>。</li>
</ol>
<h2 id="启用-t9k-审计日志"><a class="header" href="#启用-t9k-审计日志">启用 T9k 审计日志</a></h2>
<p>如需启用 T9k 审计日志，请确保进行了下列配置。</p>
<h3 id="启用-kubernetes-审计"><a class="header" href="#启用-kubernetes-审计">启用 Kubernetes 审计</a></h3>
<p>安装 K8s 集群时，按照 <a href="online/products/pre-install/../../k8s-install.html#%E8%AE%BE%E7%BD%AE-kubernetes-%E5%AE%A1%E8%AE%A1">设置 Kubernetes 审计</a> 对集群进行配置。</p>
<h3 id="配置-loki"><a class="header" href="#配置-loki">配置 loki</a></h3>
<p>安装 loki 时，按照 <a href="online/products/pre-install/../../k8s-components/loki.html#t9k-%E5%AE%A1%E8%AE%A1%E6%97%A5%E5%BF%97">Values 配置-&gt;T9k 审计日志</a> 对 promtail 进行配置。</p>
<h3 id="产品模块设置"><a class="header" href="#产品模块设置">产品模块设置</a></h3>
<p>在安装 T9k 产品时通过 values.yaml 文件中设置合适的选项。</p>
<p><strong>t9k-cluster-admin</strong></p>
<p>确保 t9k-cluster-admin 使用的 values.yaml 设置了下列字段：</p>
<pre><code class="language-yaml">options:
  proxyOperationCtl:
    enabled: true
</code></pre>
<p><strong>t9k-security-console-api</strong></p>
<p>确保产品模块 t9k-security-console-api 使用的 values.yaml 中字段 <code>global.t9k.securityService.resourceManagement.proxyoperation.enabled</code> 的值是 <code>true</code>。</p>
<p><strong>t9k-cost</strong></p>
<p>确保产品模块 t9k-cost 使用的 values.yaml 中字段 <code>global.t9k.cost.proxyoperation.enabled</code> 的值是 <code>true</code>。</p>
<h2 id="禁用-t9k-审计日志"><a class="header" href="#禁用-t9k-审计日志">禁用 T9k 审计日志</a></h2>
<p>如需禁用 T9k 审计日志，请进行下列设置。</p>
<h3 id="关闭-kubernetes-审计"><a class="header" href="#关闭-kubernetes-审计">关闭 Kubernetes 审计</a></h3>
<p>推荐在安装 K8s 集群时，将 <a href="online/products/pre-install/../../k8s-install.html#k8s-clusteryml">k8s-cluster.yml</a> 的 <code>kubernetes_audit</code> 字段设为 false，禁用 Kubernetes 审计。</p>
<h3 id="配置-loki-1"><a class="header" href="#配置-loki-1">配置 loki</a></h3>
<p>在安装 loki 时，删除 <a href="online/products/pre-install/../../k8s-components/loki.html#t9k-%E5%AE%A1%E8%AE%A1%E6%97%A5%E5%BF%97">Values 配置-&gt;T9k 审计日志</a> 中收集 K8s 审计日志的配置内容。</p>
<h3 id="产品模块设置-1"><a class="header" href="#产品模块设置-1">产品模块设置</a></h3>
<p>在安装 T9k 产品时通过 values.yaml 文件中设置合适的选项。</p>
<p><strong>t9k-cluster-admin</strong></p>
<p>确保产品模块 t9k-cluster-admin 使用的 values.yaml 设置了下列字段：</p>
<pre><code class="language-yaml">options:
  proxyOperationCtl:
    enabled: false
</code></pre>
<p><strong>t9k-security-console-api</strong></p>
<p>确保产品模块 t9k-security-console-api 使用的 values.yaml 中字段 <code>global.t9k.securityService.resourceManagement.proxyoperation.enabled</code> 的值是 <code>false</code>。</p>
<p><strong>t9k-cost</strong></p>
<p>确保产品模块 t9k-cost 使用的 values.yaml 中字段 <code>global.t9k.cost.proxyoperation.enabled</code> 的值是 <code>false</code>。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装产品-user-console-模式"><a class="header" href="#安装产品-user-console-模式">安装产品-User Console 模式</a></h1>
<p>本安装模式是有别于<a href="online/products/./install-traditional-mode.html">传统模式</a>的另一种安装模式。在本安装模式中：</p>
<ol>
<li>普通用户将通过 User Console 模块进行操作，安装和使用所需的其它应用程序；</li>
<li>管理员则使用 Admin Console 模块，对集群进行管理。</li>
</ol>
<p>User Console 模式的产品安装分为两步：</p>
<ol>
<li><a href="online/products/./install-uc.html">安装产品</a></li>
<li><a href="online/products/./register-app.html">注册 APP</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装产品"><a class="header" href="#安装产品">安装产品</a></h1>
<h2 id="目的-11"><a class="header" href="#目的-11">目的</a></h2>
<p>在 K8s 集群中以 User Console 模式安装 T9k 产品。</p>
<h2 id="前置条件-2"><a class="header" href="#前置条件-2">前置条件</a></h2>
<ul>
<li>完成了 <a href="online/products/./pre-install.html">安装前准备</a>。</li>
<li>可以访问 Registry <code>tsz.io</code>，或者通过其它方式获得需要的 Helm Charts。</li>
</ul>
<h2 id="安装产品模块"><a class="header" href="#安装产品模块">安装产品模块</a></h2>
<p>安装以下产品模块：</p>
<pre><code class="language-console">t9k-core
t9k-security-console-api
t9k-monitoring
t9k-build-console-api
t9k-cost
t9k-jobs
t9k-notebook
t9k-services
t9k-csi-s3
t9k-workflow-manager-api
t9k-cluster-admin
t9k-user-console
</code></pre>
<h3 id="使用本地-helm-charts-安装"><a class="header" href="#使用本地-helm-charts-安装">使用本地 Helm Charts 安装</a></h3>
<p>首先，确认当前路径下已经准备好了 <code>values.yaml</code>，并下载好了 Helm Charts。Helm Charts 可以联系向量栈的工程师来获取：</p>
<pre><code class="language-bash">tree .
</code></pre>
<p>输出：</p>
<pre><code class="language-console">.
├── charts
│   ├── t9k-build-console-api-1.79.5.tgz
│   ├── t9k-cluster-admin-1.79.5.tgz
│   ├── t9k-core-1.79.5.tgz
│   ├── t9k-cost-1.79.2.tgz
│   ├── t9k-csi-s3-1.79.5.tgz
│   ├── t9k-jobs-1.79.5.tgz
│   ├── t9k-monitoring-1.79.5.tgz
│   ├── t9k-notebook-1.79.5.tgz
│   ├── t9k-security-console-api-1.79.5.tgz
│   ├── t9k-services-1.79.5.tgz
│   ├── t9k-user-console-1.79.5.tgz
│   └── t9k-workflow-manager-api-1.79.5.tgz
└── values.yaml
</code></pre>
<aside class="note">
<div class="title">注意</div>
<ol>
<li>请根据实际的 Helm Chart 名称修改下文的安装命令。</li>
<li>产品模块 <code>t9k-monitoring</code> 安装的 namespace 与其他产品模块不同，复制命令时需要注意。</li>
</ol>
</aside>
<p>进行安装：</p>
<pre><code class="language-bash">helm -n t9k-system install t9k-core \
    charts/t9k-core-1.79.5.tgz -f values.yaml

helm -n t9k-system install t9k-security-console-api \
    charts/t9k-security-console-api-1.79.5.tgz -f values.yaml

helm -n t9k-monitoring install t9k-monitoring \
    charts/t9k-monitoring-1.79.5.tgz -f values.yaml

helm -n t9k-system install t9k-build-console-api \
    charts/t9k-build-console-api-1.79.5.tgz -f values.yaml

helm -n t9k-system install t9k-cost \
    charts/t9k-cost-1.79.2.tgz -f values.yaml

helm -n t9k-system install t9k-jobs \
    charts/t9k-jobs-1.79.5.tgz -f values.yaml

helm -n t9k-system install t9k-notebook \
    charts/t9k-notebook-1.79.5.tgz -f values.yaml

helm -n t9k-system install t9k-services \
    charts/t9k-services-1.79.5.tgz -f values.yaml

helm -n t9k-system install t9k-csi-s3 \
    charts/t9k-csi-s3-1.79.5.tgz -f values.yaml

helm -n t9k-system install t9k-workflow-manager-api \
    charts/t9k-workflow-manager-api-1.79.5.tgz -f values.yaml

helm -n t9k-system install t9k-cluster-admin \
    charts/t9k-cluster-admin-1.79.5.tgz -f values.yaml

helm -n t9k-system install t9k-user-console \
    charts/t9k-user-console-1.79.5.tgz -f values.yaml
</code></pre>
<h3 id="使用在线-helm-charts-安装"><a class="header" href="#使用在线-helm-charts-安装">使用在线 Helm Charts 安装</a></h3>
<p>首先，确认当前路径下已经准备好了 <code>values.yaml</code>，并可以访问 Registry <code>tsz.io</code> 来在线下载 Helm Charts。验证：</p>
<pre><code class="language-bash">helm pull oci://tsz.io/t9kcharts/t9k-core --version 1.79.5
</code></pre>
<aside class="note">
<div class="title">注意</div>
<ol>
<li>如果需要指定版本号，可以设置 <code>--version &lt;version&gt;</code> 参数。如果未设置，则默认使用最新的版本号。</li>
</ol>
</aside>
<p>进行安装：</p>
<pre><code class="language-bash">helm -n t9k-system install t9k-core \
    oci://tsz.io/t9kcharts/t9k-core \
    -f values.yaml

helm -n t9k-system install t9k-security-console-api \
    oci://tsz.io/t9kcharts/t9k-security-console-api \
    -f values.yaml

helm -n t9k-monitoring install t9k-monitoring \
    oci://tsz.io/t9kcharts/t9k-monitoring \
    -f values.yaml

helm -n t9k-system install t9k-build-console-api \
    oci://tsz.io/t9kcharts/t9k-build-console-api \
    -f values.yaml

helm -n t9k-system install t9k-cost \
    oci://tsz.io/t9kcharts/t9k-cost \
    -f values.yaml

helm -n t9k-system install t9k-jobs \
    oci://tsz.io/t9kcharts/t9k-jobs \
    -f values.yaml

helm -n t9k-system install t9k-notebook \
    oci://tsz.io/t9kcharts/t9k-notebook \
    -f values.yaml

helm -n t9k-system install t9k-services \
    oci://tsz.io/t9kcharts/t9k-services \
    -f values.yaml

helm -n t9k-system install t9k-csi-s3 \
    oci://tsz.io/t9kcharts/t9k-csi-s3 \
    -f values.yaml

helm -n t9k-system install t9k-workflow-manager-api \
    oci://tsz.io/t9kcharts/t9k-workflow-manager-api \
    -f values.yaml

helm -n t9k-system install t9k-cluster-admin \
    oci://tsz.io/t9kcharts/t9k-cluster-admin \
    -f values.yaml

helm -n t9k-system install t9k-user-console \
    oci://tsz.io/t9kcharts/t9k-user-console \
    -f values.yaml
</code></pre>
<h2 id="基本检查"><a class="header" href="#基本检查">基本检查</a></h2>
<p>等待并确认集群中所有的 Pod 都正常工作。等待的时间取决于是否预先拉取了镜像、网络情况等，可能需要 5~60 分钟不等：</p>
<pre><code class="language-bash"># 持续查看 K8s 集群中的所有 Pod 状态
kubectl get pod -A -w
</code></pre>
<pre><code class="language-bash"># 查看 K8s 集群中是否有异常状态的 Pod
kubectl get pod -A -o wide | grep -Eiv &quot;running|complete&quot;
</code></pre>
<p>查看产品模块的安装信息（helm chart release），以 t9k-core 为例：</p>
<pre><code class="language-bash">helm status -n t9k-system t9k-core
</code></pre>
<pre><code class="language-console">NAME: t9k-core
LAST DEPLOYED: November 19 04:53:53 2023
NAMESPACE: t9k-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
</code></pre>
<p>查看所有的产品模块安装情况（helm chart releases）：</p>
<pre><code class="language-bash">helm list -A -d
</code></pre>
<details><summary><code class="hljs">output</code></summary>
<pre><code class="language-console">NAME                    	NAMESPACE     	REVISION	UPDATED                             	STATUS  	CHART                          	APP VERSION
cilium                  	kube-system   	2       	2024-06-19 14:59:01.294733 +0800 CST	deployed	cilium-1.15.5                  	1.15.5     
elasticsearch-master    	t9k-monitoring	1       	2024-07-19 18:46:08.279436 +0800 CST	deployed	elasticsearch-7.13.4           	7.13.4     
elasticsearch-client    	t9k-monitoring	1       	2024-07-19 18:46:13.609795 +0800 CST	deployed	elasticsearch-7.13.4           	7.13.4     
elasticsearch-data      	t9k-monitoring	1       	2024-07-19 18:46:18.339152 +0800 CST	deployed	elasticsearch-7.13.4           	7.13.4     
t9k-gatekeeper          	t9k-system    	1       	2024-07-19 18:46:49.341557 +0800 CST	deployed	gatekeeper-3.11.0              	v3.11.0    
t9k-security-console-api	t9k-system    	1       	2024-07-21 14:11:41.737822 +0800 CST	deployed	t9k-security-console-api-1.79.5	1.79.5     
t9k-csi-s3              	t9k-system    	1       	2024-07-21 15:38:15.055841 +0800 CST	deployed	t9k-csi-s3-1.79.5              	1.79.5     
t9k-jobs                	t9k-system    	1       	2024-07-21 15:38:34.894072 +0800 CST	deployed	t9k-jobs-1.79.5                	1.79.5     
t9k-notebook            	t9k-system    	1       	2024-07-21 15:44:47.773993 +0800 CST	deployed	t9k-notebook-1.79.5            	1.79.5     
t9k-services            	t9k-system    	1       	2024-07-21 15:45:07.225526 +0800 CST	deployed	t9k-services-1.79.5            	1.79.5     
t9k-workflow-manager-api	t9k-system    	1       	2024-07-21 15:45:49.413001 +0800 CST	deployed	t9k-workflow-manager-api-1.79.5	1.79.5     
t9k-monitoring          	t9k-monitoring	1       	2024-07-21 15:47:45.142087 +0800 CST	deployed	t9k-monitoring-1.79.5          	1.79.5     
t9k-cluster-admin       	t9k-system    	1       	2024-07-21 15:50:23.983369 +0800 CST	deployed	t9k-cluster-admin-1.79.5       	1.79.5     
t9k-cost                	t9k-system    	2       	2024-07-21 15:55:27.888551 +0800 CST	deployed	t9k-cost-1.79.2                	1.79.2     
t9k-user-console        	t9k-system    	1       	2024-07-21 15:56:39.249047 +0800 CST	deployed	t9k-user-console-1.79.5        	1.79.5     
t9k-core                	t9k-system    	6       	2024-07-21 16:07:27.06814 +0800 CST 	deployed	t9k-core-1.79.5                	1.79.5     
t9k-build-console-api   	t9k-system    	2       	2024-07-21 17:18:05.392658 +0800 CST	deployed	t9k-build-console-api-1.79.5   	1.79.5 
</code></pre>
</details>
<h2 id="下一步-9"><a class="header" href="#下一步-9">下一步</a></h2>
<p>进行 <a href="online/products/./post-install.html">安装后配置</a>。</p>
<h2 id="参考-24"><a class="header" href="#参考-24">参考</a></h2>
<p><a href="https://helm.sh/docs/">https://helm.sh/docs/</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="注册-app"><a class="header" href="#注册-app">注册 APP</a></h1>
<h2 id="创建项目"><a class="header" href="#创建项目">创建项目</a></h2>
<p>在浏览器中访问 <a href="https://home.sample.t9kcloud.cn/t9k/cluster-admin/web/">https://home.sample.t9kcloud.cn/t9k/cluster-admin/web/</a>，使用管理员账号登录。你可以从 values.yaml 的 <code>global.t9k.securityService.t9kAdminAccount</code> 字段中获得管理员账号，例如使用 yq 命令获取：</p>
<pre><code class="language-bash"># yq 4.x.x 的获取方式如下：
yq e &quot;.global.t9k.securityService.t9kAdminAccount&quot; values.yaml
</code></pre>
<p>点击左侧导航菜单 “项目管理 -&gt; 项目”，点击右上角 “创建项目”；在弹出的菜单中填写 “项目名称” 和 “管理员”；点击右下角的 “创建” 创建项目：</p>
<figure class="screenshot">
  <img alt="create-project" src="online/products/../../assets/online/product/create-project.png" />
</figure>
<h2 id="创建-api-key"><a class="header" href="#创建-api-key">创建 API Key</a></h2>
<p>在浏览器中访问 <a href="https://home.sample.t9kcloud.cn/t9k/user-console/web/">https://home.sample.t9kcloud.cn/t9k/user-console/web/</a>。</p>
<p>点击左侧导航菜单的 “账户设置 &gt; 安装设置”，展开“管理 APIKey”：</p>
<figure class="screenshot">
  <img alt="manage-apikey" src="online/products/../../assets/online/product/manage-apikey.png" />
</figure>
<p>点击启用并输入密码，然后 “生成 API Key”；在弹出的菜单中勾选“集群”；点击左下角的“生成 API Key” 创建 API Key：</p>
<figure class="screenshot">
  <img alt="manage-apikey" src="online/products/../../assets/online/product/create-apikey.png" />
</figure>
<p>从页面中复制 API Key 备用。</p>
<h2 id="注册-app-1"><a class="header" href="#注册-app-1">注册 APP</a></h2>
<p>首先，获取 <a target="_blank" rel="noopener noreferrer" href="https://github.com/t9k/apps">apps</a> 中的 APP Template：</p>
<pre><code class="language-bash">git clone https://github.com/t9k/apps.git &amp;&amp; cd apps
</code></pre>
<p><code>user-console</code> 目录中包含了多个 APP，你可以在 <a target="_blank" rel="noopener noreferrer" href="https://github.com/t9k/apps/blob/master/user-console/README.md">README.md</a> 中看到注册和注销应用的方法。</p>
<p>从 <a target="_blank" rel="noopener noreferrer" href="https://github.com/t9k/user-manuals/releases">releases</a> 中获取最新的 t9k-app 命令行工具。以 Linux 操作系统为例：</p>
<pre><code class="language-bash">wget https://github.com/t9k/user-manuals/releases/download/2024-08-07/t9k-app-linux-amd64
mv t9k-app-linux-amd64 t9k-app
sudo chmod +x t9k-app
</code></pre>
<p>注册 APP，这里提供 terminal 和 notebook (jupyter-lab-cpu) 两个例子：</p>
<pre><code class="language-bash"># 将 API Key 保存在环境变量中
export API_KEY=&quot;&lt;admin-api-key&gt;&quot;

# 注册 terminal
./t9k-app register -k $API_KEY \
    -s https://home.sample.t9kcloud.cn/t9k/app/server \
    -f ./user-console/terminal/template.yaml \
    -u -v=2

# 注册 Notebook 中的 jupyter-lab-cpu APP
./t9k-app register -k $API_KEY \
    -s https://home.sample.t9kcloud.cn/t9k/app/server \
    -f ./user-console/notebook/jupyter-lab-cpu/template.yaml \
    -u -v=2
</code></pre>
<p>每个 APP 注册完成后，都会有一条注册成功的信息：</p>
<pre><code class="language-console">I1 06/13 14:23:45 logr.go:280 app-server [register app successfully] 
</code></pre>
<h2 id="验证-9"><a class="header" href="#验证-9">验证</a></h2>
<p>在 User Console (<a href="https://home.sample.t9kcloud.cn/t9k/user-console/web/">https://home.sample.t9kcloud.cn/t9k/user-console/web/</a>) 中点击左侧导航菜单的 “应用”，点击右上角 “部署应用”，查看可选的应用项：</p>
<figure class="screenshot">
  <img alt="manage-apikey" src="online/products/../../assets/online/product/list-apps.png" />
</figure>
<p>你也可以通过命令行工具进行确认：</p>
<pre><code class="language-bash">./t9k-app -k $API_KEY -s https://home.sample.t9kcloud.cn/t9k/app/server list
</code></pre>
<pre><code class="language-bash">NAME                DISPLAY NAME                DEFAULT VERSION     CATEGORIES
codeserver          Code Server                 0.1.2               Tool
comfyui             ComfyUI                     0.1.1               AI
dify                Dify                        0.3.7               AI
filebrowser         FileBrowser                 0.1.2               Tool
fish-speech         Fish Speech                 0.1.0               AI
gpt-researcher      GPT Researcher              0.1.5               AI
job-manager         Job Manager                 0.1.3               Tool, AI
jupyterlab-cpu      JupyterLab (CPU)            0.1.2               IDE
jupyterlab-gpu      JupyterLab (Nvidia GPU)     0.1.2               IDE
jupyterlab-test     JupyterLab (TEST)           0.1.2               IDE
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装产品-1"><a class="header" href="#安装产品-1">安装产品</a></h1>
<pre><code>TODO:
    1. 增加 github 上 产品 release 链接
    2. 减少 TensorStack 产品之间的依赖，并更新推荐的产品安装顺序
</code></pre>
<h2 id="目的-12"><a class="header" href="#目的-12">目的</a></h2>
<p>在 K8s 集群中以传统模式安装 T9k 产品。</p>
<h2 id="前置条件-3"><a class="header" href="#前置条件-3">前置条件</a></h2>
<ul>
<li>完成了 <a href="online/products/./pre-install.html">安装前准备</a>。</li>
<li>可以访问 Registry <code>tsz.io</code>，或者通过其它方式获得需要的 Helm Charts。</li>
</ul>
<h2 id="安装产品模块-1"><a class="header" href="#安装产品模块-1">安装产品模块</a></h2>
<p>安装以下产品模块：</p>
<pre><code class="language-console">t9k-core
t9k-security-console
t9k-landing-page
t9k-scheduler
t9k-monitoring
t9k-build-console
t9k-cost
t9k-jobs
t9k-notebook
t9k-services
t9k-tools
t9k-csi-s3
t9k-deploy-console
t9k-workflow-manager
t9k-cluster-admin
</code></pre>
<p>可选，安装 t9k AI Data 系列的产品模块：</p>
<pre><code class="language-console">t9k-aistore
t9k-asset-hub
t9k-experiment-management
</code></pre>
<h3 id="使用本地-helm-charts-安装-1"><a class="header" href="#使用本地-helm-charts-安装-1">使用本地 Helm Charts 安装</a></h3>
<p>首先，确认当前路径下已经准备好了 <code>values.yaml</code>，并下载好了 Helm Charts。Helm Charts 可以联系向量栈的工程师来获取：</p>
<pre><code class="language-bash">tree .
</code></pre>
<p>输出：</p>
<pre><code class="language-console">.
├── charts
│   ├── t9k-build-console-&lt;version&gt;.tgz
│   ├── t9k-cluster-admin-&lt;version&gt;.tgz
│   ├── t9k-core-&lt;version&gt;.tgz
│   ├── t9k-cost-&lt;version&gt;.tgz
│   ├── t9k-csi-s3-&lt;version&gt;.tgz
│   ├── t9k-deploy-console-&lt;version&gt;.tgz
│   ├── t9k-jobs-&lt;version&gt;.tgz
│   ├── t9k-landing-page-&lt;version&gt;.tgz
│   ├── t9k-monitoring-&lt;version&gt;.tgz
│   ├── t9k-notebook-&lt;version&gt;.tgz
│   ├── t9k-scheduler-&lt;version&gt;.tgz
│   ├── t9k-security-console-&lt;version&gt;.tgz
│   ├── t9k-services-&lt;version&gt;.tgz
│   ├── t9k-tools-&lt;version&gt;.tgz
│   └── t9k-workflow-manager-&lt;version&gt;.tgz
└── values.yaml
</code></pre>
<aside class="note">
<div class="title">注意</div>
<ol>
<li>请根据实际的 Helm Chart 名称修改下文的安装命令。</li>
<li>产品模块 <code>t9k-monitoring</code> 安装的 namespace 与其他产品模块不同，复制命令时需要注意。</li>
</ol>
</aside>
<p>进行安装：</p>
<pre><code class="language-bash">helm -n t9k-system install t9k-core \
    charts/t9k-core-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-security-console \
    charts/t9k-security-console-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-landing-page \
    charts/t9k-landing-page-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-scheduler \
    charts/t9k-scheduler-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-monitoring install t9k-monitoring \
    charts/t9k-monitoring-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-build-console \
    charts/t9k-build-console-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-cost \
    charts/t9k-cost-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-jobs \
    charts/t9k-jobs-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-notebook \
    charts/t9k-notebook-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-services \
    charts/t9k-services-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-tools \
    charts/t9k-tools-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-csi-s3 \
    charts/t9k-csi-s3-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-deploy-console \
    charts/t9k-deploy-console-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-workflow-manager \
    charts/t9k-workflow-manager-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-cluster-admin \
    charts/t9k-cluster-admin-&lt;version&gt;.tgz -f values.yaml
</code></pre>
<p>安装可选的 AI Data 系列产品模块：</p>
<pre><code class="language-bash">helm -n t9k-system install t9k-aistore \
    charts/t9k-aistore-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-asset-hub \
    charts/t9k-asset-hub-&lt;version&gt;.tgz -f values.yaml

helm -n t9k-system install t9k-experiment-management \
    charts/t9k-experiment-management-&lt;version&gt;.tgz -f values.yaml
</code></pre>
<h3 id="使用在线-helm-charts-安装-1"><a class="header" href="#使用在线-helm-charts-安装-1">使用在线 Helm Charts 安装</a></h3>
<p>首先，确认当前路径下已经准备好了 <code>values.yaml</code>，并可以访问 Registry <code>tsz.io</code> 来在线下载 Helm Charts。验证：</p>
<pre><code class="language-bash">helm pull oci://tsz.io/t9kcharts/t9k-core --version 1.79.5
</code></pre>
<aside class="note">
<div class="title">注意</div>
<ol>
<li>如果需要指定版本号，可以设置 <code>--version &lt;version&gt;</code> 参数。如果未设置，则默认使用最新的版本号。</li>
</ol>
</aside>
<p>进行安装：</p>
<pre><code class="language-bash">helm -n t9k-system install t9k-core \
    oci://tsz.io/t9kcharts/t9k-core -f values.yaml

helm -n t9k-system install t9k-security-console \
    oci://tsz.io/t9kcharts/t9k-security-console -f values.yaml

helm -n t9k-system install t9k-landing-page \
    oci://tsz.io/t9kcharts/t9k-landing-page -f values.yaml

helm -n t9k-system install t9k-scheduler \
    oci://tsz.io/t9kcharts/t9k-scheduler -f values.yaml

helm -n t9k-monitoring install t9k-monitoring \
    oci://tsz.io/t9kcharts/t9k-monitoring -f values.yaml

helm -n t9k-system install t9k-build-console \
    oci://tsz.io/t9kcharts/t9k-build-console -f values.yaml

helm -n t9k-system install t9k-cost \
    oci://tsz.io/t9kcharts/t9k-cost -f values.yaml

helm -n t9k-system install t9k-jobs \
    oci://tsz.io/t9kcharts/t9k-jobs -f values.yaml

helm -n t9k-system install t9k-notebook \
    oci://tsz.io/t9kcharts/t9k-notebook -f values.yaml

helm -n t9k-system install t9k-services \
    oci://tsz.io/t9kcharts/t9k-services -f values.yaml

helm -n t9k-system install t9k-tools \
    oci://tsz.io/t9kcharts/t9k-tools -f values.yaml

helm -n t9k-system install t9k-csi-s3 \
    oci://tsz.io/t9kcharts/t9k-csi-s3 -f values.yaml

helm -n t9k-system install t9k-deploy-console \
    oci://tsz.io/t9kcharts/t9k-deploy-console -f values.yaml

helm -n t9k-system install t9k-workflow-manager \
    oci://tsz.io/t9kcharts/t9k-workflow-manager -f values.yaml

helm -n t9k-system install t9k-cluster-admin \
    oci://tsz.io/t9kcharts/t9k-cluster-admin -f values.yaml
</code></pre>
<p>安装可选的 AI Data 系列产品模块：</p>
<pre><code class="language-bash">helm -n t9k-system install t9k-aistore \
    oci://tsz.io/t9kcharts/t9k-aistore -f values.yaml

helm -n t9k-system install t9k-asset-hub \
    oci://tsz.io/t9kcharts/t9k-asset-hub -f values.yaml

helm -n t9k-system install t9k-experiment-management \
    oci://tsz.io/t9kcharts/t9k-experiment-management -f values.yaml
</code></pre>
<h2 id="基本检查-1"><a class="header" href="#基本检查-1">基本检查</a></h2>
<p>等待并确认集群中所有的 Pod 都正常工作。等待的时间取决于是否预先拉取了镜像、网络情况等，可能需要 5~60 分钟不等：</p>
<pre><code class="language-bash"># 持续查看 K8s 集群中的所有 Pod 状态
kubectl get pod -A -w

# 查看 K8s 集群中是否有异常状态的 Pod
kubectl get pod -A -o wide | grep -Eiv &quot;running|complete&quot;
</code></pre>
<p>查看产品模块的安装信息（helm chart release），以 t9k-core 为例：</p>
<pre><code class="language-bash">helm status -n t9k-system t9k-core
</code></pre>
<pre><code>NAME: t9k-core
LAST DEPLOYED: November 19 04:53:53 2023
NAMESPACE: t9k-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
</code></pre>
<p>查看所有的产品模块安装情况（helm chart releases）：</p>
<pre><code class="language-bash">helm list -A -d
</code></pre>
<details><summary><code class="hljs">output</code></summary>
<pre><code class="language-console">NAME                            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                                   APP VERSION
elasticsearch-single            t9k-monitoring  1               2023-11-19 04:42:24.939067616 +0000 UTC deployed        elasticsearch-7.13.4                    7.13.4
t9k-gatekeeper                  t9k-system      2               2023-11-19 04:47:12.871874737 +0000 UTC deployed        gatekeeper-3.11.0                       v3.11.0
t9k-core                        t9k-system      1               2023-11-19 04:52:52.591086929 +0000 UTC deployed        t9k-core-1.78.3                         1.78.3
t9k-scheduler                   t9k-system      1               2023-11-19 04:53:22.047545558 +0000 UTC deployed        t9k-scheduler-1.78.4                    1.78.4
t9k-csi-s3                      t9k-system      1               2023-11-19 04:53:46.694820382 +0000 UTC deployed        t9k-csi-s3-1.78.3                       1.78.3
t9k-jobs                        t9k-system      1               2023-11-19 04:54:12.858122721 +0000 UTC deployed        t9k-jobs-1.78.4                         1.78.4
t9k-services                    t9k-system      1               2023-11-19 04:54:36.863984918 +0000 UTC deployed        t9k-services-1.78.4                     1.78.4
t9k-landing-page                t9k-system      1               2023-11-19 04:55:00.60533111 +0000 UTC  deployed        t9k-landing-page-1.78.4                 1.78.4
t9k-security-console            t9k-system      1               2023-11-19 04:55:19.309728043 +0000 UTC deployed        t9k-security-console-1.78.5             1.78.5
t9k-notebook                    t9k-system      1               2023-11-19 04:55:54.230482157 +0000 UTC deployed        t9k-notebook-1.78.4                     1.78.4
t9k-monitoring                  t9k-monitoring  1               2023-11-19 04:56:12.617506927 +0000 UTC deployed        t9k-monitoring-1.78.5                   1.78.5
t9k-build-console               t9k-system      1               2023-11-19 04:57:19.251309469 +0000 UTC deployed        t9k-build-console-1.78.5                1.78.5
t9k-deploy-console              t9k-system      1               2023-11-19 04:57:36.088260359 +0000 UTC deployed        t9k-deploy-console-1.78.4               1.78.4
t9k-workflow-manager            t9k-system      1               2023-11-19 04:57:56.56433641 +0000 UTC  deployed        t9k-workflow-manager-1.78.4             1.78.4
t9k-asset-hub                   t9k-system      1               2023-11-19 04:58:28.991306879 +0000 UTC deployed        t9k-asset-hub-1.78.4                    1.78.4
t9k-experiment-management       t9k-system      1               2023-11-19 04:58:49.350846324 +0000 UTC deployed        t9k-experiment-management-1.78.4        1.78.4
t9k-cluster-admin               t9k-system      1               2023-11-19 06:02:45.082613774 +0000 UTC deployed        t9k-cluster-admin-1.78.8                1.78.8
t9k-aistore                     t9k-system      3               2023-11-19 06:37:17.947109956 +0000 UTC deployed        t9k-aistore-1.78.5                      1.78.5
</code></pre>
</details>
<h2 id="下一步-10"><a class="header" href="#下一步-10">下一步</a></h2>
<p>进行 <a href="online/products/./post-install.html">安装后配置</a>。</p>
<h2 id="参考-25"><a class="header" href="#参考-25">参考</a></h2>
<p><a href="https://helm.sh/docs/">https://helm.sh/docs/</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装后配置-3"><a class="header" href="#安装后配置-3">安装后配置</a></h1>
<h2 id="目的-13"><a class="header" href="#目的-13">目的</a></h2>
<p>进行初次安装后的配置工作，包括报警发送方式、日志系统等。</p>
<h2 id="前提条件-5"><a class="header" href="#前提条件-5">前提条件</a></h2>
<p>完成 <a href="online/products/./install-uc-mode.html">产品安装</a>。</p>
<h2 id="管理员登录"><a class="header" href="#管理员登录">管理员登录</a></h2>
<aside class="note warning">
<div class="title">注意</div>
<p>下文使用的 <code>home.sample.t9kcloud.cn</code> 仅为示例，请使用安装时实际配置的域名。</p>
</aside>
<p>在浏览器中访问集群管理控制台 <a href="https://home.sample.t9kcloud.cn/t9k/cluster-admin/web/">https://home.sample.t9kcloud.cn/t9k/cluster-admin/web/</a>，通过 <a href="online/products/./install-uc-mode.html">安装产品</a> 一节使用的配置文件中设置的 T9k 平台管理员账号密码登录。</p>
<p>点击<strong>用户管理 &gt; 用户</strong>，进入用户列表页面。</p>
<figure class="screenshot">
  <img alt="user-list" src="online/products/../../assets/online/user-list.png" />
</figure>
<p>点击右上角的<strong>创建用户</strong>来创建一个新用户。</p>
<figure class="screenshot">
  <img alt="create-user" src="online/products/../../assets/online/create-user.png" />
</figure>
<p>点击<strong>项目管理 &gt; 项目</strong>，进入项目列表页面。</p>
<figure class="screenshot">
  <img alt="project-list" src="online/products/../../assets/online/project-list.png" />
</figure>
<p>点击右上角的<strong>创建项目</strong>来创建一个新项目。</p>
<figure class="screenshot">
  <img alt="create-project" src="online/products/../../assets/online/create-project.png" />
</figure>
<h2 id="监控系统"><a class="header" href="#监控系统">监控系统</a></h2>
<h3 id="安装-cadvisor-服务"><a class="header" href="#安装-cadvisor-服务">安装 cAdvisor 服务</a></h3>
<aside class="note warning">
<div class="title">注意</div>
<p>需要确认 t9k-monitoring 已经正确安装。</p>
</aside>
<p>在 K8s 1.24 及之后的一些版本，kubelet cadvisor 无法提供有效的 metrics 信息。管理员需要单独部署 cadvisor 服务来提供集群的 metrics 信息。已知 K8s 版本 1.24.10，1.25.9 存在此问题，根据 <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cadvisor/issues/2785#issuecomment-1205538108">issue</a> 中的讨论，其它版本也可能存在相同的问题。</p>
<p>cAdvisor 服务的安装步骤：</p>
<ol>
<li>
<p>删除 servicemonitor kubelet 的 cadvisor 部分：</p>
<pre><code class="language-bash">kubectl -n t9k-monitoring edit servicemonitor kubelet

# 需要删除的部分
kubectl -n t9k-monitoring get servicemonitor kubelet \
    -o jsonpath=&quot;{.spec.endpoints[?(@.path=='/metrics/cadvisor')]}&quot;
</code></pre>
</li>
<li>
<p>部署 cadvisor 服务：</p>
<aside class="note warning">
 <div class="title">离线安装</div>
<p>如果采用本地容器镜像服务器，需要修改镜像仓库的设置：</p>
<pre><code class="language-bash">sed -i &quot;s|docker.io/t9kpublic|192.168.101.159:5000/t9kpublic|g&quot; \
  ../ks-clusters/additionals/monitoring/cadvisor.yaml
</code></pre>
</aside>
<pre><code class="language-bash">kubectl apply -n kube-system -f ../ks-clusters/additionals/monitoring/cadvisor.yaml
</code></pre>
</li>
</ol>
<h3 id="监控-nvidia-gpu-operator"><a class="header" href="#监控-nvidia-gpu-operator">监控 NVIDIA GPU Operator</a></h3>
<p>运行下列命令创建 ServiceMonitor，配置 Prometheus 收集 NVIDIA DCGM Exporter 的 metrics 数据：</p>
<pre><code class="language-bash">kubectl -n t9k-monitoring create -f - &lt;&lt; EOF
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    tensorstack.dev/default-config: &quot;true&quot;
    tensorstack.dev/metrics-collected-by: t9k-monitoring
  name: nvidia-dcgm-exporter
  namespace: t9k-monitoring
spec:
  endpoints:
  - interval: 30s
    port: gpu-metrics
  jobLabel: app
  namespaceSelector:
    matchNames:
    - gpu-operator
  selector:
    matchLabels:
      app: nvidia-dcgm-exporter
EOF
</code></pre>
<p>NVIDIA GPU Operator 部署的 DCGM Exporter 的 service YAML 如下所示：</p>
<pre><code class="language-bash">kubectl -n gpu-operator get svc nvidia-dcgm-exporter  -o yaml
</code></pre>
<details><summary><code class="hljs">svc-nvidia-dcgm-exporter.yaml</code></summary>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: &quot;true&quot;
  labels:
    app: nvidia-dcgm-exporter
  name: nvidia-dcgm-exporter
  namespace: gpu-operator
  ownerReferences:
  - apiVersion: nvidia.com/v1
    blockOwnerDeletion: true
    controller: true
    kind: ClusterPolicy
    name: cluster-policy
    uid: aa21a324-8efc-43c9-b1fc-6e7b5e0869fd
spec:
  clusterIP: 10.233.51.234
  clusterIPs:
  - 10.233.51.234
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: gpu-metrics
    port: 9400
    protocol: TCP
    targetPort: 9400
  selector:
    app: nvidia-dcgm-exporter
  sessionAffinity: None
  type: ClusterIP
</code></pre>
</details>
<h3 id="告警通知"><a class="header" href="#告警通知">告警通知</a></h3>
<p>配置告警通知，可以让 T9k 系统将告警信息通过邮件、企业微信的形式发送给运维人员。</p>
<p>参考 <code>管理员手册 &gt; 5.1.2. 系统配置 &gt; 告警通知</code>。</p>
<h2 id="logging-系统"><a class="header" href="#logging-系统">Logging 系统</a></h2>
<h3 id="节点-label"><a class="header" href="#节点-label">节点 Label</a></h3>
<p>检查节点的以下 label：</p>
<pre><code class="language-yaml"># 查看所有节点
kubectl get node

# 查看具有 fluentd-ds-ready 标签的节点，期望的输出是所有的节点
kubectl get node -l beta.kubernetes.io/fluentd-ds-ready=&quot;true&quot;

# 查看具有 control-plane 标签的节点，期望的输出是所有控制平面节点
kubectl get node -l node-role.kubernetes.io/control-plane

# 查看具有 ingress 标签的节点，期望的输出是 1-2 个负责控制 ingress 的节点
kubectl get node -l node-role.kubernetes.io/ingress
</code></pre>
<h3 id="配置-elasticsearch"><a class="header" href="#配置-elasticsearch">配置 ElasticSearch</a></h3>
<p>新部署好的 ElasticSearch 需要添加以下设置：</p>
<ol>
<li>index 的生命周期：30 天自动删除，防止数据过多</li>
<li>timestamp 类型设置为纳秒级别</li>
</ol>
<p>详情请参考：<a href="online/products/../../../monitoring-and-log-system/es.html#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE">管理员手册 &gt; 配置 ElasticSearch</a></p>
<h3 id="配置-fluentd"><a class="header" href="#配置-fluentd">配置 fluentd</a></h3>
<p>Kubernetes 底层可以使用不同的容器运行时。不同的运行时，存储的日志格式是不同的，因此需要根据使用的容器运行时进行配置。</p>
<p>详情：<a href="online/products/../../../monitoring-and-log-system/fluentd.html#%E4%BF%AE%E6%94%B9-fluentd-%E9%85%8D%E7%BD%AE">管理员手册 &gt; 配置 Fluentd</a></p>
<h2 id="下一步-11"><a class="header" href="#下一步-11">下一步</a></h2>
<p>运行<a target="_blank" rel="noopener noreferrer" href="https://t9k.github.io/user-manuals/latest/get-started/index.html">用户手册 &gt; 快速入门的例子</a>，检验产品的功能。</p>
<p>执行 <a href="online/products/./post-install-optional.html">安装后可选配置</a>。</p>
<h2 id="参考-26"><a class="header" href="#参考-26">参考</a></h2>
<p><a href="https://prometheus-operator.dev/docs/api-reference/api/#monitoring.coreos.com/v1alpha1.AlertmanagerConfig">https://prometheus-operator.dev/docs/api-reference/api/#monitoring.coreos.com/v1alpha1.AlertmanagerConfig</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装后可选配置"><a class="header" href="#安装后可选配置">安装后可选配置</a></h1>
<h2 id="目的-14"><a class="header" href="#目的-14">目的</a></h2>
<p>安装 T9k 产品时，如果你启用了一些可选的功能，请参考下列教程对启用的功能进行初始化配置。</p>
<h2 id="前提条件-6"><a class="header" href="#前提条件-6">前提条件</a></h2>
<p>完成 <a href="online/products/./post-install.html">安装后配置</a>。</p>
<h2 id="t9k-调度器配置"><a class="header" href="#t9k-调度器配置">T9k 调度器配置</a></h2>
<p>如果你安装了 T9k 调度器，请完成下列配置。</p>
<h3 id="创建默认队列"><a class="header" href="#创建默认队列">创建默认队列</a></h3>
<p>如果集群部署了 T9k 调度器，您需要在集群管理页面：</p>
<ol>
<li>为 T9k 调度器创建名为 <code>default</code> 的队列；</li>
<li>修改 default 队列的配置，允许所有用户使用该队列。</li>
</ol>
<p>打开 <strong>集群管理（Cluster Admin）</strong> 页面（参考 <a href="online/products/post-install-optional.html#%E7%99%BB%E5%BD%95%E7%AE%A1%E7%90%86%E5%91%98%E8%B4%A6%E5%8F%B7">登录管理员账号</a>），点击 <strong>资源管理 &gt; T9k 调度器 &gt; 队列</strong>，进入队列列表页面。</p>
<figure class="screenshot">
  <img alt="queue-list" src="online/products/../../assets/online/queue-list.png" />
</figure>
<p>点击右上角的 <strong>+</strong> 来创建一个新队列，队列名称填写为 <code>default</code>，其他字段按需填写（参考 <a href="online/products/../../../resource-management/queue.html#%E8%AE%BE%E7%BD%AE-queue-%E7%9A%84%E5%B1%9E%E6%80%A7">管理员手册 &gt; 设置 Queue 的属性</a>）。</p>
<figure class="screenshot">
  <img alt="queue-list" src="online/products/../../assets/online/create-queue.png" />
</figure>
<p>回到队列列表页面，点击 <code>default</code> 队列的名称，进入队列详情页面。</p>
<figure class="screenshot">
  <img alt="queue-list" src="online/products/../../assets/online/queue-detail.png" />
</figure>
<p>点击<strong>限制 &gt; 用户/组</strong>的编辑按钮，将用户权限设置为所有人。</p>
<figure class="screenshot">
  <img alt="queue-list" src="online/products/../../assets/online/queue-all-users.png" />
</figure>
<h2 id="下一步-12"><a class="header" href="#下一步-12">下一步</a></h2>
<p>运行 <a target="_blank" rel="noopener noreferrer" href="https://t9k.github.io/user-manuals/latest/get-started/index.html"> 用户手册 &gt; 快速入门的例子 </a>，检验产品的功能。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="正确性检查-1"><a class="header" href="#正确性检查-1">正确性检查</a></h1>
<h2 id="准备工作"><a class="header" href="#准备工作">准备工作</a></h2>
<p>我们通过运行一个 <a target="_blank" rel="noopener noreferrer" href="https://docs.ansible.com/ansible/latest/index.html">ansible</a> playbook 来检查安装的正确性，需要完成以下准备工作。</p>
<h3 id="ansible-环境"><a class="header" href="#ansible-环境">ansible 环境</a></h3>
<p>参考<a href="online/./prepare-nodes-and-install-k8s.html">准备节点与安装 K8s</a> 完成以下准备工作：</p>
<ol>
<li>安装 ansible</li>
<li>将本机的 SSH 公钥复制到所有受控节点上</li>
</ol>
<h3 id="inventory-准备"><a class="header" href="#inventory-准备">inventory 准备</a></h3>
<p>inventory 文件记录了所有目标节点的名称、分组等信息。</p>
<p>示例 inventory 见 <a href="https://github.com/t9k/ks-clusters/blob/master/t9k-playbooks/inventory.yml">https://github.com/t9k/ks-clusters/blob/master/t9k-playbooks/inventory.yml</a>：</p>
<pre><code class="language-yaml">all:
 hosts:
   nuc:
   nc11:
   nc13:
 vars:
   ansible_user: t9k

k8s_cluster:
 children:
   kube_control_plane:
   kube_node:

kube_control_plane:
 hosts:
   nuc:

kube_node:
 hosts:
   nc11:
   nc13:

gpu_nodes:
 hosts:
</code></pre>
<p>以下节点分组必填：</p>
<ul>
<li><code>all</code>: 需要检查的所有节点</li>
<li><code>k8s_cluster</code>: k8s 集群的所有节点</li>
<li><code>kube_control_plane</code>: k8s 集群的控制平面节点</li>
<li><code>kube_node</code>: k8s 集群的工作节点</li>
<li><code>gpu_nodes</code>: k8s 集群中装有 GPU 的节点</li>
</ul>
<h3 id="变量准备"><a class="header" href="#变量准备">变量准备</a></h3>
<p>根据目标集群的实际情况，在默认配置的基础上修改变量。</p>
<p>变量默认值见 <a href="https://github.com/t9k/ks-clusters/blob/master/t9k-playbooks/roles/check-installation/defaults/main.yml">https://github.com/t9k/ks-clusters/blob/master/t9k-playbooks/roles/check-installation/defaults/main.yml</a>：</p>
<pre><code class="language-yaml"># default settings in the inventory
kube_config_dir: &quot;/etc/kubernetes&quot;
bin_dir: &quot;/usr/local/bin&quot;

ceph_enabled: false
nfs_enabled: true
ib_enabled: false
pvc_test_image: t9kpublic/busybox:2023
gpu_test_image: t9kpublic/nvidia-tensorflow:18.07-py3 # nvcr.io/nvidia/tensorflow:18.07-py3
gpu_test_tf_batch_size: 128 # (choose from 32, 64, 128, 256, 512)
gpu_test_tf_layers: 50 # (choose from 18, 34, 50, 101, 152)
system_namespaces:
- kube-system
- ingress-nginx
- istio-system
- t9k-system
- t9k-monitoring
- gpu-operator
- network-operator
s3:
 access_key: &lt;access-key&gt;
 secret_key: &lt;secret-key&gt;
 host: &lt;host&gt;
</code></pre>
<p>其中：</p>
<ul>
<li><code>kube_config_dir</code>: YAML 配置文件的存放路径</li>
<li><code>bin_dir</code>: kubectl 可执行文件的存放路径</li>
<li><code>ceph_enabled</code>: 目标集群是否支持基于 ceph 的 pvc</li>
<li><code>nfs_enabled</code>: 目标集群是否支持基于 nfs 的 pvc</li>
<li><code>ib_enabled</code>: 目标集群是否支持 ib 网络</li>
<li><code>pvc_test_image</code>: 运行 pvc 测试所用的镜像</li>
<li><code>gpu_test_image</code>: 运行 gpu 测试所用的镜像</li>
<li><code>gpu_test_tf_batch_size</code>: 运行 gpu 测试时 resnet 训练的 batch 大小</li>
<li><code>gpu_test_tf_layers</code>: 运行 gpu 测试时 resnet 的层数</li>
<li><code>system_namespaces</code>: 目标集群中有哪些系统级 namesapce 需要检查</li>
<li><code>s3</code>: 用于检查 s3 服务的可访问性以及 StorageShim 功能</li>
</ul>
<h2 id="运行-playbook"><a class="header" href="#运行-playbook">运行 playbook</a></h2>
<p>通过以下命令运行 playbook 来检查安装正确性：</p>
<pre><code class="language-bash">$ git clone https://github.com/t9k/ks-clusters.git
$ cd ks-clusters/t9k-playbooks
# check inventory
$ cat ./inventory.yml
# check variables
$ cat ./roles/check-installation/defaults/main.yml 
# run playbook
$ ansible-playbook -i inventory.yml 99-check-installation.yml --ask-become-pass
</code></pre>
<p>ansible 会对每个节点输出一行 task 运行结果统计，需要关注的是<strong>标记为 failed 的数量</strong>。</p>
<p>检查成功的运行结果如下：</p>
<pre><code>...
...
...
TASK [check-installation : Check s3 | Verify pod log] ***********************************************************************************************
skipping: [nc11]
skipping: [nc13]
changed: [nuc]

TASK [check-installation : Check s3 | Delete pod and storageshim] ***********************************************************************************
skipping: [nc11]
skipping: [nc13]
ok: [nuc]

TASK [check-installation : Check s3 | Delete bucket] ************************************************************************************************
skipping: [nc11]
skipping: [nc13]
changed: [nuc]

PLAY RECAP ******************************************************************************************************************************************
nc11                       : ok=7    changed=4    unreachable=0    failed=0    skipped=22   rescued=0    ignored=0   
nc13                       : ok=7    changed=4    unreachable=0    failed=0    skipped=22   rescued=0    ignored=0   
nuc                        : ok=23   changed=15   unreachable=0    failed=0    skipped=6    rescued=0    ignored=0   
</code></pre>
<p>检查失败的运行结果如下：</p>
<pre><code>...
...
...
TASK [check-installation : Check s3 | Verify pod log] ***********************************************************************************************
skipping: [nc11]
skipping: [nc13]

TASK [check-installation : Check s3 | Delete pod and storageshim] ***********************************************************************************
skipping: [nc11]
skipping: [nc13]

TASK [check-installation : Check s3 | Delete bucket] ************************************************************************************************
skipping: [nc11]
skipping: [nc13]

PLAY RECAP ******************************************************************************************************************************************
nc11                       : ok=7    changed=4    unreachable=0    failed=0    skipped=22   rescued=0    ignored=0   
nc13                       : ok=7    changed=4    unreachable=0    failed=0    skipped=22   rescued=0    ignored=0   
nuc                        : ok=7    changed=4    unreachable=0    failed=1    skipped=6    rescued=0    ignored=0  
</code></pre>
<p>如果 failed 数量不为 0，需要往前翻查看 ansible 报错确定具体原因。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装-harbor-registry"><a class="header" href="#安装-harbor-registry">安装 Harbor Registry</a></h1>
<aside class="note">
<div class="title">注意</div>
<p>如果使用公有容器镜像服务，可跳过这一步。</p>
</aside>
<p>本文档介绍如何在一个 Ubuntu 系统中安装 Harbor，以提供本地的快速容器镜像服务。</p>
<h2 id="前提条件-7"><a class="header" href="#前提条件-7">前提条件</a></h2>
<p>Harbor 对于节点有以下要求，详见 <a target="_blank" rel="noopener noreferrer" href="https://goharbor.io/docs/2.0.0/install-config/installation-prereqs/">Harbor Installation Prerequisites</a>。</p>
<p>硬件要求：</p>
<div class="table-wrapper"><table><thead><tr><th>Resource</th><th>Minimum</th><th>Recommended</th></tr></thead><tbody>
<tr><td>CPU</td><td>2 cores</td><td>4 cores</td></tr>
<tr><td>Mem</td><td>4 GB</td><td>8 GB</td></tr>
<tr><td>Disk</td><td>40 GB</td><td>160 GB</td></tr>
</tbody></table>
</div>
<p>软件要求：</p>
<div class="table-wrapper"><table><thead><tr><th>Software</th><th>Version</th></tr></thead><tbody>
<tr><td>Docker engine</td><td>Version 17.06.0-ce+ or higher</td></tr>
<tr><td>Docker Compose</td><td>Version 1.18.0 or higher</td></tr>
<tr><td>OpenSSL</td><td>Latest is preferred</td></tr>
</tbody></table>
</div>
<p>端口要求：</p>
<div class="table-wrapper"><table><thead><tr><th>端口</th><th>协议</th></tr></thead><tbody>
<tr><td>443</td><td>HTTPS</td></tr>
<tr><td>4443</td><td>HTTPS</td></tr>
<tr><td>80</td><td>HTTP</td></tr>
</tbody></table>
</div>
<h2 id="快速安装"><a class="header" href="#快速安装">快速安装</a></h2>
<p>本节用于在一个 Ubuntu 系统中快速配置 Docker 运行环境并安装 Harbor。该 host 仅用于运行 Harbor，不应有其他系统。</p>
<p>获取 <a href="online/registry/../../assets/online/harbor.sh">harbor.sh</a> 脚本，在安装路径中保存为 <code>harbor.sh</code> 并进行以下修改。</p>
<aside class="note">
<div class="title">注意</div>
<ol>
<li>该脚本来源于 <a href="https://goharbor.io/docs/2.0.0/install-config/quick-install-script/">https://goharbor.io/docs/2.0.0/install-config/quick-install-script/</a>，在其基础上进行了修改。</li>
<li>不要在计划或已经加入 K8s 的节点中运行该脚本。该安装脚本中安装的 docker 版本、docker 配置与 kubespray 不兼容，在同一个节点上运行这两个脚本（无论先后）会导致错误。</li>
<li>该脚本安装的 Harbor 不使用 https，但可以在后续修改其设置。</li>
</ol>
</aside>
<p>调整脚本，设置其中这两个参数：</p>
<pre><code class="language-bash">COMPOSEVERSION=&quot;v2.23.0&quot;
HARBORVERSION=&quot;v2.7.3&quot;
</code></pre>
<p>运行以下命令，根据提示设置 IP 或者域名（FQDN）进行安装：</p>
<pre><code class="language-bash">sudo chmod u+x harbor.sh
sudo ./harbor.sh
</code></pre>
<p>运行结束后，根据提示信息运行 docker login 验证 Harbor 是否安装成功。后续请参考<a href="online/registry/harbor.html#%E9%85%8D%E7%BD%AE-harbor">配置 Harbor</a>，比如修改<a href="online/registry/harbor.html#%E7%AE%A1%E7%90%86%E5%91%98%E5%88%9D%E5%A7%8B%E5%AF%86%E7%A0%81">管理员密码</a>。</p>
<h2 id="安装-harbor"><a class="header" href="#安装-harbor">安装 Harbor</a></h2>
<p>如果无法进行“快速安装”（例如已经配置了 Docker），可参照本节，设置更多选项后进行高级安装。</p>
<p>首先确认节点安装了 Docker 和 Docker Compose。如未安装，可参考<a href="online/registry/../../appendix/install-docker.html">附录：安装 Docker</a> 和<a href="online/registry/../../appendix/install-docker-compose.html">附录：安装 Docker Compose</a> 进行安装。</p>
<p>参考：<a target="_blank" rel="noopener noreferrer" href="https://goharbor.io/docs/2.7.0/install-config/">Harbor Installation and Configuration</a></p>
<h3 id="步骤"><a class="header" href="#步骤">步骤</a></h3>
<ol>
<li>
<p>从 <a href="https://github.com/goharbor/harbor/releases">https://github.com/goharbor/harbor/releases</a> 获取 online-installer 或者 offline-installer。online-installer 会在安装过程中在线拉取镜像，而 offline-installer 中已经包含了需要的所有镜像，适用于离线安装。</p>
<pre><code class="language-bash"># get offline installer; use proxy if needed: export https_proxy=&lt;your-proxy&gt;
wget https://github.com/goharbor/harbor/releases/download/v2.7.3/harbor-offline-installer-v2.7.3.tgz
</code></pre>
</li>
<li>
<p>解压安装包：</p>
<pre><code class="language-bash">  tar xzvf harbor-online-installer-v2.7.3.tgz
</code></pre>
</li>
<li>
<p>进入 harbor 目录，创建 harbor.yml：</p>
<pre><code class="language-bash">cd harbor
cp harbor.yml.tmpl harbor.yml
</code></pre>
<p>然后根据<a href="online/registry/harbor.html#%E9%85%8D%E7%BD%AE-harbor">配置 Harbor</a> 的说明，修改 harbor.yml。</p>
</li>
<li>
<p>启动 Harbor：</p>
<pre><code class="language-bash">sudo ./install.sh

# 确认服务正常运行
sudo docker-compose ps
</code></pre>
</li>
</ol>
<h3 id="harbor-客户端"><a class="header" href="#harbor-客户端">harbor 客户端</a></h3>
<p>如果未配置 Harbor 通过 HTTPS 提供服务 ，则所有需要访问该 Registry 的节点应当允许 insecure 访问。</p>
<p>假设客户端使用 Docker，可进行如下设置：</p>
<pre><code class="language-bash">IPorFQDN=&quot;&lt;ip-or-domain&gt;&quot;

# 运行前请确认 /etc/docker/daemon.json 文件不存在或为空
# 否则只需要把 &quot;insecure-registries&quot; 设置增加到 /etc/docker/daemon.json 文件中
sudo cat &gt; /etc/docker/daemon.json &lt;&lt;EOF
{
  &quot;insecure-registries&quot; : [&quot;$IPorFQDN:80&quot;,&quot;0.0.0.0/0&quot;]
}
EOF
</code></pre>
<p>然后重启服务：</p>
<pre><code class="language-bash">sudo systemctl daemon-reload
sudo systemctl restart docker
</code></pre>
<h2 id="配置-harbor"><a class="header" href="#配置-harbor">配置 Harbor</a></h2>
<p>Harbor 安装包（例如：<code>harbor-online-installer-v2.7.3.tgz</code>）解压后，会在当前路径生成 harbor 目录：</p>
<pre><code class="language-console">harbor
├── common
│   └── config/ # 其中包含的文件略
├── common.sh
├── docker-compose.yml
├── harbor.yml
├── harbor.yml.tmpl
├── install.sh
├── LICENSE
└── prepare
</code></pre>
<p>执行 <code>install.sh</code> 将根据 <code>harbor.yml</code> 文件中的配置生成 <code>docker-compose.yml</code> 文件，并运行 <code>docker-compose up -d</code> 以启动/重启服务。</p>
<p>因此，我们建议按照以下步骤修改 Harbor 的配置：</p>
<ol>
<li>修改 <code>harbor.yml</code> 文件</li>
<li>运行 <code>install.sh</code> 以重新启动服务</li>
</ol>
<p>关于 <code>harbor.yml </code>文件的详细说明，请参阅相应版本的 Harbor 官方文档 <a target="_blank" rel="noopener noreferrer" href="https://goharbor.io/docs/2.7.0/install-config/configure-yml-file/">Configure the Harbor YML File</a>。本章节仅对 TensorStack 常用的配置进行说明。</p>
<h3 id="管理员初始密码"><a class="header" href="#管理员初始密码">管理员初始密码</a></h3>
<p>Harbor 的管理员用户为 <code>admin</code>，初始密码在 <code>harbor_admin_password</code> 字段中设置，默认值是 <code>Harbor12345</code>。</p>
<p>可使用该用户登录 Harbor 网页，修改 <code>admin</code> 的密码。</p>
<aside class="note">
<div class="title">注意</div>
<p>修改 <code>harbor.yml</code> 并再次运行 <code>install.sh</code> 不会重置管理员的密码。</p>
</aside>
<h3 id="使用-https-访问-harbor"><a class="header" href="#使用-https-访问-harbor">使用 HTTPS 访问 Harbor</a></h3>
<p>harbor.yml 中的以下字段用于配置 HTTPS 服务：</p>
<pre><code class="language-yaml">hostname: registry.sample.t9kcloud.cn
https:
  port: 443
  certificate: &lt;/your/certificate/path&gt;
  private_key: &lt;/your/private/key/path&gt;
</code></pre>
<h3 id="设置存储路径"><a class="header" href="#设置存储路径">设置存储路径</a></h3>
<p>Harbor 具有以下存储需求：</p>
<ol>
<li>关系型数据库： Harbor 需要关系型数据库来存储元数据，如项目、用户、项目成员、镜像元数据、策略等</li>
<li>数据存储：用于持久化存储镜像和 Helm Charts</li>
<li>Redis：提供数据缓存功能</li>
</ol>
<p>使用以下字段设置存储路径：</p>
<pre><code class="language-yaml">data_volume: /data/harbor
</code></pre>
<p>该路径下会被创建以下目录：</p>
<ol>
<li>database：关系型数据库</li>
<li>job_logs：保存 job 日志</li>
<li>redis：保存数据缓存</li>
<li>registry：存储数据，即镜像和 Helm Chart</li>
</ol>
<h3 id="使用-s3-作为数据存储"><a class="header" href="#使用-s3-作为数据存储">使用 S3 作为数据存储</a></h3>
<p>在 <code>harbor.yml</code> 中，<code>storage_service</code> 字段用于设置保存镜像和 Helm Chart 的外部存储服务。当 <code>storage_service</code> 未被设置时，数据存储路径为 <code>data_volume</code> 中的 <code>registry</code> 目录；设置 <code>storage_service</code> 并重启服务后，使用该存储服务进行存储。</p>
<p>Harbor 支持多种存储后端，例如 azure, gcs, s3, swift 等，详情请参考：<a href="https://goharbor.io/docs/1.10/install-config/configure-yml-file/#backend">https://goharbor.io/docs/1.10/install-config/configure-yml-file/#backend</a></p>
<p>如使用 S3 服务，设置以下字段：</p>
<pre><code class="language-yaml">storage_service:
  s3:
    accesskey: &lt;access Key&gt;
    secretkey: &lt;secret Key&gt;
    region: &lt;region&gt;
    regionendpoint: http://&lt;s3-url&gt;
    bucket: &lt;bucket-name&gt;
</code></pre>
<p>其中 bucket 的 <code>region</code> 可以通过下面命令获取：</p>
<pre><code class="language-bash">s3cmd info s3://my-bucket | grep Location
</code></pre>
<h2 id="常用操作"><a class="header" href="#常用操作">常用操作</a></h2>
<p>Harbor 服务通过 docker-compose 运行，本章节说明 docker-compose 的常用操作。</p>
<aside class="note">
<div class="title">注意</div>
<p>下列命令需要在 <a href="online/registry/harbor.html#%E9%85%8D%E7%BD%AE-harbor">harbor 目录</a>（即 docker-compose.yml 所在路径）下运行。</p>
</aside>
<p>查看运行中的容器及其状态：</p>
<pre><code class="language-bash">sudo docker-compose ps

# 注意第四列的 SERVICE
# docker-compose 使用 SERVICE 而不是第一列的 NAME 来指代具体的容器

# 查看 service core 的容器状态
sudo docker-compose ps core
</code></pre>
<p>启动 docker-compose.yml 指定的容器：</p>
<pre><code class="language-bash"># 启动所有容器
sudo docker-compose up -d

# 使用 SERVICE 名指定的容器
sudo docker-compose up -d core
</code></pre>
<p>关闭并删除 docker-compse.yml 指定的容器：</p>
<pre><code class="language-bash"># 关闭所有容器
sudo docker-compose down

# 关闭 SERVICE 名指定的容器
sudo docker-compose down core
</code></pre>
<p>重启 docker-compose.yml 指定的容器：</p>
<pre><code class="language-bash"># 重启所有容器
sudo docker-compose restart

# 重启 SERVICE 名指定的容器
sudo docker-compose restart core
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>如果修改了 harbor.yml 配置或更新了证书，应当通过 install.sh 来重启使得修改生效：</p>
</aside>
<pre><code class="language-bash">sudo ./install.sh
</code></pre>
<p>查看容器的运行日志：</p>
<pre><code class="language-bash"># 查看当前所有日志
sudo docker-compose logs

# 仅查看指定 SERVICE 的日志
sudo docker-compose logs core

# 查看指定时间段内的日志
sudo docker-compose logs \
  --since 2023-10-20T00:00:00Z \
  --until 2023-10-20T12:00:00Z

# 查看最新的 20 条日志
sudo docker-compose logs --tail 20

# 持续跟踪日志输出
sudo docker-compose log -f
</code></pre>
<p>在容器中运行命令：</p>
<pre><code class="language-bash"># 运行 SERVICE core 的 bash 以进行调试
sudo docker-compose exec core bash
</code></pre>
<h2 id="参考-27"><a class="header" href="#参考-27">参考</a></h2>
<p><a href="https://goharbor.io/docs/">https://goharbor.io/docs/</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装存储服务"><a class="header" href="#安装存储服务">安装存储服务</a></h1>
<p>这里介绍如何安装如下存储服务，以方便被 K8s 及 TensorStack AI 计算平台使用：</p>
<ul>
<li><a href="online/storage-service/./minio.html">MinIO</a> - 轻量级的对象存储服务，支持 S3 协议；</li>
<li><a href="online/storage-service/./nfs.html">NFS</a> - NFS 网络文件系统服务 + K8s CSI 驱动程序；</li>
<li><a href="online/storage-service/./ceph.html">Ceph</a> - Ceph 存储服务 + K8s CSI 驱动程序；</li>
<li><a href="online/storage-service/./lustre.html">Lustre</a> - K8s CSI 驱动程序；</li>
<li><a href="online/storage-service/./gpfs.html">GPFS - IBM Storage Scale</a> - K8s CSI 驱动程序；</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="minio"><a class="header" href="#minio">MinIO</a></h1>
<p>本节说明针对测试场景，如何安装一个 “单节点、单存储盘” 的 minio 服务，以提供 <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Amazon_S3">S3 协议的对象存储服务（Object Storage Service）</a>。</p>
<p>参考： <a target="_blank" rel="noopener noreferrer" href="https://min.io/docs/minio/linux/operations/install-deploy-manage/deploy-minio-single-node-single-drive.html">Deploy MinIO: Single-Node Single-Drive</a>。</p>
<aside class="note">
<div class="title">注意</div>
<p>本文档仅介绍了一个单节点、测试目的的 minIO；生产级别的安装请参考  <a target="_blank" rel="noopener noreferrer" href="https://min.io/docs/">minio 官方文档</a>。</p>
</aside>
<h2 id="安装-8"><a class="header" href="#安装-8">安装</a></h2>
<p>连接到计划安装 minio 的节点，下载并安装：</p>
<pre><code class="language-bash"># Download from internet
wget https://dl.min.io/server/minio/release/linux-amd64/archive/minio_20231007150738.0.0_amd64.deb -O minio.deb

# 注意：在离线安装方案中，minio.deb 已经被复制到 ~/minio.deb
sudo dpkg -i minio.deb
</code></pre>
<p>创建 minio 用户及存储路径：</p>
<pre><code class="language-bash">sudo groupadd -r minio-user
sudo useradd -M -r -g minio-user minio-user

sudo mkdir -p /data/minio
sudo chown minio-user:minio-user /data/minio
sudo chmod 750 /data/minio
</code></pre>
<p>在 <code>/etc/default/minio</code> 生成配置：</p>
<pre><code class="language-bash">cat &lt;&lt; EOF | sudo tee /etc/default/minio
MINIO_ROOT_USER=myminioadmin
MINIO_ROOT_PASSWORD=minio-secret-key-change-me

# MINIO_VOLUMES sets the storage volume or path to use for the MinIO server.

MINIO_VOLUMES=&quot;/data/minio&quot;

# MINIO_SERVER_URL sets the hostname of the local machine for use with the MinIO Server
# MinIO assumes your network control plane can correctly resolve this hostname to the local machine

# Uncomment the following line and replace the value with the correct hostname for the local machine and port for the MinIO server (9000 by default).
# MINIO_SERVER_URL=&quot;http://minio.example.net:9000&quot;
EOF

# ensure its privacy
sudo chmod 600 /etc/default/minio

# verify contents
sudo cat /etc/default/minio
</code></pre>
<p>启动 minio 系统服务：</p>
<pre><code class="language-bash">sudo systemctl enable --now minio
</code></pre>
<p>查看 minio 运行状态，以确认服务正常启动：</p>
<pre><code class="language-bash">sudo systemctl status minio
sudo journalctl -f -u minio
</code></pre>
<h2 id="测试"><a class="header" href="#测试">测试</a></h2>
<p>Web UI 的地址可以在其 log 中获得：</p>
<pre><code class="language-bash">sudo journalctl -f -u minio
</code></pre>
<p>若通过 S3 协议，使用命令行访问，需要<a href="online/storage-service/../../appendix/install-s3cmd.html">安装 s3cmd</a>，并配置 s3cfg：</p>
<pre><code class="language-bash">cat &gt; ~/mytest.s3cfg &lt;&lt; EOF
[default]
access_key = &lt;MINIO_ROOT_USER&gt;
host_base = &lt;MINIO_SERVER_URL&gt;
host_bucket = &lt;MINIO_SERVER_URL&gt;
secret_key = &lt;MINIO_ROOT_PASSWORD&gt;
use_https = False

EOF
</code></pre>
<p>使用 s3cmd：</p>
<pre><code class="language-bash"># make a bucket
s3cmd -c ~/mytest.s3cfg mb s3://aistore

# put an object
touch test.txt
s3cmd -c ~/mytest.s3cfg put test.txt s3://aistore/

# list all
s3cmd -c ~/mytest.s3cfg la s3://

# get a file
s3cmd -c ~/mytest.s3cfg get s3://aistore/test.txt test2.txt
</code></pre>
<h2 id="检查状态"><a class="header" href="#检查状态">检查状态</a></h2>
<p>可通过查看服务状态及其 log：</p>
<pre><code class="language-bash">sudo systemctl status minio

sudo journalctl -f -u minio

ps u -C minio
</code></pre>
<h2 id="卸载"><a class="header" href="#卸载">卸载</a></h2>
<p>停止 minio 服务（此时可以通过 systemctl disable --now minio 重启）：</p>
<pre><code class="language-bash">sudo systemctl disable --now minio

# 确认服务已经停止
systemctl status minio
</code></pre>
<p>进一步卸载 minio 会造成不可逆的影响。在卸载 minio 之前，您需要妥善处理下面事项：</p>
<ol>
<li>备份 minio 中的数据（或确认其中只有测试数据）</li>
<li>处理依赖 minio 的服务（比如 aistore、lakefs），卸载这些服务或者将它们配置为使用其他底层存储。</li>
</ol>
<p>卸载 minio package：</p>
<pre><code class="language-bash">sudo apt remove minio
</code></pre>
<p>[可选] 删除 minio 数据：</p>
<pre><code class="language-bash"># 根据 /etc/default/minio 中的 MINIO_VOLUMES 配置确定路径

# 确认路径中的内容
sudo ls -alh /data/minio

# 删除 minio 数据
sudo rm -rf /data/minio
</code></pre>
<p>删除 minio 配置文件：</p>
<pre><code class="language-bash">sudo rm -rf /etc/default/minio
</code></pre>
<p>删除 minio user （如果该 user 是 group minio-user 的唯一成员，group 也会被一起删除）：</p>
<pre><code class="language-bash">sudo userdel minio-user
</code></pre>
<h2 id="参考-28"><a class="header" href="#参考-28">参考</a></h2>
<p><a href="https://en.wikipedia.org/wiki/Amazon_S3">https://en.wikipedia.org/wiki/Amazon_S3</a></p>
<p><a href="https://min.io/docs/">https://min.io/docs/</a></p>
<p><a href="https://min.io/docs/minio/linux/operations/install-deploy-manage/deploy-minio-single-node-single-drive.html">https://min.io/docs/minio/linux/operations/install-deploy-manage/deploy-minio-single-node-single-drive.html</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="nfs-和-storageclass"><a class="header" href="#nfs-和-storageclass">NFS 和 StorageClass</a></h1>
<p>本文档是在 K8s 集群选定的单个节点上安装一个 NFS 服务，并基于该服务在 K8s 集群中安装 NFS CSI driver 和相应的 StorageClass <code>nfs-csi</code>。</p>
<aside class="note">
<div class="title">注意</div>
<p>使用 NFS 作为 K8s 集群存储仅在小规模或者测试场景下适用。</p>
</aside>
<h2 id="前置条件-4"><a class="header" href="#前置条件-4">前置条件</a></h2>
<p>完成 <a href="online/storage-service/../k8s-install.html">K8s 基本集群</a>的部署。</p>
<h2 id="安装-9"><a class="header" href="#安装-9">安装</a></h2>
<p>如直接使用 Internet 的镜像仓库（例如 docker hub、阿里云等）安装，则跳过 “离线设置” 部分。</p>
<h3 id="离线设置"><a class="header" href="#离线设置">离线设置</a></h3>
<aside class="note">
<div class="title">注意</div>
<p>如果<a href="online/storage-service/../../offline/install/k8s.html#%E5%AE%89%E8%A3%85-k8s-%E9%9B%86%E7%BE%A4">离线安装 K8s 集群</a>时已经设置过这个变量，则检查变量设置符合预期即可。</p>
</aside>
<p>如果无 Internet 连接，需要使用本地设置的容器镜像仓库，设置如下：</p>
<pre><code class="language-bash"># 进入为此次安装准备的 inventory 目录
cd ~/ansible/$T9K_CLUSTER

# 修改 inventory/group_vars/all/download.yml
vim inventory/group_vars/all/download.yml
</code></pre>
<p>下面的示例使用运行在 <code>192.169.101.159:5000</code> 的 image registry：</p>
<pre><code class="language-diff">diff -u ./inventory/group_vars/all/download.yml ./inventory/group_vars/all/new-download.yml
--- ./inventory/group_vars/all/download.yml
+++ ./inventory/group_vars/all/new-download.yml
@@ -16,7 +16,7 @@
 ## Container Registry overrides
 gcr_image_repo: &quot;docker.io/t9kpublic&quot;
 kube_image_repo: &quot;docker.io/t9kpublic&quot;
-docker_image_repo: &quot;docker.io/t9kpublic&quot;
+docker_image_repo: &quot;192.169.101.159:5000/t9kpublic&quot;
 quay_image_repo: &quot;docker.io/t9kpublic&quot;
 # github_image_repo: &quot;{{ registry_host }}&quot;
</code></pre>
<h3 id="设置变量"><a class="header" href="#设置变量">设置变量</a></h3>
<p>在 <code>~/ansible/$T9K_CLUSTER/inventory/inventory.ini</code> 中，将选定的节点放在节点组 <code>nfs_server</code> 中：</p>
<pre><code class="language-ini">[nfs_server] # group nfs_server 仅可设置一个节点，多余的节点会被忽略
node-name
</code></pre>
<h3 id="运行-ansible"><a class="header" href="#运行-ansible">运行 ansible</a></h3>
<p>安装 nfs。注意，需要手工指定 2 个变量的值 <code>nfs_server_ip, nfs_share_network</code>：</p>
<pre><code class="language-bash">cd ~/ansible/$T9K_CLUSTER

# 方法 1: 交互式输入 become password
ansible-playbook ../ks-clusters/t9k-playbooks/10-install-nfs.yml \
  -i inventory/inventory.ini \
  --become -K \
  -e nfs_server_ip=&quot;x.x.x.x&quot; \
  -e nfs_share_network=&quot;x.x.x.x/24&quot;

# 方法 2: 使用 ansible vault 中保存的 become password
ansible-playbook ../ks-clusters/t9k-playbooks/10-install-nfs.yml \
  -i inventory/inventory.ini \
  --become \
  -e &quot;@~/ansible/$T9K_CLUSTER/vault.yml&quot; \
  --vault-password-file=~/ansible/.vault-password.txt \
  -e nfs_server_ip=&quot;x.x.x.x&quot; \
  -e nfs_share_network=&quot;x.x.x.x/24&quot;
</code></pre>
<p>该 ansible 脚本执行如下操作：</p>
<ol>
<li>在所有节点上安装 nfs 的基础包 <code>nfs-common</code></li>
<li>在 nfs_server 节点创建 nfs 共享目录；运行 nfs 服务</li>
<li>在 K8s 集群中安装 NFS CSI Driver，创建 StorageClass <code>nfs-csi</code></li>
<li>运行测试用例
<ol>
<li>使用 storageClass <code>nfs-csi</code> 创建 PVC</li>
<li>创建 Pod 挂载该 PVC
<ol>
<li>向 PVC 路径写入一段特定字符串</li>
<li>Pod 中运行 <code>cat</code> 命令，获取刚写入的文件内容作为 Pod log</li>
</ol>
</li>
<li>等待 Pod 状态变为 <code>Succeeded</code></li>
<li>验证 Pod log 是否和特定字符串一致</li>
<li>删除测试用例</li>
</ol>
</li>
</ol>
<h2 id="验证-10"><a class="header" href="#验证-10">验证</a></h2>
<p>安装过程的最后步骤包含了使用此 NFS 的测试案例，详情见上一小节。</p>
<p>还可以运行如下步骤，手工验证安装的 pacakges 和服务。</p>
<h3 id="检查-package"><a class="header" href="#检查-package">检查 Package</a></h3>
<p>检查节点中的 nfs package：</p>
<ul>
<li>所有集群中的节点（具体节点可参见 <a target="_blank" rel="noopener noreferrer" href="https://github.com/t9k/ks-clusters/blob/master/t9k-playbooks/10-install-nfs.yml#L1">playbook</a>）都需要安装了 nfs-common；</li>
<li>nfs-server 的节点额外需要 nfs-kernel-server。</li>
</ul>
<pre><code class="language-bash"># use apt to show installed pacakgess
apt list --installed | grep nfs

# or, use dpkg
dpkg --get-selections | grep nfs
</code></pre>
<p>输出：</p>
<pre><code class="language-console">libnfs13:amd64					install
libnfsidmap2:amd64				install
nfs-common					install
nfs-kernel-server				install
</code></pre>
<h3 id="测试-nfs-server"><a class="header" href="#测试-nfs-server">测试 NFS server</a></h3>
<p>测试 nfs-server，首先在 nfs-server 中创建文件：</p>
<pre><code class="language-bash"># 进入 nfs_dir 路径，默认值是 /data/nfs_share
cd /data/nfs_share
echo &quot;Hello World!&quot; &gt; test.txt
</code></pre>
<p>然后在另一个 nfs_share_network 地址范围内的节点运行：</p>
<pre><code class="language-bash">sudo mkdir -p /mnt/nfs_client_on_nfs_server

# nfs_server_ip 见上文设置，nfs_dir 默认值为 /data/nfs_share
sudo mount -t nfs &lt;nfs_server_ip&gt;:&lt;nfs_dir&gt; \
  /mnt/nfs_client_on_nfs_server

cat /mnt/nfs_client_on_nfs_server/test.txt
</code></pre>
<p>期望的测试结果：</p>
<pre><code class="language-console">Hello World!
</code></pre>
<p>卸载：</p>
<pre><code class="language-bash">sudo umount /mnt/nfs_client_on_nfs_server
sudo rmdir /mnt/nfs_client_on_nfs_server
</code></pre>
<h3 id="检查-nfs-csi-driver"><a class="header" href="#检查-nfs-csi-driver">检查 NFS CSI Driver</a></h3>
<p>查看 Controller Pod 运行状态：</p>
<pre><code class="language-bash">kubectl -n kube-system get pod -l app=csi-nfs-controller
</code></pre>
<p>输出：</p>
<pre><code class="language-console">NAME                                  READY   STATUS    RESTARTS         AGE
csi-nfs-controller-6b9894ff59-p6fgj   4/4     Running   15 (2d18h ago)   27d
</code></pre>
<p>查看 Node Pods：</p>
<pre><code class="language-bash">kubectl -n kube-system get pod -l app=csi-nfs-node
</code></pre>
<p>输出：</p>
<pre><code class="language-console">NAME                 READY   STATUS    RESTARTS          AGE
csi-nfs-node-sd8vl   3/3     Running   9 (37d ago)       77d
csi-nfs-node-tppsd   3/3     Running   6 (38d ago)       77d
csi-nfs-node-xm94s   3/3     Running   261 (2d18h ago)   77d
</code></pre>
<p>说明如下：</p>
<ul>
<li>csi-nfs-controller 用于处理创建、删除、管理 PV 和 PVC 的请求，期望的 Pod 数量为 1。</li>
<li>csi-nfs-node 用于在每个节点上挂载和卸载存储卷，以支持 Pod 使用 PVC。期望的 Pod 数量与 K8s 集群节点数量相同。</li>
</ul>
<p>检查 K8s 中的 Storage Class：</p>
<pre><code class="language-bash">kubectl get sc
</code></pre>
<p>输出：</p>
<pre><code class="language-console">NAME                PROVISIONER      RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-csi (default)   nfs.csi.k8s.io   Delete          Immediate           false                  14d
</code></pre>
<h2 id="参考-29"><a class="header" href="#参考-29">参考</a></h2>
<p><a href="https://github.com/kubernetes-csi/csi-driver-nfs">https://github.com/kubernetes-csi/csi-driver-nfs</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="ceph"><a class="header" href="#ceph">Ceph</a></h1>
<p>请参阅： <a href="https://t9k.github.io/ceph-admin-docs/installation.html">https://t9k.github.io/ceph-admin-docs/installation.html</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="lustre"><a class="header" href="#lustre">Lustre</a></h1>
<h2 id="参考-30"><a class="header" href="#参考-30">参考</a></h2>
<p><a href="https://github.com/DDNStorage/exa-csi-driver">https://github.com/DDNStorage/exa-csi-driver</a></p>
<p><a href="https://github.com/lustre/lustre-release">https://github.com/lustre/lustre-release</a></p>
<p><a href="https://github.com/DDNStorage/LustrePerfMon">https://github.com/DDNStorage/LustrePerfMon</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="gpfs"><a class="header" href="#gpfs">GPFS</a></h1>
<h2 id="参考-31"><a class="header" href="#参考-31">参考</a></h2>
<p><a href="https://github.com/IBM/ibm-spectrum-scale-csi">https://github.com/IBM/ibm-spectrum-scale-csi</a></p>
<p><a href="https://www.ibm.com/docs/it/scalecsi?topic=200-introduction-spectrum-scale-container-storage-interface-driver">https://www.ibm.com/docs/it/scalecsi?topic=200-introduction-spectrum-scale-container-storage-interface-driver</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="离线安装-1"><a class="header" href="#离线安装-1">离线安装</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="准备离线安装包"><a class="header" href="#准备离线安装包">准备离线安装包</a></h1>
<p>准备离线安装包的节点需满足以下要求：</p>
<ol>
<li>能够连接到互联网</li>
<li>与计划安装 K8s 集群/TensorStack 产品的 “目标节点” 操作系统相同：当前只支持 Ubuntu 20.04 或者 22.04</li>
<li>该节点可用存储空间大于 200 GB</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="kubespray"><a class="header" href="#kubespray">Kubespray</a></h1>
<pre><code>TODO:
    1. 增加验证部分；
</code></pre>
<p>本文准备的离线文件：</p>
<div class="table-wrapper"><table><thead><tr><th>内容</th><th>存放路径</th></tr></thead><tbody>
<tr><td>apt packages</td><td>apt-packages/</td></tr>
<tr><td>pypi 包</td><td>python-packages/</td></tr>
<tr><td>Server 用容器镜像</td><td>server-images/</td></tr>
<tr><td>文件</td><td>offline-files/</td></tr>
<tr><td>其他容器镜像</td><td>可修改，默认值为 container-images/</td></tr>
</tbody></table>
</div>
<h2 id="准备"><a class="header" href="#准备">准备</a></h2>
<p>获取 <a target="_blank" rel="noopener noreferrer" href="https://github.com/t9k/ks-clusters/tree/master">ks-cluster</a> 项目：</p>
<pre><code class="language-bash">mkdir -p ~/ansible &amp;&amp; cd ~/ansible
git clone https://github.com/t9k/ks-clusters.git

cd ~/ansible/ks-clusters/tools/offline-k8s
</code></pre>
<h2 id="下载"><a class="header" href="#下载">下载</a></h2>
<h3 id="apt-包"><a class="header" href="#apt-包">apt 包</a></h3>
<p>确认下载内容：</p>
<pre><code class="language-bash">cat pkglist/ubuntu/pkgs.txt
cat pkglist/ubuntu/20.04/pkgs.txt
cat pkglist/ubuntu/22.04/pkgs.txt
</code></pre>
<p>如需离线安装 NVIDIA Driver，可以编辑 <code>pkglist/ubuntu/pkgs.txt</code>，添加以下内容：</p>
<pre><code class="language-bash"># nvidia driver
nvidia-driver-525-server
</code></pre>
<p>运行脚本下载 apt repositories，下载目录为 <code>apt-packages</code>：</p>
<pre><code class="language-bash">./create-repo.sh
</code></pre>
<p>如果报错 <code>Can't find a source to download version '5:24.0.7-1~ubuntu.20.04~focal' of 'docker-ce:amd64'</code>，运行以下命令，增加 docker 的 apt source 后再试一次：</p>
<pre><code class="language-bash">cat &gt; download_docker_com_linux_ubuntu.list &lt;&lt; EOF
deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable
EOF

sudo mv download_docker_com_linux_ubuntu.list \
  /etc/apt/sources.list.d/download_docker_com_linux_ubuntu.list
sudo apt update
</code></pre>
<h3 id="python-包"><a class="header" href="#python-包">Python 包</a></h3>
<p>确认下载内容：</p>
<pre><code class="language-bash">cat ~/ansible/kubespray/requirements.txt
</code></pre>
<p>下载 python packages：</p>
<pre><code class="language-bash"># 如遇到网络联通性问题，考虑使用代理： -i https://pypi.tuna.tsinghua.edu.cn/simple
python3 -m pip download \
  -d python-packages \
  -r ~/ansible/kubespray/requirements.txt
</code></pre>
<h3 id="nginx--registry-server-镜像"><a class="header" href="#nginx--registry-server-镜像">nginx / registry server 镜像</a></h3>
<p>这两个镜像用于在 “控制节点” 上创建 server 来提供离线内容：</p>
<pre><code>docker.io/t9kpublic/nginx:offline-2023-09
docker.io/t9kpublic/registry:offline-2023-09
</code></pre>
<p>为了方便使用，我们单独下载这两个镜像。</p>
<pre><code class="language-bash">mkdir server-images &amp;&amp; cd server-images

sudo docker pull docker.io/t9kpublic/nginx:offline-2023-09
sudo docker save docker.io/t9kpublic/nginx:offline-2023-09 \
  -o docker.io-t9kpublic-nginx-offline-2023-09.tar

sudo docker pull docker.io/t9kpublic/registry:offline-2023-09
sudo docker save docker.io/t9kpublic/registry:offline-2023-09 \
  -o docker.io-t9kpublic-registry-offline-2023-09.tar

cd ~/ansible/ks-clusters/tools/offline-k8s
</code></pre>
<h3 id="文件"><a class="header" href="#文件">文件</a></h3>
<aside class="note">
<div class="title">注意</div>
<p>下面需要设置环境变量 <code>K8S_VER</code> 为实际的 k8s 发布版本，例如 <code>1.22.0</code>、<code>1.25.9</code> 等。</p>
</aside>
<p>查看下载文件列表，文件列表的生成方式见 <a href="offline/prepare-offline-packages/../../appendix/generate-k8s-file-and-image-list.html">附录：生成 K8s 文件和镜像列表</a>：</p>
<pre><code class="language-bash">K8S_VER=1.22.0
cat filelist/k8s-${K8S_VER}.list
</code></pre>
<p>根据 files.list 下载 files，下载目录为 <code>offline-files</code>：</p>
<pre><code class="language-bash">./download-offline-files.sh --config filelist/k8s-${K8S_VER}.list
</code></pre>
<p>如果使用了 <code>mirror.ghproxy.com </code>作为 github 的代理，需要修改目录的路径：</p>
<pre><code class="language-bash">mv offline-files/mirror.ghproxy.com/https\:/github.com \
  offline-files/github.com
</code></pre>
<h3 id="镜像"><a class="header" href="#镜像">镜像</a></h3>
<p>查看镜像列表，镜像列表的生成方式见 <a href="offline/prepare-offline-packages/../../appendix/generate-k8s-file-and-image-list.html">附录：生成 K8s 文件和镜像列表</a>：</p>
<pre><code class="language-bash">cat imagelist/k8s-${K8S_VER}.list
</code></pre>
<p>根据 images.list 下载 images，并指定下载目录为 container-images：</p>
<pre><code class="language-bash">./manage-offline-container-images.sh --option create \
  --config imagelist/k8s-${K8S_VER}.list --dir container-images
</code></pre>
<h2 id="验证-11"><a class="header" href="#验证-11">验证</a></h2>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="k8s-组件离线安装准备"><a class="header" href="#k8s-组件离线安装准备">K8s 组件离线安装准备</a></h1>
<pre><code>TODO:
    1. 增加验证部分；
</code></pre>
<p>本文准备的离线文件：</p>
<div class="table-wrapper"><table><thead><tr><th>内容</th><th>存放路径</th></tr></thead><tbody>
<tr><td>容器镜像（images）</td><td>可修改，默认值为 container-images/</td></tr>
<tr><td>Helm Charts</td><td>charts/</td></tr>
<tr><td>其他</td><td>misc/</td></tr>
</tbody></table>
</div>
<blockquote>
<p>说明：“其他” 中包含 <code>istio</code> 的命令行工具，<code>minio</code> 的 apt 包。</p>
</blockquote>
<h2 id="准备-1"><a class="header" href="#准备-1">准备</a></h2>
<p>切换目录，设置 K8s 版本：</p>
<pre><code class="language-bash">cd ~/ansible/ks-clusters/tools/offline-additionals

K8S_VER=1.22.0
</code></pre>
<h2 id="下载-1"><a class="header" href="#下载-1">下载</a></h2>
<h3 id="容器镜像"><a class="header" href="#容器镜像">容器镜像</a></h3>
<p>查看镜像列表，镜像列表的生成方式见 <a href="offline/prepare-offline-packages/../../appendix/generate-k8s-file-and-image-list.html">附录：生成 K8s 文件和镜像列表</a>：</p>
<pre><code class="language-bash">cat imagelist/k8s-${K8S_VER}.list
</code></pre>
<p>根据 <code>images.list</code> 下载 images，并指定下载目录为 <code>container-images</code>：</p>
<pre><code class="language-bash">./manage-offline-container-images.sh \
  --option create \
  --config imagelist/k8s-${K8S_VER}.list \
  --dir container-images
</code></pre>
<h3 id="helm-charts"><a class="header" href="#helm-charts">Helm Charts</a></h3>
<p>查看下载版本：</p>
<pre><code class="language-bash">cat productlist/k8s-${K8S_VER}.list
</code></pre>
<p>下载 Helm Chart，下载目录为 <code>charts</code>：</p>
<pre><code class="language-bash">./download-charts.sh --config productlist/k8s-${K8S_VER}.list
</code></pre>
<h3 id="minio-的-apt-包"><a class="header" href="#minio-的-apt-包">minio 的 apt 包</a></h3>
<p>下载 minio：</p>
<pre><code class="language-bash">mkdir misc
wget https://dl.min.io/server/minio/release/linux-amd64/archive/minio_20231007150738.0.0_amd64.deb \
    -O misc/minio.deb
</code></pre>
<h3 id="istio-命令行工具"><a class="header" href="#istio-命令行工具">istio 命令行工具</a></h3>
<p>下载 istio 命令行工具：</p>
<pre><code class="language-bash">wget https://mirror.ghproxy.com/https://github.com/istio/istio/releases/download/1.15.2/istio-1.15.2-linux-amd64.tar.gz \
    -O misc/istio-1.15.2-linux-amd64.tar.gz
</code></pre>
<h2 id="验证-12"><a class="header" href="#验证-12">验证</a></h2>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="产品"><a class="header" href="#产品">产品</a></h1>
<pre><code>TODO:
    1. 增加验证部分；
</code></pre>
<p>本文准备的离线文件：</p>
<div class="table-wrapper"><table><thead><tr><th>内容</th><th>存放路径</th></tr></thead><tbody>
<tr><td>Helm Charts</td><td>charts/</td></tr>
<tr><td>容器镜像（images）</td><td>可修改，默认值为 container-images/</td></tr>
<tr><td>域名证书</td><td>certs/</td></tr>
<tr><td>其他</td><td>misc/</td></tr>
</tbody></table>
</div>
<blockquote>
<p>说明：“其他” 中包含 Serving 使用的镜像，kubectl 和 helm 命令行工具，用于保障单独部署 T9k 产品时这些功能可以正常使用。</p>
</blockquote>
<h2 id="准备-2"><a class="header" href="#准备-2">准备</a></h2>
<pre><code class="language-bash">cd ~/ansible/ks-clusters/tools/offline-t9k
</code></pre>
<h2 id="下载-2"><a class="header" href="#下载-2">下载</a></h2>
<h3 id="helm-chart"><a class="header" href="#helm-chart">Helm Chart</a></h3>
<p>查看下载版本：</p>
<pre><code class="language-bash">cat productlist/t9k-2024-01-12.list
</code></pre>
<p>下载 Helm Chart，下载目录为 <code>charts</code>：</p>
<pre><code class="language-bash">./download-charts.sh --config productlist/t9k-2024-02-01.list
</code></pre>
<h3 id="镜像-1"><a class="header" href="#镜像-1">镜像</a></h3>
<aside class="note">
<div class="title">注意</div>
<ol>
<li>如果您需要修改 values.yaml 中对容器镜像的设置，您需要自行<a href="offline/prepare-offline-packages/../../appendix/generate-t9k-product-image-list.html">准备 T9k 产品镜像列表</a>。</li>
<li>这个步骤的下载耗时很长，建议使用 root 用户下载，以避免多次输入 sudo 密码的需求。</li>
<li>需要确保剩余的可用存储空间大于 200 GB。</li>
</ol>
</aside>
<p>查看镜像列表：</p>
<pre><code class="language-bash">cat imagelist/t9k-2024-02-01.list
</code></pre>
<p>下载镜像，下载目录为 <code>container-images</code>：</p>
<pre><code class="language-bash">./manage-offline-container-images.sh \
  --option create \
  --config imagelist/t9k-2024-01-12.list \
  --dir container-images
</code></pre>
<h3 id="域名证书"><a class="header" href="#域名证书">域名证书</a></h3>
<p>准备一份域名证书。参考<a href="offline/prepare-offline-packages/../../appendix/manage-domain-certificate.html">管理域名证书</a>生成域名证书。然后复制到目录中：</p>
<pre><code class="language-bash">mkdir certs
cp -r ~/.acme.sh/\*.sample.t9kcloud.cn_ecc  certs/
</code></pre>
<h3 id="registry-镜像"><a class="header" href="#registry-镜像">Registry 镜像</a></h3>
<p>下载 Registry 镜像：</p>
<pre><code class="language-bash">mkdir misc &amp;&amp; cd misc
sudo docker pull docker.io/t9kpublic/registry:offline-2023-09
sudo docker save docker.io/t9kpublic/registry:offline-2023-09 \
    -o docker.io-t9kpublic-registry-offline-2023-09.tar
</code></pre>
<h3 id="命令行工具"><a class="header" href="#命令行工具">命令行工具</a></h3>
<p>下载 <code>kubectl</code> 和 <code>helm</code> ：</p>
<pre><code class="language-bash">wget -O kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.25.9/bin/linux/amd64/kubectl

wget -O helm-v3.12.0-linux-amd64.tar.gz \
  https://get.helm.sh/helm-v3.12.0-linux-amd64.tar.gz 
</code></pre>
<h2 id="验证-13"><a class="header" href="#验证-13">验证</a></h2>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装-10"><a class="header" href="#安装-10">安装</a></h1>
<h2 id="环境要求"><a class="header" href="#环境要求">环境要求</a></h2>
<p>TensorStack AI 平台的离线部署对环境有以下要求：</p>
<ol>
<li>一个或多个“目标节点”，这些节点用来组建 K8s 集群。
<ol>
<li>这些节点的操作系统要求是 Ubuntu；</li>
<li>这些节点和“控制节点”之间可以通过网络访问。</li>
</ol>
</li>
<li>一个“控制节点”（ansible 的控制节点，与 K8s 无关），用来运行 ansible 脚本，并在离线环境中提供所有下载内容。
<ol>
<li>OS 是 Ubuntu (推荐 22.04）；</li>
<li>可以不连接外部网络，支持通过移动硬盘、本地网络等途径从外部传入数据（离线安装包）即可；</li>
<li>和 “目标节点” 之间可以通过网络访问；</li>
<li>如需离线部署 K8s，则该节点不能加入 K8s 集群；</li>
<li>可用存储空间大于 500 GB。</li>
</ol>
</li>
<li>非测试部署时，离线环境中应存在一个可用的 DNS server。
<ol>
<li>TensorStack AI 平台的服务需要通过域名访问，因此需要域名解析服务。</li>
</ol>
</li>
</ol>
<h2 id="复制离线安装包"><a class="header" href="#复制离线安装包">复制离线安装包</a></h2>
<p>通过移动硬盘等途径，将<a href="offline/install/../prepare-offline-packages/index.html">准备的离线安装包</a>传输到 “控制节点” 的 <code>~/ansible</code> 路径。</p>
<p>复制命令：</p>
<pre><code class="language-bash">mkdir ~/ansible &amp;&amp; cd ~/ansible

# 从移动硬盘复制离线安装包到控制节点中
rsync -aP &lt;path-to-media&gt;/ks-clusters ~/ansible/
rsync -aP &lt;path-to-media&gt;/kubespray ~/ansible/
</code></pre>
<p>其中 <code>./ks-clusters</code> 包含 <a target="_blank" rel="noopener noreferrer" href="https://github.com/t9k/ks-clusters">ks-clusters</a> + <a href="offline/install/../prepare-offline-packages/index.html">准备的离线安装包</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="k8s"><a class="header" href="#k8s">K8s</a></h1>
<pre><code>TODO:
    1. &quot;运行 NGINX 和上传镜像&quot; 中需明确如何启动本地 Registry
    2. “手动设置” 部分，为支持很多节点集群，改为使用 ansible
</code></pre>
<h2 id="检查离线安装包"><a class="header" href="#检查离线安装包">检查离线安装包</a></h2>
<p><code>ks-clusters/tools/offline-k8s</code> 中准备的离线文件一览：</p>
<div class="table-wrapper"><table><thead><tr><th>内容</th><th>存放路径</th></tr></thead><tbody>
<tr><td>1. apt packages</td><td>apt-packages/</td></tr>
<tr><td>2. pypi 包</td><td>python-packages/</td></tr>
<tr><td>3. 容器镜像（nginx, registry）</td><td>server-images/</td></tr>
<tr><td>4. 容器镜像（其他）</td><td>container-images/</td></tr>
<tr><td>5. 一些可执行文件</td><td>offline-files/</td></tr>
</tbody></table>
</div>
<h2 id="运行-nginx-和上传镜像"><a class="header" href="#运行-nginx-和上传镜像">运行 NGINX 和上传镜像</a></h2>
<p>1）如果 “控制节点” 未安装 docker，请先使用 dpkg 命令安装 Docker（具体安装包的内容请查看已有的内容）：</p>
<pre><code class="language-bash">cd ~/ansible/ks-clusters/tools/offline-k8s/apt-packages/debs/local/pkgs

# 查看其中 apt 包的版本
ls
# 根据看到的版本进行安装
sudo dpkg -i containerd.io_1.6.25-1_amd64.deb
sudo dpkg -i docker-ce-cli_5%3a24.0.7-1~ubuntu.20.04~focal_amd64.deb
sudo dpkg -i docker-ce_5%3a24.0.7-1~ubuntu.20.04~focal_amd64.deb
</code></pre>
<p>启用 docker：</p>
<pre><code class="language-bash">sudo systemctl enable --now docker
</code></pre>
<p>验证：</p>
<pre><code class="language-bash">sudo docker info
</code></pre>
<p>2）进入 offline-k8s 目录：</p>
<pre><code class="language-bash">cd ~/ansible/ks-clusters/tools/offline-k8s
</code></pre>
<p>3）装载 NGINX 和 Registry 镜像：</p>
<pre><code class="language-bash">sudo docker load \
  -i ./server-images/docker.io-t9kpublic-nginx-offline-2023-09.tar

sudo docker load \
  -i ./server-images/docker.io-t9kpublic-registry-offline-2023-09.tar
</code></pre>
<p>4）运行一个 nginx（默认 8080 端口），来 serve 保存的文件（offline-files）和 apt 包：</p>
<pre><code class="language-bash">./serve-offline-files.sh
</code></pre>
<p>5）如果离线环境中已经存在镜像仓库服务，我们用 <code>&lt;registry&gt;</code> 指代该镜像仓库服务的域名或 IP 地址以及服务端口，<code>&lt;any-prefix&gt;</code> 是任意名称前缀。您需要配置控制节点和镜像仓库，来满足以下条件：</p>
<ol>
<li>控制节点和 K8s 集群中的节点可以访问该镜像仓库</li>
<li>控制节点有权限向镜像仓库的地址 <code>&lt;registry&gt;/&lt;any-prefix&gt;/t9kpublic</code> 上传镜像
<ol>
<li>如果条件允许，推荐省略 <code>/&lt;any-prefix&gt;</code>，直接使用 <code>&lt;registry&gt;/t9kpublic</code></li>
</ol>
</li>
<li>K8s 集群中的节点有权限拉取第 2 步上传的镜像</li>
</ol>
<p>验证上述需求：</p>
<pre><code class="language-bash"># 在控制节点测试上传镜像
sudo docker tag t9kpublic/registry:offline-2023-09 \
  &lt;registry&gt;/&lt;any-prefix&gt;/t9kpublic/registry:offline-2023-09
sudo docker push &lt;registry&gt;/&lt;any-prefix&gt;/t9kpublic/registry:offline-2023-09

# 在 K8s 节点中测试下载镜像
sudo docker pull &lt;registry&gt;/&lt;any-prefix&gt;/t9kpublic/registry:offline-2023-09
</code></pre>
<p>在控制节点中运行命令，上传（注册, <code>--option register</code>）镜像到镜像仓库服务中：</p>
<pre><code class="language-bash">./manage-offline-container-images.sh \
  --option register --registry &lt;registry&gt;/&lt;any-prefix&gt;
</code></pre>
<p>在使用已有的镜像仓库服务时，下文所有的镜像仓库地址 <code>&lt;control-node-ip&gt;:5000</code> 都需要替换为 <code>&lt;registry&gt;/&lt;any-prefix&gt;</code>。</p>
<p>6）如果离线环境中没有可用的镜像仓库服务，则运行一个容器 Registry（默认 5000 端口）服务，并将 container-images/ 中的镜像上传到该 Registry 中：</p>
<pre><code class="language-bash">$ ./manage-offline-container-images.sh --option register
</code></pre>
<p>补充说明：</p>
<ol>
<li>当名称为“registry”的容器已经存在时，运行该脚本不会创建新的 Registry，而是向 localhost:5000 上传镜像。</li>
<li>如果要向其他地址上传镜像，可以用命令行参数 <code>--registry</code> 来指定，示例见第 5）步。</li>
</ol>
<h2 id="验证-nginx-和-registry-服务"><a class="header" href="#验证-nginx-和-registry-服务">验证 NGINX 和 Registry 服务</a></h2>
<p>在 “控制节点” 进行测试，验证 NGINX 和 Registry 的可用性。</p>
<h3 id="验证-apt-服务"><a class="header" href="#验证-apt-服务">验证 apt 服务</a></h3>
<p>获取 apt 包的信息：</p>
<pre><code class="language-bash">curl http://&lt;hostname&gt;:8080/debs/local/Packages
</code></pre>
<h3 id="验证文件下载服务"><a class="header" href="#验证文件下载服务">验证文件下载服务</a></h3>
<p>通过查看文件夹和文件名称，来确认节点上 cri-tools 的版本信息：</p>
<pre><code class="language-bash">ls offline-files/github.com/kubernetes-sigs/cri-tools/releases/download
</code></pre>
<p>根据上文获得的版本信息（例如 <code>v1.25.0/crictl-v1.25.0-linux-amd64.tar.gz</code>），下载 crictl 的压缩包：</p>
<pre><code class="language-bash">curl http://&lt;hostname&gt;:8080/github.com/kubernetes-sigs/cri-tools/releases/download/v1.25.0/crictl-v1.25.0-linux-amd64.tar.gz \
    -o ./crictl-v1.25.0-linux-amd64.tar.gz
</code></pre>
<h3 id="验证-registry-服务"><a class="header" href="#验证-registry-服务">验证 Registry 服务</a></h3>
<p>查看镜像版本：</p>
<pre><code class="language-bash">ls container-images | grep etcd
</code></pre>
<p>下载镜像：</p>
<pre><code class="language-bash">sudo docker pull &lt;hostname&gt;:5000/t9kpublic/etcd:v3.5.6
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>如果遇到错误信息 <code>server gave HTTP response to HTTPS client</code>，可以参考 <a href="offline/install/../../appendix/configure-docker-insecure-registry.html">附录：配置 Docker Insecure Registry</a> 解决。</p>
</aside>
<h2 id="配置-kubespray-运行环境"><a class="header" href="#配置-kubespray-运行环境">配置 kubespray 运行环境</a></h2>
<p>1）如果控制节点未安装 python 和 pip，请先进行安装。首先确认是否已经安装了 python：</p>
<pre><code class="language-bash">python3 --version
</code></pre>
<p>配置 apt 设置：</p>
<pre><code class="language-bash">sudo cat &gt; /etc/apt/sources.list.d/offline.list &lt;&lt; EOF
deb [trusted=yes] http://&lt;control-node-ip&gt;:8080/debs/local/ ./
EOF

sudo mv /etc/apt/sources.list ~/
</code></pre>
<p>安装：</p>
<pre><code class="language-bash">sudo apt update &amp;&amp; sudo apt install python3 python3-venv python3-pip
</code></pre>
<p>[可选] 恢复 apt 配置：</p>
<pre><code class="language-bash">sudo rm -rf /etc/apt/sources.list.d/offline.list
sudo mv ~/sources.list /etc/apt/ 
sudo apt update
</code></pre>
<p>2）将 kubespray 切换到合适分支：</p>
<pre><code class="language-bash"># 将 kubespray 切换到合适分支，例如 origin/kubernetes-offline-1.25.9
cd ~/ansible/kubespray
git checkout -b kubernetes-offline-&lt;version&gt; \
    origin/kubernetes-offline-&lt;version&gt;
cd ..
</code></pre>
<p>3）然后参考<a href="offline/install/../../online/inventory/basic-settings.html">基本设置</a>，完成安装 ansible 和准备 inventory。</p>
<p>如果离线环境中没有 conda，则使用下面命令安装 ansible 运行环境：</p>
<pre><code class="language-bash">python3 -m venv kubespray-venv
source kubespray-venv/bin/activate

python3 -m pip install --no-index \
  --find-links=python-packages -r kubespray/requirements.txt
</code></pre>
<p>验证：</p>
<pre><code class="language-bash">ansible --version
</code></pre>
<h2 id="手动设置"><a class="header" href="#手动设置">手动设置</a></h2>
<p>在每个计划安装 K8s 的 “目标节点”中，做以下 apt source 设置。其中 <control-node-ip> 为“控制节点”的 IP 地址：</p>
<pre><code class="language-bash">sudo cat &gt; /etc/apt/sources.list.d/offline.list &lt;&lt; EOF
deb [trusted=yes] http://&lt;control-node-ip&gt;:8080/debs/local/ ./
EOF

sudo mv /etc/apt/sources.list ~/
</code></pre>
<h3 id="安装-docker"><a class="header" href="#安装-docker">安装 Docker</a></h3>
<p>为节点安装 docker：</p>
<pre><code class="language-bash">sudo apt update
sudo apt install docker-ce docker-ce-cli containerd.io
sudo systemctl enable --now docker
</code></pre>
<p>配置 docker：</p>
<pre><code class="language-bash">sudo cat &gt; /etc/docker/daemon.json &lt;&lt; EOF
{
  &quot;insecure-registries&quot; : [&quot;&lt;control-node-ip&gt;:5000&quot;]
}
EOF

sudo systemctl restart docker
</code></pre>
<p>测试拉取镜像（根据 <a href="offline/install/k8s.html#%E9%AA%8C%E8%AF%81-registry-%E6%9C%8D%E5%8A%A1">验证 Registry 服务</a>的结果灵活调整 etcd 镜像的 tag）：</p>
<pre><code class="language-bash">sudo docker pull &lt;control-node-ip&gt;:5000/t9kpublic/etcd:v3.5.6
</code></pre>
<h3 id="可选-安装-nvidia-driver"><a class="header" href="#可选-安装-nvidia-driver">[可选] 安装 NVIDIA Driver</a></h3>
<p>如需离线安装 NVIDIA Driver，可以运行以下命令（要求在 <a href="offline/install/../prepare-offline-packages/kubespray.html#%E4%B8%8B%E8%BD%BD-apt-%E5%8C%85">准备离线安装包</a>时包含了该 package）：</p>
<pre><code class="language-bash">sudo apt install -y nvidia-driver-525-server
</code></pre>
<p>安装完成后，需要参考 <a href="offline/install/../../online/">NVIDIA GPU Operator</a> 进行 Post Install 设置。</p>
<h2 id="安装-k8s-集群-1"><a class="header" href="#安装-k8s-集群-1">安装 K8s 集群</a></h2>
<p>1）编辑 (<code>~/ansible/$T9K_CLUSTER/inventory/group_vars/all/download.yml</code>)，设置以下变量。其中 <code>&lt;control-node-ip&gt;</code> 为“控制节点”的 IP 地址：</p>
<pre><code class="language-yaml">files_repo: &quot;http://&lt;control-node-ip&gt;:8080&quot;
gcr_image_repo: &quot;&lt;control-node-ip&gt;:5000/t9kpublic&quot;
kube_image_repo: &quot;&lt;control-node-ip&gt;:5000/t9kpublic&quot;
docker_image_repo: &quot;&lt;control-node-ip&gt;:5000/t9kpublic&quot;
quay_image_repo: &quot;&lt;control-node-ip&gt;:5000/t9kpublic&quot;

kubeadm_download_url: &quot;http://&lt;control-node-ip&gt;:8080/storage.googleapis.com/kubernetes-release/release/{{ kubeadm_version }}/bin/linux/{{ image_arch }}/kubeadm&quot;
kubelet_download_url: &quot;http://&lt;control-node-ip&gt;:8080/storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/{{ image_arch }}/kubelet&quot;
kubectl_download_url: &quot;http://&lt;control-node-ip&gt;:8080/storage.googleapis.com/kubernetes-release/release/{{ kube_version }}/bin/linux/{{ image_arch }}/kubectl&quot;
helm_download_url: &quot;http://&lt;control-node-ip&gt;:8080/get.helm.sh/helm-{{ helm_version }}-linux-{{ image_arch }}.tar.gz&quot;
</code></pre>
<p>另外，需要考虑离线环境的特殊设置，例如修改 <code>upstream_dns_servers</code> 的设置。</p>
<p>2）参考 <a href="offline/install/../../online/prepare-nodes.html">准备节点</a> 与 <a href="offline/install/../../online/k8s-index.html">安装 K8s</a>，完成节点配置和安装 K8s 集群。</p>
<h2 id="设置-kubeconfig-并验证"><a class="header" href="#设置-kubeconfig-并验证">设置 KUBECONFIG 并验证</a></h2>
<p>1）若“控制节点”未安装命令行工具 kubectl 和 Helm，需要先行安装。</p>
<p>Kubespray 会为 K8s 集群 master 节点的 root 用户安装 kubectl 和 helm。但是不会为 ansible 控制节点安装 kubectl 和 helm。我们需要为 ansible 控制节点安装 kubectl 和 helm。</p>
<p>安装 kubectl（如果使用的不是 <code>storage.googleapis.com</code> 下载源，请相应地更换路径）：</p>
<pre><code class="language-bash"># 复制本地文件
cp ~/ansible/ks-clusters/tools/offline/offline-files/storage.googleapis.com/kubernetes-release/release/v1.25.9/bin/linux/amd64/kubectl  .
</code></pre>
<p>或者通过 nginx 服务下载：</p>
<pre><code class="language-bash">wget http://&lt;control-node-ip&gt;:8080/storage.googleapis.com/kubernetes-release/release/v1.25.9/bin/linux/amd64/kubectl

# 移动到 PATH 路径
chmod +x kubectl
sudo mv kubectl /usr/local/bin/kubectl
</code></pre>
<p>安装 helm（请根据版本、操作系统的不同灵活修改路径）：</p>
<pre><code class="language-bash"># 复制本地文件
cp ~/ansible/ks-clusters/tools/offline/offline-files/get.helm.sh/helm-v3.12.0-linux-amd64.tar.gz .
</code></pre>
<p>或者通过 nginx 服务下载：</p>
<pre><code class="language-bash">wget http://&lt;control-node-ip&gt;:8080/get.helm.sh/helm-v3.12.0-linux-amd64.tar.gz

# 解压并移动到 PATH 路径
tar zxvf helm-v3.12.0-linux-amd64.tar.gz
sudo mv linux-amd64/helm /usr/local/bin/helm
rm -rf linux-amd64/
</code></pre>
<p>2）设置 KUBECONFIG：</p>
<pre><code class="language-bash">mkdir -p ~/.kube/

cp ~/ansible/$T9K_CLUSTER/inventory/artifacts/admin.conf \
  ~/.kube/sample.conf
export KUBECONFIG=~/.kube/sample.conf
</code></pre>
<p>3）验证：</p>
<pre><code class="language-bash">kubectl get pod -A -o wide
kubectl get node -o wide
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="k8s-组件"><a class="header" href="#k8s-组件">K8s 组件</a></h1>
<h2 id="检查离线安装包-1"><a class="header" href="#检查离线安装包-1">检查离线安装包</a></h2>
<p><code>ks-clusters/tools/offline-additionals</code> 中提前准备的离线文件一览：</p>
<div class="table-wrapper"><table><thead><tr><th>内容</th><th>存放路径</th></tr></thead><tbody>
<tr><td>1. 镜像（images）</td><td>container-images/</td></tr>
<tr><td>2. Helm Chart</td><td>charts/</td></tr>
<tr><td>3. 其他</td><td>misc/</td></tr>
</tbody></table>
</div>
<h2 id="上传镜像"><a class="header" href="#上传镜像">上传镜像</a></h2>
<p>1）确认已经运行了 Registry 服务，详见 <a href="offline/install/./k8s.html#%E8%BF%90%E8%A1%8C-nginx-%E5%92%8C%E4%B8%8A%E4%BC%A0%E9%95%9C%E5%83%8F">运行 NGINX 和上传镜像</a>。</p>
<p>2）将保存的镜像上传到 Registry 中：</p>
<pre><code class="language-bash">cd ~/ansible/ks-clusters/tools/offline-additionals
./manage-offline-container-images.sh --option register
</code></pre>
<h2 id="安装组件"><a class="header" href="#安装组件">安装组件</a></h2>
<p>首先将 minio 复制到计划安装 minio 的存储节点上：</p>
<pre><code class="language-bash">rsync -aP misc/minio.deb &lt;user&gt;@&lt;minio-node-ip&gt;:~/
</code></pre>
<p>参考以下文档进行安装：</p>
<ul>
<li><a href="offline/install/../../online/storage-service/minio.html">minio</a></li>
<li><a href="offline/install/../../online/storage-service/nfs.html">NFS</a></li>
<li><a href="offline/install/../../online/k8s-components/index.html">K8s 组件</a></li>
</ul>
<aside class="note">
<div class="title">注意</div>
<ol>
<li>需要执行标注了 [离线安装场景] 的修改。</li>
<li>部分离线安装命令与在线安装命令不同，注意 <code>“# offline install”</code> 的注释。</li>
</ol>
</aside>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="产品-1"><a class="header" href="#产品-1">产品</a></h1>
<h2 id="检查安装包"><a class="header" href="#检查安装包">检查安装包</a></h2>
<p><code>ks-clusters/tools/offline-t9k</code> 中提前准备的离线文件一览：</p>
<div class="table-wrapper"><table><thead><tr><th>内容</th><th>存放路径</th></tr></thead><tbody>
<tr><td>1. Helm Chart</td><td>charts/</td></tr>
<tr><td>2. 镜像（images）</td><td>container-images/</td></tr>
<tr><td>3. 域名证书</td><td>certs/</td></tr>
<tr><td>4. 其他</td><td>misc/</td></tr>
</tbody></table>
</div>
<blockquote>
<p>说明：“其他” 中包含 Serving 使用的镜像，kubectl 和 helm 命令行工具，用于保障单独部署 T9k 产品时这些功能可以正常使用。</p>
</blockquote>
<h2 id="上传镜像-1"><a class="header" href="#上传镜像-1">上传镜像</a></h2>
<p>1）进入 offline-t9k 目录：</p>
<pre><code class="language-bash">cd ~/ansible/ks-clusters/tools/offline-t9k
</code></pre>
<p>2）如果离线环境中不存在镜像仓库服务，或者仅存在 <a href="offline/install/./k8s.html#%E8%BF%90%E8%A1%8C-nginx-%E5%92%8C%E4%B8%8A%E4%BC%A0%E9%95%9C%E5%83%8F">运行 NGINX 和上传镜像</a> 创建的镜像仓库。</p>
<p>（可选）如果 Registry 镜像不存在，则装载 Registry 镜像：</p>
<pre><code class="language-bash">sudo docker load -i ./misc/docker.io-t9kpublic-registry-offline-2023-09.tar
</code></pre>
<p>运行一个 Registry（默认 5000 端口），并上传（注册, --option register）镜像到 Registry 中（注意，该步骤耗时较长）：</p>
<pre><code class="language-bash">./manage-offline-container-images.sh --option register
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>当名称为 Registry 的容器已经存在时，运行该脚本不会创建新的 Registry，而是向已经存在的 Registry 上传镜像。</p>
</aside>
<p>3）如果离线环境中已经存在其他镜像仓库服务 ，我们用 <code>&lt;registry&gt;</code> 指代该镜像仓库服务的域名或 IP 地址以及服务端口，<code>&lt;any-prefix&gt;</code> 是任意名称前缀。您需要配置控制节点和镜像仓库，来满足以下条件：</p>
<ol>
<li>控制节点和 K8s 集群中的节点可以访问该镜像仓库</li>
<li>控制节点有权限向镜像仓库的地址 <code>&lt;registry&gt;/&lt;any-prefix&gt;/t9kpublic</code> 上传镜像
<ol>
<li>如果条件允许，推荐省略 <code>/&lt;any-prefix&gt;</code>，直接使用 <code>&lt;registry&gt;/t9kpublic</code></li>
</ol>
</li>
<li>K8s 集群中的节点有权限拉取第 2 步上传的镜像</li>
</ol>
<p>验证上述需求：</p>
<pre><code class="language-bash"># 在控制节点测试上传镜像
sudo docker load -i ./server-images/docker.io-t9kpublic-registry-offline-2023-09.tar
sudo docker tag t9kpublic/registry:offline-2023-09 \
    &lt;registry&gt;/&lt;any-prefix&gt;/t9kpublic/registry:offline-2023-09
sudo docker push &lt;registry&gt;/&lt;any-prefix&gt;/t9kpublic/registry:offline-2023-09

# 在 K8s 节点中测试下载镜像
sudo docker pull &lt;registry&gt;/&lt;any-prefix&gt;/t9kpublic/registry:offline-2023-09
</code></pre>
<p>在控制节点中运行命令，上传镜像到镜像仓库服务中：</p>
<pre><code class="language-bash">./manage-offline-container-images.sh \
  --option register --registry &lt;registry&gt;/&lt;any-prefix&gt;
</code></pre>
<p>在使用已有的镜像仓库服务时，下文所有的镜像仓库地址 <code>&lt;control-node-ip&gt;:5000</code> 都需要替换为 <code>&lt;registry&gt;/&lt;any-prefix&gt;</code>。</p>
<h2 id="验证镜像下载"><a class="header" href="#验证镜像下载">验证镜像下载</a></h2>
<p>在“控制节点”查看镜像版本：</p>
<pre><code class="language-bash">ls container-images | grep landing-page-web
</code></pre>
<p>下载镜像：</p>
<pre><code class="language-bash">docker pull &lt;hostname&gt;:5000/t9kpublic/landing-page-web:1.78.4
</code></pre>
<h2 id="安装-t9k-产品"><a class="header" href="#安装-t9k-产品">安装 T9k 产品</a></h2>
<h3 id="安装前准备-1"><a class="header" href="#安装前准备-1">安装前准备</a></h3>
<p>参考 <a href="offline/install/../../online/products/pre-install.html">安装前准备</a>。</p>
<h4 id="tls-证书"><a class="header" href="#tls-证书">TLS 证书</a></h4>
<p>域名证书位于 <code>certs/</code>，可以供测试使用。</p>
<h4 id="设置-dns"><a class="header" href="#设置-dns">设置 DNS</a></h4>
<h5 id="独立-dns-server"><a class="header" href="#独立-dns-server">独立 DNS server</a></h5>
<p>如果部署环境有 DNS server，则在此 DNS server 中增加相应记录即可。</p>
<p>例如，增加如下记录：</p>
<pre><code>home.sample.t9kcloud.cn
auth.sample.t9kcloud.cn
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>需使用和 TLS 证书对应的 DNS。</p>
</aside>
<h5 id="使用-k8s-的-coredns"><a class="header" href="#使用-k8s-的-coredns">使用 K8s 的 coredns</a></h5>
<p>如果没有 DNS server，可直接修改 K8s 中的 coredns。</p>
<p>这种情况下所有需要访问集群服务的节点都需要修改 <code>/etc/hosts</code>，且无法支持 mlservice 的使用。只适用于测试场景。</p>
<p>集群内，通过编辑 kube-system 的 configmap <code>coredns</code> 来设置 DNS：</p>
<pre><code class="language-bash">kubectl -n kube-system edit cm coredns
</code></pre>
<pre><code class="language-diff">diff -u ./old-corefile.yaml ./new-corefile.yaml 
--- ./old-corefile.yaml	2023-09-26 12:09:48.000000000 +0800
+++ ./new-corefile.yaml	2023-09-26 12:09:41.000000000 +0800
@@ -5,6 +5,10 @@
             lameduck 5s
         }
         ready
+        hosts {
+            192.168.101.75 home.sample.t9kcloud.cn auth.sample.t9kcloud.cn
+            fallthrough
+        }
         kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
</code></pre>
<p>重启 coredns 使上述修改生效：</p>
<pre><code class="language-bash">kubectl -n kube-system rollout restart deploy/coredns
</code></pre>
<p>在其他需要解析域名的节点设置静态解析。修改 /etc/hosts：</p>
<pre><code class="language-bash">sudo cat &gt;&gt; /etc/hosts &lt; EOF
192.168.101.75 home.sample.t9kcloud.cn auth.sample.t9kcloud.cn
EOF
</code></pre>
<h3 id="安装产品-2"><a class="header" href="#安装产品-2">安装产品</a></h3>
<p>参考 <a href="offline/install/../../online/products/install-uc-mode.html">安装产品</a>。</p>
<aside class="note">
<div class="title">注意</div>
<p>需要将 values.yaml 中的 docker.io/t9kpublic 替换为 <control-node-ip>:5000/t9kpublic。其中 <control-node-ip> 为控制节点的 IP 地址。</p>
</aside>
<p>可以使用的修改命令：</p>
<pre><code class="language-bash">cd ~/ansible/$T9K_CLUSTER
sed -i &quot;s|docker.io/t9kpublic|&lt;control-node-ip&gt;:5000/t9kpublic|g&quot; \
    values.yaml 
</code></pre>
<p>产品列表见：</p>
<pre><code class="language-bash">ls ~/ansible/ks-clusters/tools/offline-t9k/productlist

cat ~/ansible/ks-clusters/tools/offline-t9k/productlist/t9k-2023-12-20.list 
</code></pre>
<p>安装产品：</p>
<pre><code class="language-bash"># 安装命令
helm install &lt;product&gt; \
  ../ks-clusters/tools/offline-t9k/charts/&lt;product&gt;-&lt;version.tgz&gt; \
  -f values.yaml \
  -n t9k-system

# 以安装 t9k-core 为例
helm install t9k-core \
  ../ks-clusters/tools/offline-t9k/charts/t9k-core-1.78.4.tgz \
  -f values.yaml \
  -n t9k-system

# t9k-monitoring 的 namespace 与其他产品不同
helm install t9k-monitoring \
  ../ks-clusters/tools/offline-t9k/charts/t9k-monitoring \
  -f values.yaml \
  -n t9k-monitoring
</code></pre>
<h3 id="安装后配置-4"><a class="header" href="#安装后配置-4">安装后配置</a></h3>
<p>参考 <a href="offline/install/../../online/products/post-install.html">安装后配置</a>。</p>
<aside class="note">
<div class="title">注意</div>
<p>需要执行标注了 [离线安装场景] 的修改。</p>
</aside>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="产品升级"><a class="header" href="#产品升级">产品升级</a></h1>
<pre><code>TODO:
    1. 提供 charts 列表
</code></pre>
<h2 id="升级"><a class="header" href="#升级">升级</a></h2>
<p>首先参考<a href="update/../../data-backup/index.html">数据备份</a>进行必要的数据备份。</p>
<p>更新 ks-clusters：</p>
<pre><code class="language-bash">cd ~/ansible/ks-clusters
git pull
</code></pre>
<p>进入 inventory 所在的目录，并获取新版本产品对应的 <code>values.yaml</code>：</p>
<pre><code class="language-bash">cd ~/ansible/$T9K_CLUSTER
cp ../ks-clusters/values/values-sample-1.79.2.yaml ./new-values.yaml
</code></pre>
<p>获取安装产品时使用的 <code>values.yaml</code>：</p>
<pre><code class="language-bash">helm get values t9k-core -n t9k-system &gt; old-values.yaml
</code></pre>
<p>进行对比：</p>
<pre><code class="language-bash"># 要求 yq 4.x.x 版本
yq eval 'sortKeys(..)' old-values.yaml &gt; old-values-sort.yaml
yq eval 'sortKeys(..)' new-values.yaml &gt; new-values-sort.yaml
diff -u old-values-sort.yaml new-values-sort.yaml
</code></pre>
<p>结合这两个文件得到最终的 values.yaml。然后使用 Helm upgrade 命令逐个升级产品。这里以产品 &quot;t9k-core&quot; 的 1.79.0 版本为例：</p>
<pre><code class="language-bash">$ helm upgrade -n t9k-system t9k-core \
    oci://tsz.io/t9kcharts/t9k-core \
    --version 1.79.0 \
    -f values.yaml
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="附录-1"><a class="header" href="#附录-1">附录</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="在线安装-docker"><a class="header" href="#在线安装-docker">在线安装 Docker</a></h1>
<p>安装 apt packages：</p>
<pre><code class="language-bash">$ sudo apt update -y
$ sudo apt install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common
</code></pre>
<p>设置 apt 源：</p>
<pre><code class="language-bash">$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg \
     | sudo apt-key add -

$ sudo add-apt-repository -y \
  &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;

$ sudo apt-get update -y
</code></pre>
<p>安装 Docker（可以参考 安装 Kubespray 指定版本的 Docker）：</p>
<pre><code class="language-bash">$ apt-get install -y docker-ce docker-ce-cli containerd.io
</code></pre>
<p>配置 Docker：</p>
<pre><code class="language-bash">$ IPorFQDN=&quot;ip-or-domain&quot;

# 以下配置可能与 kubespray 不一致导致冲突
$ sudo tee /etc/docker/daemon.json &gt;/dev/null &lt;&lt;EOF
{
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;insecure-registries&quot; : [&quot;$IPorFQDN:443&quot;,&quot;$IPorFQDN:80&quot;,&quot;0.0.0.0/0&quot;],
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: {
    &quot;max-size&quot;: &quot;100m&quot;
  },
  &quot;storage-driver&quot;: &quot;overlay2&quot;
}
EOF
</code></pre>
<p>进行用户的配置：</p>
<pre><code class="language-bash">$ mkdir -p /etc/systemd/system/docker.service.d
$ groupadd -f docker
$ MAINUSER=$(logname)
$ usermod -aG docker $MAINUSER
$ sudo systemctl daemon-reload
$ sudo systemctl restart docker
</code></pre>
<p>确认 docker 正常运行：</p>
<pre><code class="language-bash">$ sudo systemctl status docker
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="在线安装-docker-compose"><a class="header" href="#在线安装-docker-compose">在线安装 Docker Compose</a></h1>
<p>使用下列命令下载并安装 Docker Compose：</p>
<pre><code class="language-bash">$ COMPOSEVERSION=&quot;v2.23.0&quot;

$ curl -kL $(curl -s https://api.github.com/repos/docker/compose/releases/tags/$COMPOSEVERSION|grep browser_download_url|grep -i &quot;$(uname -s)-$(uname -m)&quot;|grep -v sha25|head -1|cut -d'&quot;' -f4) -o /usr/local/bin/docker-compose

$ chmod +x /usr/local/bin/docker-compose
$ ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose || true
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="配置-docker-insecure-registry"><a class="header" href="#配置-docker-insecure-registry">配置 Docker Insecure Registry</a></h1>
<p>局域网环境内运行的 Registry 没有使用域名证书，从该 Registry 拉取镜像时可能遇到 <code>server gave HTTP response to HTTPS client</code> 的错误信息。此时可以通过修改 Docker 守护进程的配置来解决该问题。</p>
<p>确认 daemon.json 文件是否存在：</p>
<pre><code class="language-bash">$ sudo ls /etc/docker/daemon.json
</code></pre>
<p>如果文件不存在：</p>
<pre><code class="language-bash">$ sudo mkdir -p /etc/docker
$ sudo cat &gt; /etc/docker/daemon.json &lt;&lt; EOF
{
  &quot;insecure-registries&quot; : [&quot;&lt;IP or hostname&gt;:5000&quot;]
}
EOF
</code></pre>
<p>如果文件存在，则在 json 中增加一行内容：</p>
<pre><code class="language-json">  &quot;insecure-registries&quot; : [&quot;&lt;IP or hostname&gt;:5000&quot;],
</code></pre>
<p>重启 Docker 使配置生效：</p>
<pre><code class="language-bash">$ sudo systemctl restart docker
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="在线安装-s3cmd"><a class="header" href="#在线安装-s3cmd">在线安装 s3cmd</a></h1>
<p>s3cmd 是最常用的访问 S3 服务的命令行工具，可用于创建/删除 bucket、上传/下载少量文件等常见操作。</p>
<p>使用以下命令安装 s3cmd：</p>
<pre><code class="language-bash"># macOS
brew install s3cmd

# Linux
sudo apt install s3cmd

# Windows
pip install s3cmd
</code></pre>
<p>详见 <a href="https://github.com/s3tools/s3cmd/blob/master/INSTALL.md">https://github.com/s3tools/s3cmd/blob/master/INSTALL.md</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装-k8s-注释"><a class="header" href="#安装-k8s-注释">安装 K8s 注释</a></h1>
<h2 id="过程解释"><a class="header" href="#过程解释">过程解释</a></h2>
<h3 id="脚本内容"><a class="header" href="#脚本内容">脚本内容</a></h3>
<p>集群安装脚本 <code>cluster.yml</code> 需要执行 ~1300 个 ansible Tasks，可以通过下面的命令输出完整的 Task 列表：</p>
<pre><code class="language-bash">ansible-playbook ../kubespray/playbooks/cluster.yml \
    -i inventory/inventory.ini \
    --list-tasks
</code></pre>
<h3 id="安装时长"><a class="header" href="#安装时长">安装时长</a></h3>
<p>安装 K8s 集群所需要的时间受网络下载速度（主要因素）、节点性能、节点当前状态影响。</p>
<p>初次运行该脚本的用时通常在 30 分钟到 1 小时范围内。其中，命令行工具、镜像等内容的下载约 25 分钟，下载之外的运行时间约 20 分钟。但在网络环境较差的情况下，下载时间可能会延长到数个小时。</p>
<h3 id="安装进度"><a class="header" href="#安装进度">安装进度</a></h3>
<p>Ansible 在执行过程中会输出当前运行的 Task 名称（方括号中的内容），及对每个节点的运行结果。格式如下：</p>
<pre><code>TASK [reset : reset | Restart network]********************************
changed: [nc12]
changed: [nc13]
changed: [nc11]
changed: [nuc]
changed: [nc14]
</code></pre>
<h3 id="查看结果"><a class="header" href="#查看结果">查看结果</a></h3>
<p>Ansible playbook 在运行结束后会输出一个运行回顾，示例如下：</p>
<pre><code>PLAY RECAP *****************************************************************************************************
localhost                  : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
nc11                       : ok=609  changed=35   unreachable=0    failed=0    skipped=1108 rescued=0    ignored=1   
nc12                       : ok=452  changed=22   unreachable=0    failed=0    skipped=758  rescued=0    ignored=1   
nc13                       : ok=610  changed=34   unreachable=0    failed=0    skipped=1107 rescued=0    ignored=1  
nc14                       : ok=452  changed=22   unreachable=0    failed=0    skipped=758  rescued=0    ignored=1   
nuc                        : ok=728  changed=45   unreachable=0    failed=0    skipped=1237 rescued=0    ignored=7
</code></pre>
<p>如果出现异常，需进一步检查：</p>
<ol>
<li>unreachable: 如果有节点显示为不可达，那么您应该检查该节点状态和网络连接。</li>
<li>failed: 如果有任务失败，那么您应该检查错误原因，并尝试解决问题。</li>
<li>ignored: 如果有错误被忽略，那么您应该检查错误信息和忽略原因，并确定是否需要采取进一步的措施。</li>
</ol>
<p>Kubespray 运行过程中一些错误可被忽略 （ignored） 。这些被忽略的错误并不会影响 Kubespray 正常运行和安装 K8s 集群（但是会显示在 PLAY RECAP 的 <code>ignored</code> 项中）。</p>
<p>如果部分节点的 unreachable 或 failed 异常数量不为 0，则该节点的 K8s 安装失败，需要处理错误。</p>
<h2 id="常见失败原因"><a class="header" href="#常见失败原因">常见失败原因</a></h2>
<p>安装过程中，有以下常见失败原因：</p>
<ol>
<li>镜像、命令行工具下载失败
<ol>
<li>设置的镜像名称错误</li>
<li>Registry 不可访问，或者网络不通、不稳定</li>
</ol>
</li>
<li>验证未通过
<ol>
<li>重启 cri-dockerd 等系统服务时，等待时间超过预设值</li>
<li>用户配置不符合要求，如 etcd 节点数量为偶数</li>
</ol>
</li>
<li>节点遗留设置与现有设置冲突，导致命令运行出错
<ol>
<li>节点中设置了额外的 apt 源，导致冲突</li>
<li>节点中已经安装了新版本 docker，导致试图安装指定版本时失败</li>
</ol>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="生成-k8s-文件和镜像列表"><a class="header" href="#生成-k8s-文件和镜像列表">生成 K8s 文件和镜像列表</a></h1>
<p>这里需要一个节点来准备 offline 安装包，该节点满足以下要求：</p>
<ol>
<li>可以连接到互联网</li>
<li>与计划离线安装 K8s 集群的“目标节点”操作系统相同</li>
</ol>
<h2 id="准备-ansible-运行环境和-inventory"><a class="header" href="#准备-ansible-运行环境和-inventory">准备 ansible 运行环境和 inventory</a></h2>
<ol>
<li>需要在节点设置 ansible 运行环境，参见<a href="appendix/../online/inventory/basic-settings.html">基本设置</a>。</li>
<li>准备 inventory 并<a href="appendix/../online/prepare-nodes-and-install-k8s.html#%E4%BF%AE%E6%94%B9%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">设置变量</a>。变量会影响需要下载的 files、images 版本。</li>
</ol>
<p>变量使用在线安装的配置设置即可，建议检查下面的镜像、文件源配置（download.yaml）：</p>
<pre><code class="language-yaml">files_repo: &quot;https://mirror.ghproxy.com&quot;
gcr_image_repo: &quot;docker.io/t9kpublic&quot;
kube_image_repo: &quot;docker.io/t9kpublic&quot;
docker_image_repo: &quot;docker.io/t9kpublic&quot;
quay_image_repo: &quot;docker.io/t9kpublic&quot;
</code></pre>
<h2 id="生成列表"><a class="header" href="#生成列表">生成列表</a></h2>
<p>1）生成 files 和 images 列表的 template（保存在 ks-clusters/tools/offline-k8s/temp 路径下）： </p>
<pre><code class="language-yaml"># 进入 kubespray 专用的目录
$ cd ~/ansible

# 将 kubespray 切换到合适分支（推荐使用 offline 分支）
$ cd kubespray
$ git checkout kubernetes-&lt;version&gt;
$ cd ..

# 读取 kubespray 的 download role 的设置，生成一个 template
ks-clusters/tools/offline-k8s/generate_list_template.sh \
    -d ~/ansible/kubespray
</code></pre>
<p>2）运行 ansible 来渲染 template，生成实际使用的 files.list 和 images.list。</p>
<p>这里需要一个可访问的、与“目标节点”操作系统相同的节点（可以是当前节点自身）。修改 inventory.ini，将该节点设置为 kube_control_plane[0]，下面以 nuc 节点为例：</p>
<pre><code class="language-yaml"># 进入 inventory 目录
$ export T9K_CLUSTER=&lt;cluster&gt;
# 该目录在准备 inventory 时创建
$ cd ~/ansible/$T9K_CLUSTER/inventory
</code></pre>
<p>修改 inventory.ini：</p>
<pre><code class="language-yaml">$ diff -u ./inventory-old.ini ./inventory-new.ini 
--- ./inventory.ini	2023-12-05 13:23:30.000000000 +0800
+++ new-inventory.ini	2023-12-12 14:23:19.000000000 +0800
@@ -1,12 +1,13 @@
 [all]
 nc15 ansible_host=x.x.x.x
+nuc ansible_host=100.64.4.159
 
 ; ## configure a bastion host if your nodes are not directly reachable
 ; [bastion]
 ; bastion ansible_host=x.x.x.x ansible_user=some_user
 
 [kube_control_plane]
-nc15
+nuc
 
 [etcd]
 nc15
@@ -31,7 +32,7 @@
 calico_rr
 
 [all:vars]
-ansible_user=&lt;user&gt;
+ansible_user=t9k
 
 [kube_control_plane:vars]
 node_labels={&quot;beta.kubernetes.io/fluentd-ds-ready&quot;: &quot;true&quot;}
</code></pre>
<p>3）运行命令生成 files.list 和 images.list（这个 playbook 不会对节点做任何修改）：</p>
<pre><code class="language-yaml"># 进入 inventory 所在的目录
$ cd ~/ansible/$T9K_CLUSTER


# 运行之前是需要生成 SSH Key 并运行 ssh-copy-id t9k@nuc 的，这里省略
$ ansible-playbook ../ks-clusters/tools/offline-k8s/generate_list.yml \
    -i inventory/inventory.ini 
</code></pre>
<p>生成的 list 被保存在 <code>../ks-clusters/tools/offline-k8s/temp/</code> 路径中。</p>
<h2 id="修改文件镜像列表"><a class="header" href="#修改文件镜像列表">修改文件、镜像列表</a></h2>
<p>上面生成了文件和镜像的列表，但这个列表需要修改。</p>
<p>列表需要修改的原因如下：</p>
<ol>
<li>由于生成 template 和生成列表这两个步骤的局限性，这里生成的文件/镜像列表是 kubespray 的 download role 中所列举的所有文件/镜像。没有考虑 inventory 设置中是否用到了这些文件/镜像，也没有考虑 inventory 的部分特殊设置会覆盖 download role 中的设置。</li>
</ol>
<p>1）删除以下未被使用的镜像：</p>
<pre><code class="language-bash">sed -i '/t9kpublic/!d' ../ks-clusters/tools/offline-k8s/temp/images.list
sed -i '/t9kpublic.*\/.*\//d' ../ks-clusters/tools/offline-k8s/temp/images.list
</code></pre>
<p>说明：</p>
<ul>
<li>第一条删除了名称中不包含 t9kpublic 的镜像</li>
<li>第二条删除了在 t9kpublic 后有多于一个斜线 “/” 符号的镜像</li>
</ul>
<p>这里是利用了我们仅使用 docker.io/t9kpublic 作为镜像源的设置。所有我们需要使用的镜像都必然满足上述两个条件，而我们没用到的镜像因为没有进行相应设置，所以不会同时满足上述两点。</p>
<p>2）调整命令行工具下载源</p>
<p>调整 K8s 命令行工具的下载源，通常国内使用该下载源会更快（详细说明见：<a href="https://docs.google.com/document/d/19Wx4UqCbidSagGp7XXln_x_OlLtDQqCGoKKqJyJLJl0/edit#heading=h.9as21su2s6ig">K8s 命令行工具下载路径错误</a>）：</p>
<pre><code class="language-bash">sed -i 's|dl.k8s.io|storage.googleapis.com/kubernetes-release|g' \
    ../ks-clusters/tools/offline-k8s/temp/files.list
</code></pre>
<p>3）[推荐] 设置 github 代理</p>
<p>根据实际情况进行设置，如果列表输出中使用了 ghproxy.com 作为代理，则替换为最新的：</p>
<pre><code class="language-bash">sed -i 's|https://ghproxy.com/https://github.com|https://mirror.ghproxy.com/https://github.com|g' \
    ../ks-clusters/tools/offline-k8s/temp/files.list
</code></pre>
<p>如果未使用，则增加代理：</p>
<pre><code class="language-bash">sed -i 's|https://github.com|https://mirror.ghproxy.com/https://github.com|g' 
    ../ks-clusters/tools/offline-k8s/temp/files.list
</code></pre>
<p>4）检查文件列表</p>
<p>检查 <code>../ks-clusters/tools/offline-k8s/temp/files.list</code> 中的文件下载地址，确认符合预期。</p>
<p>[可选] 如果对 kubespray 的设置很熟悉，您可以：</p>
<ul>
<li>编辑文件列表，去除不需要的文件。</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="生成-t9k-产品镜像列表"><a class="header" href="#生成-t9k-产品镜像列表">生成 T9k 产品镜像列表</a></h1>
<p>这里的操作在 ks-clusters/tools/offline-t9k 中进行：</p>
<pre><code class="language-bash">$ cd ~/ansible/ks-clusters/tools/offline-t9k
</code></pre>
<p>1）确认您完成了<a href="appendix/../offline/prepare-offline-packages/k8s-components.html#%E4%B8%8B%E8%BD%BD-helm-chart">下载 Helm Chart</a>。</p>
<p>2）然后准备 values.yaml，您可以在 <a href="https://gitlab.dev.tensorstack.net/t9k/admin-docs/blob/master/docs/deploy-tools/values-sample-1.78.7.yaml">sample</a> 的基础上修改。您需要确保该 values.yaml 对容器镜像的设置与实际安装使用的一致。</p>
<p>3）生成镜像列表，保存在 images.list：</p>
<pre><code class="language-bash">$ ./generate-image-list.sh --values values.yaml --config productlist/t9k-2024-03-25.list
</code></pre>
<p>请注意这里的提示：</p>
<pre><code>The generation script may miss some specially set images.
You can use the following command to list them:

for file in $(ls temp | grep -E &quot;^gen\..*\.yaml$&quot;); do
   cat temp/$file | grep t9kpublic | grep -v &quot;image:&quot;;
done
</code></pre>
<p>4）您需要手动处理这里的其他镜像，来生成最终的 <code>images.list</code>。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="helm-chart-修改"><a class="header" href="#helm-chart-修改">Helm Chart 修改</a></h1>
<h2 id="elastic-search"><a class="header" href="#elastic-search">Elastic Search</a></h2>
<p>获取 elastic search 官方的 helm chart：</p>
<pre><code class="language-bash">$ helm repo add elastic https://helm.elastic.co
$ helm pull elastic/elasticsearch --version 7.13.4
$ tar zxvf elasticsearch-7.13.4.tgz
</code></pre>
<p>进行以下修改：</p>
<pre><code class="language-bash">$ git diff elasticsearch/templates/poddisruptionbudget.yaml
  diff --git a/skj/elasticsearch/templates/poddisruptionbudget.yaml 
b/skj/elasticsearch/templates/poddisruptionbudget.yaml
  index df6c74e..7f887da 100644
  --- a/skj/elasticsearch/templates/poddisruptionbudget.yaml
  +++ b/skj/elasticsearch/templates/poddisruptionbudget.yaml
  @@ -1,6 +1,6 @@
   ---
   {{- if .Values.maxUnavailable }}
  -apiVersion: policy/v1beta1
  +apiVersion: policy/v1
   kind: PodDisruptionBudget
   metadata:
     name: &quot;{{ template &quot;elasticsearch.uname&quot; . }}-pdb&quot;


$ git diff elasticsearch/values.yaml
  diff --git a/skj/elasticsearch/values.yaml b/skj/elasticsearch/values.yaml
  index 82b82b9..561a403 100644
  --- a/skj/elasticsearch/values.yaml
  +++ b/skj/elasticsearch/values.yaml
  @@ -58,7 +58,7 @@ hostAliases: []
   #  - &quot;foo.local&quot;
   #  - &quot;bar.local&quot;
   
  -image: &quot;docker.elastic.co/elasticsearch/elasticsearch&quot;
  +image: &quot;docker.io/t9kpublic/elasticsearch&quot;
   imageTag: &quot;7.13.4&quot;
   imagePullPolicy: &quot;IfNotPresent&quot;
</code></pre>
<p>打包上传：</p>
<pre><code class="language-bash">$ rm elasticsearch-7.13.4.tgz
$ helm package elasticsearch
$ helm push ./elasticsearch-7.13.4.tgz oci://tsz.io/t9kcharts
</code></pre>
<h2 id="gpu-operator-2"><a class="header" href="#gpu-operator-2">GPU Operator</a></h2>
<p>下载并解压 Helm Chart:</p>
<pre><code class="language-bash"># 添加 nvidia helm repo
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia \
   &amp;&amp; helm repo update
$ helm pull --untar nvidia/gpu-operator --version v22.9.2
$ ls gpu-operator
gpu-operator
</code></pre>
<p>将其中的几个 repository 地址全部换成 t9kpublic，具体操作如下：</p>
<pre><code class="language-bash">sed -i &quot;s|repository: nvcr.io/nvidia/cloud-native|repository: t9kpublic|g&quot; \
    gpu-operator/values.yaml
sed -i &quot;s|repository: nvcr.io/nvidia/k8s|repository: t9kpublic|g&quot; \
    gpu-operator/values.yaml
sed -i &quot;s|repository: nvcr.io/nvidia|repository: t9kpublic|g&quot; \
    gpu-operator/values.yaml
</code></pre>
<p>完整的对比如下：</p>
<pre><code class="language-bash">$ diff -u ./gpu-operator-22/values.yaml ./gpu-operator/values.yaml
--- ./gpu-operator-22/values.yaml	2024-01-30 14:24:59.000000000 +0800
+++ ./gpu-operator/values.yaml	2024-01-30 14:25:43.000000000 +0800
@@ -33,7 +33,7 @@
     maxUnavailable: &quot;1&quot;
 
 validator:
-  repository: nvcr.io/nvidia/cloud-native
+  repository: t9kpublic
   image: gpu-operator-validator
   # If version is not specified, then default is to use chart.AppVersion
   #version: &quot;&quot;
@@ -48,7 +48,7 @@
         value: &quot;true&quot;
 
 operator:
-  repository: nvcr.io/nvidia
+  repository: t9kpublic
   image: gpu-operator
   # If version is not specified, then default is to use chart.AppVersion
   #version: &quot;&quot;
@@ -65,7 +65,7 @@
   upgradeCRD: false
   initContainer:
     image: cuda
-    repository: nvcr.io/nvidia
+    repository: t9kpublic
     version: 11.8.0-base-ubi8
     imagePullPolicy: IfNotPresent
   tolerations:
@@ -105,7 +105,7 @@
 
 driver:
   enabled: true
-  repository: nvcr.io/nvidia
+  repository: t9kpublic
   image: driver
   version: &quot;525.60.13&quot;
   imagePullPolicy: IfNotPresent
@@ -141,7 +141,7 @@
       deleteEmptyDir: false
   manager:
     image: k8s-driver-manager
-    repository: nvcr.io/nvidia/cloud-native
+    repository: t9kpublic
     version: v0.6.0
     imagePullPolicy: IfNotPresent
     env:
@@ -178,7 +178,7 @@
 
 toolkit:
   enabled: true
-  repository: nvcr.io/nvidia/k8s
+  repository: t9kpublic
   image: container-toolkit
   version: v1.11.0-ubuntu20.04
   imagePullPolicy: IfNotPresent
@@ -189,7 +189,7 @@
 
 devicePlugin:
   enabled: true
-  repository: nvcr.io/nvidia
+  repository: t9kpublic
   image: k8s-device-plugin
   version: v0.13.0-ubi8
   imagePullPolicy: IfNotPresent
@@ -243,7 +243,7 @@
 dcgm:
   # disabled by default to use embedded nv-hostengine by exporter
   enabled: false
-  repository: nvcr.io/nvidia/cloud-native
+  repository: t9kpublic
   image: dcgm
   version: 3.1.3-1-ubuntu20.04
   imagePullPolicy: IfNotPresent
@@ -254,7 +254,7 @@
 
 dcgmExporter:
   enabled: true
-  repository: nvcr.io/nvidia/k8s
+  repository: t9kpublic
   image: dcgm-exporter
   version: 3.1.3-3.1.2-ubuntu20.04
   imagePullPolicy: IfNotPresent
@@ -274,7 +274,7 @@
 
 gfd:
   enabled: true
-  repository: nvcr.io/nvidia
+  repository: t9kpublic
   image: gpu-feature-discovery
   version: v0.7.0-ubi8
   imagePullPolicy: IfNotPresent
@@ -288,7 +288,7 @@
 
 migManager:
   enabled: true
-  repository: nvcr.io/nvidia/cloud-native
+  repository: t9kpublic
   image: k8s-mig-manager
   version: v0.5.0-ubuntu20.04
   imagePullPolicy: IfNotPresent
@@ -304,7 +304,7 @@
 
 nodeStatusExporter:
   enabled: false
-  repository: nvcr.io/nvidia/cloud-native
+  repository: t9kpublic
   image: gpu-operator-validator
   # If version is not specified, then default is to use chart.AppVersion
   #version: &quot;&quot;
@@ -314,7 +314,7 @@
 
 gds:
   enabled: false
-  repository: nvcr.io/nvidia/cloud-native
+  repository: t9kpublic
   image: nvidia-fs
   version: &quot;2.14.13&quot;
   imagePullPolicy: IfNotPresent
@@ -333,7 +333,7 @@
   resources: {}
   driverManager:
     image: k8s-driver-manager
-    repository: nvcr.io/nvidia/cloud-native
+    repository: t9kpublic
     version: v0.6.0
     imagePullPolicy: IfNotPresent
     env:
@@ -344,7 +344,7 @@
 
 vgpuDeviceManager:
   enabled: true
-  repository: nvcr.io/nvidia/cloud-native
+  repository: t9kpublic
   image: vgpu-device-manager
   version: &quot;v0.2.0&quot;
   imagePullPolicy: IfNotPresent
@@ -356,7 +356,7 @@
 
 vfioManager:
   enabled: true
-  repository: nvcr.io/nvidia
+  repository: t9kpublic
   image: cuda
   version: 11.7.1-base-ubi8
   imagePullPolicy: IfNotPresent
@@ -365,7 +365,7 @@
   resources: {}
   driverManager:
     image: k8s-driver-manager
-    repository: nvcr.io/nvidia/cloud-native
+    repository: t9kpublic
     version: v0.6.0
     imagePullPolicy: IfNotPresent
     env:
@@ -376,7 +376,7 @@
 
 sandboxDevicePlugin:
   enabled: true
-  repository: nvcr.io/nvidia
+  repository: t9kpublic
   image: kubevirt-gpu-device-plugin
   version: v1.2.1
   imagePullPolicy: IfNotPresent
</code></pre>
<p>重新打包，然后上传：</p>
<pre><code class="language-bash">$ rm -f gpu-operator-v22.9.2.tgz
$ helm package gpu-operator
$ helm push gpu-operator-v22.9.2.tgz oci://tsz.io/t9kcharts
</code></pre>
<p>验证：</p>
<pre><code class="language-bash">$ helm show chart oci://tsz.io/t9kcharts/gpu-operator --version v22.9.2
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="手动安装-mlnx_ofed-驱动"><a class="header" href="#手动安装-mlnx_ofed-驱动">手动安装 MLNX_OFED 驱动</a></h1>
<p>从 NVIDIA 官网下载  NVIDIA MLNX_OFED 驱动，根据实际情况选择对应的版本。例如：MLNX_OFED_LINUX-5.9-0.5.6.0-ubuntu20.04-x86_64.tgz</p>
<aside class="note">
<div class="title">注意</div>
<p>如有必要通过命令行下载，可以参考 <a href="https://docs.nvidia.com/networking/display/MLNXOFEDv562090/Installing+MLNX_OFED#InstallingMLNX_OFED-ofedinstallationusingapt-getInstallingMLNX_OFEDUsingapt-get">Installing MLNX_OFED Using apt-get</a>。</p>
</aside>
<p>从本地复制 <a href="https://network.nvidia.com/products/infiniband-drivers/linux/mlnx_ofed/">NVIDIA MLNX_OFED</a> 驱动到目标节点上：</p>
<pre><code class="language-bash">$ ssh &lt;node&gt; -- mkdir -p tensorstack-install/mellanox
$ rsync -aP MLNX_OFED_LINUX-5.9-0.5.6.0-ubuntu20.04-x86_64.tgz \
    &lt;node&gt;:~/tensorstack-install/mellanox/
</code></pre>
<p>在节点运行：</p>
<pre><code class="language-bash">$ cd ~/tensorstack-install/mellanox
$ tar zxvf ./MLNX_OFED_LINUX-5.9-0.5.6.0-ubuntu20.04-x86_64.tgz
$ cd ./MLNX_OFED_LINUX-5.9-0.5.6.0-ubuntu20.04-x86_64
</code></pre>
<p>确认节点没有安装 mlnx 相关的包：</p>
<pre><code class="language-bash">$ apt list --installed | grep -i mlnx 
</code></pre>
<p>安装之前使用 tmux，避免 ssh 网络连接影响安装过程：</p>
<pre><code class="language-bash">$ tmux
</code></pre>
<p>使用安装脚本进行安装：</p>
<pre><code class="language-bash">$ sudo ./mlnxofedinstall --all --force
</code></pre>
<p>两个命令行参数的含义：</p>
<pre><code class="language-bash">--all              Install all available packages
--force            Force installation，used for unattended installation
</code></pre>
<p>安装过程的输出：</p>
<pre><code class="language-bash">Logs dir: /tmp/MLNX_OFED_LINUX.175111.logs
General log file: /tmp/MLNX_OFED_LINUX.175111.logs/general.log

Below is the list of MLNX_OFED_LINUX packages that you have chosen
(some may have been added by the installer due to package dependencies):

ofed-scripts
mlnx-tools
…
</code></pre>
<p>安装完成后的提示：</p>
<pre><code class="language-bash">The firmware for this device is not distributed inside Mellanox driver: 31:00.0 (PSID: LNV0000000016)
To obtain firmware for this device, please contact your HW vendor.
         
Failed to update Firmware.     
See /tmp/MLNX_OFED_LINUX.175111.logs/fw_update.log                
Device (31:00.0):              
        31:00.0 Infiniband controller: Mellanox Technologies MT28908 Family [ConnectX-6]             
        Link Width: x16        
        PCI Link Speed: 16GT/s 
         
Installation passed successfully                                  
To load the new driver, run:   
/etc/init.d/openibd restart
</code></pre>
<p>其中的日志文件，内容为：</p>
<pre><code class="language-bash">$ /tmp/MLNX_OFED_LINUX.175111.logs/fw_update.log
The firmware for this device is not distributed inside Mellanox driver: 31:00.0 (PSID: LNV0000000016)
To obtain firmware for this device, please contact your HW vendor.

EXIT_STATUS: 2
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h2 id="修改变量配置文件"><a class="header" href="#修改变量配置文件">修改变量配置文件</a></h2>
<p>参考：<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible/vars.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible/vars.md</a></p>
<p>inventory sample 已经在 kubespray 提供的 inventory 范例基础上配置了常用场景的方案。下文将以使用 inventory sample-HA-1.25.9 安装高可用的 v1.25.9 K8s 集群为例，说明其中较为重要的配置。</p>
<p>下面是一些词语的解释：</p>
<ol>
<li>默认值：kubespray 提供的 inventory sample 中设置的值。</li>
<li>预设值：T9k 提供的 inventory sample 中设置的值。预设值可能不等于默认值。</li>
<li>惯例配置：安装 K8s 集群时约定俗成的配置，例如 apiserver 使用 6443 端口。除非有明确的理由，否则不应该修改这些配置值。</li>
</ol>
<h3 id="group_varsk8s_clusterk8s-clusteryml"><a class="header" href="#group_varsk8s_clusterk8s-clusteryml">group_vars/k8s_cluster/k8s-cluster.yml</a></h3>
<p>K8s 集群的设置：</p>
<ol>
<li>K8s 版本 (kube_version)
<ol>
<li>预设值：v1.25.9</li>
<li>原因：根据需要安装的 K8s 版本设置</li>
</ol>
</li>
<li>容器运行时 (container_manager)
<ol>
<li>预设值：docker</li>
<li>原因：有其他模块（Ceph、Harbor）需要使用 docker；如无相关需求，也可以选择其他容器运行时</li>
</ol>
</li>
<li>安装使用的路径设置
<ol>
<li>kube_config_dir 设置为 /etc/kubernetes</li>
<li>kube_manifest_dir 设置为 &quot;{{ kube_config_dir }}/manifests&quot;</li>
<li>kube_cert_dir 设置为 &quot;{{ kube_config_dir }}/ssl&quot;</li>
<li>kube_token_dir 设置为 &quot;{{ kube_config_dir }}/tokens&quot;</li>
<li>kube_script_dir 设置为 &quot;{{ bin_dir }}/kubernetes-scripts&quot;</li>
<li>local_release_dir （下载可执行文件使用）设置为 &quot;/tmp/releases&quot;</li>
</ol>
</li>
<li>下载文件、镜像时的重试次数 (retry_stagger)
<ol>
<li>预设值：5</li>
<li>原因：默认值</li>
</ol>
</li>
<li>K8s 网络插件相关
<ol>
<li>K8s 网络插件 (kube_network_plugin)
<ol>
<li>预设值：calico</li>
<li>原因：有更多 calico 使用经验</li>
</ol>
</li>
<li>K8s 网络插件 multus (kube_network_plugin_multus)
<ol>
<li>预设值：false</li>
<li>作用：选择是否安装 <a target="_blank" rel="noopener noreferrer" href="https://github.com/k8snetworkplumbingwg/multus-cni">multus-cni</a>，false 为不安装</li>
<li>原因：默认值</li>
</ol>
</li>
</ol>
</li>
<li>K8s IP 地址及端口相关配置
<ol>
<li>分配给 service 的 IP 地址范围 (kube_service_addresses)
<ol>
<li>预设值：10.233.0.0/18</li>
<li>原因：惯例配置</li>
</ol>
</li>
<li>分配给非 hostnetwork 的 Pod 的 IP 地址范围 (kube_pods_subnet)
<ol>
<li>预设值：10.233.64.0/18</li>
<li>原因：惯例配置</li>
</ol>
</li>
<li>节点的内部网络大小分配 (kube_network_node_prefix)
<ol>
<li>预设值：24</li>
<li>作用：分配给每个节点用于 Pod IP 地址分配的范围大小，详细说明见<a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/v2.22.1/inventory/sample/group_vars/k8s_cluster/k8s-cluster.yml#L83">注释</a></li>
<li>原因：默认值</li>
</ol>
</li>
<li>apiserver 的服务地址 (kube_apiserver_ip)
<ol>
<li>预设值：kube_service_addresses 的第一个地址，默认设置下是 10.233.0.1</li>
<li>原因：惯例配置</li>
</ol>
</li>
<li>apiserver 的监听端口 (kube_apiserver_port)
<ol>
<li>预设值：6443</li>
<li>原因：惯例配置</li>
</ol>
</li>
</ol>
</li>
<li>kube proxy 相关
<ol>
<li>kube proxy 代理模式 (kube_proxy_mode)
<ol>
<li>预设值：ipvs</li>
<li>原因：ipvs 专为大量服务的负载均衡而设计，其性能不会随着集群规模扩大、service 后端 Pod 数量增加而下降</li>
</ol>
</li>
<li>kube proxy 是否启用严格的 arp 模式 (kube_proxy_strict_arp)
<ol>
<li>预设值：true</li>
<li>原因：为了使用 kube vip 的 ARP 模式，需要设置为 true</li>
</ol>
</li>
</ol>
</li>
<li>配置 kube vip 以支持高可用集群，参考 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/ha-mode.md#ha-endpoints-for-k8s">HA endpoints for K8s</a> 及 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ingress/kube-vip.md">kube-vip</a>
<ol>
<li>启用 kube vip (kube_vip_enabled)
<ol>
<li>预设值：true</li>
<li>原因：需要启用 kube vip</li>
</ol>
</li>
<li>启用 kube vip 作为 control-plane 之间的 load balancer (kube_vip_controlplane_enabled)
<ol>
<li>预设值：true</li>
<li>原因：为了支持高可用的集群</li>
</ol>
</li>
<li>kube vip 的 virtual IP 地址 (kube_vip_address)
<ol>
<li>预设值：100.64.4.202</li>
<li>原因：T9k 办公室网络默认配置，需根据实际情况调整</li>
</ol>
</li>
<li>通过 load balancer 访问 apiserver 的地址 (loadbalancer_apiserver)
<ol>
<li>address，预设值同 kube_vip_address</li>
<li>port，预设值 6443</li>
<li>原因：惯例配置</li>
</ol>
</li>
<li>启用 kube vip 作为 service 的 load balancer (kube_vip_services_enabled)
<ol>
<li>预设值：false</li>
</ol>
</li>
<li>启用 service election (kube_vip_enableServicesElection)
<ol>
<li>预设值: true</li>
</ol>
</li>
<li>kube vip 使用 ARP 模式 (kube_vip_arp_enabled)
<ol>
<li>预设值：true</li>
<li>原因：ARP 模式可以在不需要路由器支持的情况下工作</li>
</ol>
</li>
</ol>
</li>
<li>加密 secret 中的数据 (kube_encrypt_secret_data)
<ol>
<li>预设值：false</li>
<li>作用：静态加密 Secret，参考 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/encrypting-secret-data-at-rest.md">Encrypting Secret Data at Rest</a></li>
<li>原因：未修改默认值</li>
<li>补充：考虑以后的安装中设置为 true，即使用默认的 secretbox 算法进行加密</li>
</ol>
</li>
<li>集群 DNS 相关设置
<ol>
<li>集群名称 (cluster_name)
<ol>
<li>预设值：cluster.local</li>
<li>作用：K8s 集群名称，也被用作 DNS domain</li>
<li>原因：默认值</li>
</ol>
</li>
<li>ndots
<ol>
<li>预设值：2</li>
<li>作用：解析主机名时，如果主机名中包含的点号 &quot;.&quot; 数量少于 ndots，则会在主机名后面添加搜索域并进行解    析。本配置会通过 /etc/resolv.conf 影响使用 host network 的 Pod</li>
<li>原因：默认值 </li>
<li>补充：这一条目前和主机实际情况不符合，需要调查</li>
</ol>
</li>
<li>dns_mode 选择 coredns，参考 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/advanced/dns-stack.md#dns-modes-supported-by-kubespray">DNS mode</a>
<ol>
<li>原因：惯例配置</li>
</ol>
</li>
</ol>
</li>
<li>nodelocaldns 相关配置，参考 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/advanced/dns-stack.md#nodelocal-dns-cache">Nodelocal DNS cache</a>
<ol>
<li>启用 nodelocal dns cache (enable_nodelocaldns)
<ol>
<li>预设值：true</li>
<li>原因：能够提高集群 DNS 性能</li>
</ol>
</li>
<li>Node local dns 服务地址 (nodelocaldns_ip)
<ol>
<li>预设值：169.254.25.10</li>
<li>原因：惯例配置</li>
</ol>
</li>
</ol>
</li>
<li>域名解析模式 (resolvconf_mode)
<ol>
<li>预设值: true</li>
<li>原因：默认值</li>
</ol>
</li>
<li>kubernetes 审计 (kubernetes_audit)
<ol>
<li>预设值: true</li>
<li>原因: Kubernetes 审计提供了一个与安全相关的、按时间顺序排列的记录集</li>
</ol>
</li>
<li>K8s 镜像拉取策略 (k8s_image_pull_policy)
<ol>
<li>预设值: IfNotPresent</li>
<li>原因: 避免重复拉取镜像。生产环境中，如需更改镜像必须使用不同的 tag。</li>
</ol>
</li>
<li>自动更新证书 (auto_renew_certificates)
<ol>
<li>预设值: true</li>
<li>原因: 每个月第一个周一自动更新 K8s 控制平面证书。</li>
</ol>
</li>
</ol>
<h3 id="group_varsk8s_clusteraddonsyml"><a class="header" href="#group_varsk8s_clusteraddonsyml">group_vars/k8s_cluster/addons.yml</a></h3>
<p>K8s 附加组件设置：</p>
<ol>
<li>安装 helm (helm_enabled)
<ol>
<li>预设值: true</li>
<li>作用：在节点上安装 helm 命令行工具</li>
</ol>
</li>
<li>安装 registry (registry_enabled)
<ol>
<li>预设值: false</li>
<li>原因: 不符合需求，Kubespray 的方案是通过创建 Replica set 来运行 docker.io/library/registry 镜像作为 registry 服务</li>
</ol>
</li>
<li>安装 metrics server (metrics_server_enabled)
<ol>
<li>预设值: true</li>
<li>作用：安装 metrics server</li>
</ol>
</li>
<li>启用 ingress-nginx 作为 ingress 控制器 (ingress_nginx_enabled)
<ol>
<li>预设值: true</li>
<li>原因：有更多 ingress-nginx 使用经验。</li>
<li>设置为仅在具有 <code>&quot;node-role.kubernetes.io/ingress&quot;</code> label 的节点运行。</li>
</ol>
</li>
</ol>
<h3 id="group_varsallallyml"><a class="header" href="#group_varsallallyml">group_vars/all/all.yml</a></h3>
<p>网络、证书等设置：</p>
<ol>
<li>安装使用的路径设置
<ol>
<li>bin_dir 设置为 /usr/local/bin</li>
</ol>
</li>
<li>设置 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/advanced/dns-stack.md#upstream_dns_servers">upstream_dns_servers</a>，选择 114.114.114.114（或其他可靠的 DNS 服务）作为上游 DNS 服务器。
<ol>
<li>原因：避免 DNS 查询循环，参考 <a href="https://docs.google.com/document/d/1wPHoCcTU49jlVjFhWiQfWfQRkj5Ymzq6ovtuKHAlCuM/edit#heading=h.h1mp0pkm52wl">K8s DNS 配置</a>。</li>
</ol>
</li>
</ol>
<h3 id="group_varsalldownloadyml"><a class="header" href="#group_varsalldownloadyml">group_vars/all/download.yml</a></h3>
<p>本文件来源于 kubespray sample 的 group_vars/all/offline.yml，结合了 download role 中的 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/v2.22.1/roles/download/defaults/main.yml">defaults/main.yml</a> 中的变量。用于指定命令行工具、镜像的下载源。部署 K8s 使用的镜像列表见文档：<a href="https://docs.google.com/document/d/1ktaFh43jI5cULvQe96GHeJBSjXpP2Kl4dQPST_Uw474/edit#heading=h.76vwdbfut8rn">[2023/08] Kubespray 测试</a>。</p>
<ol>
<li>通用下载源设置
<ol>
<li>files_repo: &quot;https://mirror.ghproxy.com&quot;
<ol>
<li>作用：指定 github 的文件的下载源</li>
</ol>
</li>
<li>gcr_image_repo: &quot;docker.io/t9kpublic&quot;
<ol>
<li>作用：指定 gcr registry 中镜像的下载源</li>
</ol>
</li>
<li>kube_image_repo: &quot;docker.io/t9kpublic&quot;
<ol>
<li>作用：指定 K8s registry 中镜像的下载源</li>
</ol>
</li>
<li>docker_image_repo: &quot;docker.io/t9kpublic&quot;
<ol>
<li>作用：指定 docker registry 中镜像的下载源</li>
</ol>
</li>
<li>quay_image_repo: &quot;quay.io&quot;
<ol>
<li>作用：指定 quay registry 中镜像的下载源</li>
</ol>
</li>
</ol>
</li>
<li>K8s 命令行工具下载链接
<ol>
<li>kubectl_download_url: 指定 kubectl 的下载链接</li>
<li>kubelet_download_url: 指定 kubelet 的下载链接</li>
<li>kubeadm_download_url: 指定 kubeadm 的下载链接</li>
</ol>
</li>
<li>其他组件的下载链接，包括 coredns、calico 等组件的下载链接</li>
</ol>
<h3 id="group_varsalldockeryml"><a class="header" href="#group_varsalldockeryml">group_vars/all/docker.yml</a></h3>
<p>docker 的设置，参考 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/CRI/docker.md">Docker Support</a>：</p>
<ol>
<li>Docker 数据存储路径 (docker_daemon_graph)
<ol>
<li>预设值: &quot;/var/lib/docker&quot;</li>
<li>原因：docker 默认的存储路径，可以根据具体文件系统挂载情况进行调整</li>
</ol>
</li>
<li>docker 日志设置 (docker_log_opts)
<ol>
<li>预设值: &quot;--log-opt max-size=50m --log-opt max-file=5&quot;</li>
<li>原因：默认设置</li>
</ol>
</li>
<li>设置 docker registry mirrors (docker_registry_mirrors)
<ol>
<li>原因：加速国内拉取 Docker Hub 镜像的速度。</li>
<li>设置的镜像源为:
<ol>
<li>https://dockerproxy.com/</li>
<li>https://hub-mirror.c.163.com/</li>
<li>https://mirror.baidubce.com/</li>
<li>https://ccr.ccs.tencentyun.com/</li>
</ol>
</li>
</ol>
</li>
<li>其他 docker 设置 (docker_options)
<ol>
<li>预设值: &quot;--default-ulimit=memlock=-1:-1 --default-ulimit=stack=67108864:67108864&quot;</li>
<li>原因：参考 <a href="https://github.com/awslabs/benchmark-ai/issues/17">https://github.com/awslabs/benchmark-ai/issues/17</a>，但是其中 shared memory size 的设置对 K8s 无效（<a href="https://docs.google.com/document/d/1jJ6cfRvwQaWFk2G0F4_RvNOlfTw5ic2Ae_ozsEnfxvI/edit#heading=h.gvmpht24z67q">测试记录</a>），因此没有增加。</li>
</ol>
</li>
</ol>
<h3 id="group_varsalletcdyml"><a class="header" href="#group_varsalletcdyml">group_vars/all/etcd.yml</a></h3>
<p>etcd 的设置，参考 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/operations/etcd.md#etcd">etcd</a>：</p>
<ol>
<li>容器运行时 (container_manager)
<ol>
<li>预设值：docker</li>
<li>说明：与 k8s_cluster/k8s-cluster.yml 中设置的容器运行时作用范围不同。对于属于 k8s_cluster group 的节点，k8s_cluster/k8s-cluster.yml 中的设置会生效。否则本设置会生效（比如不加入 K8s 集群的 etcd 节点）。</li>
</ol>
</li>
<li>etcd 的安装方式（etcd_deployment_type）选择 docker。
<ol>
<li>预设值: docker</li>
<li>原因：kubespray 推荐在容器运行时为 docker 时，使用该方式安装 etcd。</li>
</ol>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="ansible-debugging"><a class="header" href="#ansible-debugging">ansible debugging</a></h1>
<h2 id="常用调试方法"><a class="header" href="#常用调试方法">常用调试方法</a></h2>
<p>ansible-playbook 在运行时会输出调试信息，例如：</p>
<pre><code>TASK [kubernetes/control-plane : Kubeadm | Initialize first master] ******************************************************************
fatal: [pek01]: FAILED! =&gt; {&quot;attempts&quot;: 3, &quot;changed&quot;: true, &quot;cmd&quot;: [&quot;timeout&quot;, &quot;-k&quot;, &quot;300s&quot;, &quot;300s&quot;, &quot;/usr/local/bin/kubeadm&quot;, &quot;init&quot;, &quot;--config=/etc/kubernetes/kubeadm-config.yaml&quot;, &quot;--ignore-preflight-errors=all&quot;, &quot;--skip-phases=addon/coredns&quot;, &quot;--upload-certs&quot;], &quot;delta&quot;: &quot;0:05:00.008708&quot;, &quot;end&quot;: &quot;2024-06-17 08:39:51.635807&quot;, &quot;failed_when_result&quot;: true, &quot;msg&quot;: &quot;non-zero return code&quot;, &quot;rc&quot;: 124, &quot;start&quot;: &quot;2024-06-17 08:34:51.627099&quot;, &quot;stderr&quot;: &quot;W0617 08:34:51.673472   36882 utils.go:69] The recommended value for \&quot;clusterDNS\&quot; in \&quot;KubeletConfiguration\&quot; is: [10.233.0.10]; the provided value is: [169.254.25.10]&quot;, &quot;stderr_lines&quot;: [&quot;W0617 08:34:51.673472   36882 utils.go:69] The recommended value for \&quot;clusterDNS\&quot; in \&quot;KubeletConfiguration\&quot; is: [10.233.0.10]; the provided value is: [169.254.25.10]&quot;], &quot;stdout&quot;: &quot;[init] Using Kubernetes version: v1.28.6\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'&quot;, &quot;stdout_lines&quot;: [&quot;[init] Using Kubernetes version: v1.28.6&quot;, &quot;[preflight] Running pre-flight checks&quot;, &quot;[preflight] Pulling images required for setting up a Kubernetes cluster&quot;, &quot;[preflight] This might take a minute or two, depending on the speed of your internet connection&quot;, &quot;[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'&quot;]}
</code></pre>
<p>调试信息包括具体执行的命令和运行结果等详细信息。在相应节点上运行调试信息中的命令，通常能够复现出现的错误。如果需要更多信息，可依照本章提供的方法进一步进行调试。</p>
<h3 id="verbosity-flag"><a class="header" href="#verbosity-flag">verbosity flag</a></h3>
<p>设置命令行参数 <code>-v</code> 可以查看更详细的调试信息，例如：</p>
<pre><code class="language-bash">ansible-playbook playbook.yml -v
</code></pre>
<p>添加更多的 &quot;v&quot; 可以查看更详细的信息，最低为 <code>-v</code>，最高为 <code>-vvvvvv</code>，一般使用 <code>-vvv</code> 即可。</p>
<h3 id="ansible-debugger"><a class="header" href="#ansible-debugger">ansible debugger</a></h3>
<p>ansible 内置了一个断点调试工具 debugger。它最简单的开启方式是设置环境变量 <code>ANSIBLE_ENABLE_TASK_DEBUGGER</code>。开启断点调试工具后，当某个 Task 失败时，会自动进入 debugger 环境。你可以在 debugger 环境中通过 p (print)、r (redo)、c (continue)、q (quit) 等命令进行调试：</p>
<pre><code class="language-bash">ANSIBLE_ENABLE_TASK_DEBUGGER=True ansible-playbook playbook.yml
</code></pre>
<p>详细的使用方式参考：<a target="_blank" rel="noopener noreferrer" href="https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_debugger.html">Debugging tasks</a>。</p>
<h3 id="debug-task"><a class="header" href="#debug-task">debug task</a></h3>
<p>如果想查看当 playbook 运行到某个 Task 时节点的具体情况，可以在 playbook 中插入一个 debug Task，运行某些命令并打印输出，示例如下：</p>
<pre><code class="language-yaml">- name: Execute uname -a
  ansible.builtin.shell: uname -a
  register: result

- name: Print result
  ansible.builtin.debug:
    msg: &quot;DEBUG: {{ result }}&quot;
</code></pre>
<p>详细的使用方式参考：<a target="_blank" rel="noopener noreferrer" href="https://docs.ansible.com/ansible/latest/collections/ansible/builtin/debug_module.html">ansible.builtin.debug module</a>。</p>
<h3 id="ansible-tags"><a class="header" href="#ansible-tags">ansible tags</a></h3>
<p>设置命令行参数 <code>--tags</code> 或者 <code>skip-tags</code> 来只执行一部分任务。</p>
<p>Kubespray 使用的所有 Tag 见 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible/ansible.md#ansible-tags">Ansible tags</a>。</p>
<p>只执行带有指定 tag 的 Task：</p>
<pre><code class="language-bash">ansible-playbook playbook.yml --tags tag1,tag2,tag3
</code></pre>
<p>跳过带有指定 tag 的 Task：</p>
<pre><code class="language-bash">ansible-playbook playbook.yml --skip-tags tag1,tag2,tag3
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>具有 Tag &quot;Always&quot; 的任务一定会被执行。</p>
</aside>
<h3 id="查看所有-task"><a class="header" href="#查看所有-task">查看所有 Task</a></h3>
<p>设置命令行参数 <code>--list-tasks</code> 来查看 playbook 中所有将要执行的 Task（该命令不会执行 Task）：</p>
<pre><code class="language-bash">ansible-playbook playbook.yml --list-tasks
</code></pre>
<p><code>--list-tasks</code> 可以结合 <a href="appendix/ansible-debugging.html#ansible-tags">ansible tags</a> 等参数一起使用，输出的结果是当前设置下将要执行的 Task。</p>
<h3 id="逐步执行"><a class="header" href="#逐步执行">逐步执行</a></h3>
<p>设置命令行参数 <code>--step</code> 可以让 ansible 逐步执行所有的 Task，并在每一个 Task 开始前询问是否执行：</p>
<pre><code class="language-bash">ansible-playbook playbook.yml --step
</code></pre>
<p>执行 Task 前的询问信息：</p>
<pre><code class="language-bash">Perform task: TASK: Gathering Facts (N)o/(y)es/(c)ontinue: 
</code></pre>
<p>其含义如下：</p>
<ul>
<li>No: 跳过这个 Task</li>
<li>yes: 执行这个 Task</li>
<li>continue: 执行这个 Task 及后续所有的 Task（不再逐步询问）</li>
</ul>
<aside class="note">
<div class="title">注意</div>
<p>Kubespray 是一个相当长的 Playbook，包含了上千个 Task。逐步执行会需要较长时间。在需要逐步执行调试时，建议通过 <a href="appendix/ansible-debugging.html#ansible-tags">ansible tags</a> 指定其中一部分 Task 来执行。</p>
</aside>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="管理域名证书"><a class="header" href="#管理域名证书">管理域名证书</a></h1>
<p>本文档介绍如何使用 acme.sh 生产和更新 TLS 证书。</p>
<h2 id="安装-acmesh"><a class="header" href="#安装-acmesh">安装 acme.sh</a></h2>
<p>使用以下命令安装 acme.sh：</p>
<pre><code class="language-bash">git clone https://github.com/acmesh-official/acme.sh.git
cd ./acme.sh
./acme.sh --install -m my@example.com
</code></pre>
<p>其中 <code>-m my@example.com</code> 指定了一个电子邮箱地址，如果证书即将到期或者已经到期，该邮箱会收到相应的电子邮件提醒。</p>
<h2 id="生成域名证书"><a class="header" href="#生成域名证书">生成域名证书</a></h2>
<p>生成域名证书的更多用法和细节请参考<a href="https://github.com/acmesh-official/acme.sh">官方文档</a>。</p>
<h3 id="使用-acmesh"><a class="header" href="#使用-acmesh">使用 acme.sh</a></h3>
<p>使用 acme.sh 自动创建域名证书需要依赖域名供应商的 API，详细信息请参考文档 <a href="https://github.com/acmesh-official/acme.sh/wiki/dnsapi">How to use DNS API</a>。</p>
<p>这里以华为云的<a href="https://www.huaweicloud.com/product/dns.html">云解析服务</a>为例，说明如何创建域名证书。设置以下环境变量：</p>
<pre><code class="language-bash"># acme.sh 需要的华为云环境变量
export HUAWEICLOUD_DomainName=&quot;&lt;your-account-name&gt;&quot;
export HUAWEICLOUD_ACCESS_KEY=&quot;&lt;your-access-key&gt;&quot;
export HUAWEICLOUD_SECRET_KEY=&quot;&lt;your-secret-key&gt;&quot;
</code></pre>
<p>环境变量 <code>HUAWEICLOUD_DomainName</code> 对应华为云控制台的“账号名”或者“Account name”。</p>
<p>然后，运行下面的命令来创建证书：</p>
<pre><code class="language-bash">acme.sh --issue --dns dns_huaweicloud -d &quot;*.sample.t9kcloud.cn&quot;
</code></pre>
<p>如果需要同时指定多个域名：</p>
<pre><code class="language-bash">acme.sh --issue --dns dns_huaweicloud \
    -d &quot;home.sample.t9kcloud.cn&quot; \
    -d &quot;auth.sample.t9kcloud.cn&quot;
</code></pre>
<p>如果需要更多的日志输出以方便调试：</p>
<pre><code class="language-bash">acme.sh --issue --dns dns_huaweicloud \
    -d &quot;*.sample.t9kcloud.cn&quot; \
    --debug 2
</code></pre>
<p>创建成功的输出：</p>
<pre><code>[Wed Oct 25 14:50:05 CST 2023] Your cert is in: /Users/&lt;user&gt;/.acme.sh/*.sample.t9kcloud.cn_ecc/*.sample.t9kcloud.cn.cer
[Wed Oct 25 14:50:05 CST 2023] Your cert key is in: /Users/&lt;user&gt;/.acme.sh/*.sample.t9kcloud.cn_ecc/*.sample.t9kcloud.cn.key
[Wed Oct 25 14:50:05 CST 2023] The intermediate CA cert is in: /Users/&lt;user&gt;/.acme.sh/*.sample.t9kcloud.cn_ecc/ca.cer
[Wed Oct 25 14:50:05 CST 2023] And the full chain certs is there: /Users/&lt;user&gt;/.acme.sh/*.sample.t9kcloud.cn_ecc/fullchain.cer
</code></pre>
<p>证书会被保存在 <code>~/.acme.sh</code> 路径下，有效期是 90 天。acme.sh 会每 60 天自动更新证书，细节在<a href="appendix/manage-domain-certificate.html#%E6%9B%B4%E6%96%B0%E8%AF%81%E4%B9%A6">更新证书</a>章节中进行说明。</p>
<h2 id="更新证书"><a class="header" href="#更新证书">更新证书</a></h2>
<p>acme.sh 在安装时会添加 cronjob 来自动更新证书，您可以通过 crontab -e 命令看到该任务：</p>
<pre><code class="language-bash">crontab -e
</code></pre>
<pre><code class="language-bash">54 15 * * * &quot;/Users/&lt;user&gt;/.acme.sh&quot;/acme.sh --cron --home &quot;/Users/&lt;user&gt;/.acme.sh&quot; &gt; /dev/null
</code></pre>
<p>该任务会在每天 15:54 自动运行，<code>--cron</code> 参数会让 acme.sh 检查所有的证书，并更新其中超过更新间隔的证书。默认的更新间隔为 60 天。</p>
<p>如果您没有找到上述 cronjob，可以通过下面命令进行安装：</p>
<pre><code class="language-bash">acme.sh --install-cronjob
</code></pre>
<p>如果您想要立即更新证书，可以手动运行：</p>
<pre><code class="language-bash">acme.sh --renew --dns dns_huaweicloud -d &quot;*.sample.t9kcloud.cn&quot; --force
</code></pre>
<p>如果你想要停止一个证书的自动更新，可以运行：</p>
<pre><code class="language-bash">acme.sh --remove -d &quot;*.sample.t9kcloud.cn&quot;
</code></pre>
<h2 id="查看证书信息"><a class="header" href="#查看证书信息">查看证书信息</a></h2>
<p>查看证书有效期：</p>
<pre><code class="language-bash">openssl x509 -noout -enddate -in &lt;cert-file&gt;
</code></pre>
<p>查看证书颁发者信息：</p>
<pre><code class="language-bash">openssl x509 -noout -issuer -in &lt;cert-file&gt;
</code></pre>
<p>查看证书详细信息：</p>
<pre><code class="language-bash">openssl x509 -txt -in &lt;cert-file&gt;
</code></pre>
<p>检查私钥：</p>
<pre><code class="language-bash"># 命令要根据加密算法变化，ec 代表 ECDSA 算法
openssl ec -txt -in &lt;private-key-file&gt;
</code></pre>
<h2 id="参考-32"><a class="header" href="#参考-32">参考</a></h2>
<p><a href="https://github.com/acmesh-official/acme.sh">https://github.com/acmesh-official/acme.sh</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="cri-命令行工具"><a class="header" href="#cri-命令行工具">CRI 命令行工具</a></h1>
<p>Docker、containerd 和 CRI-O 是三个主流的 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes/cri-api"> K8s 容器运行时（CRI）</a>。本文档介绍几个常用命令行工具，用于管理镜像、容器的生命周期等。</p>
<h2 id="crictl"><a class="header" href="#crictl">crictl</a></h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md">crictl</a> 是项目 <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes-sigs/cri-tools/"> K8s cri-tools </a>提供的命令行工具。它基于容器运行时接口 (CRI) 运行，适用于所有与 CRI 兼容的容器运行时。</p>
<blockquote>
<p>参考：<a target="_blank" rel="noopener noreferrer" href="https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/">Debugging Kubernetes nodes with crictl</a> 介绍使用 <code>crictl</code> 检查和调试 K8s 节点上的容器运行时和应用程序。</p>
</blockquote>
<h3 id="pod"><a class="header" href="#pod">Pod</a></h3>
<p>查看运行中的 Pod：</p>
<pre><code class="language-bash">crictl pods
</code></pre>
<p>查看 Pod 详细信息：</p>
<pre><code class="language-bash">crictl inspectp &lt;pod-id&gt;
</code></pre>
<p>将本地转口转发到 Pod：</p>
<pre><code class="language-bash">crictl port-forward &lt;pod-id&gt; &lt;[local_port:]remote_port&gt;
</code></pre>
<h3 id="容器"><a class="header" href="#容器">容器</a></h3>
<p>查看所有容器：</p>
<pre><code class="language-bash">crictl ps -a
</code></pre>
<p>查看容器详细信息：</p>
<pre><code class="language-bash">crictl inspect &lt;container-id&gt;
</code></pre>
<p>查看容器日志：</p>
<pre><code class="language-bash">crictl logs &lt;container-id&gt;
</code></pre>
<p>停止容器：</p>
<pre><code class="language-bash">crictl stop &lt;container-id&gt;
</code></pre>
<p>在容器中执行命令：</p>
<pre><code class="language-bash">crictl exec &lt;container-id&gt; ls
</code></pre>
<h3 id="镜像-2"><a class="header" href="#镜像-2">镜像</a></h3>
<p>查看镜像：</p>
<pre><code class="language-bash">crictl images
</code></pre>
<p>拉取镜像：</p>
<pre><code class="language-bash">crictl pull &lt;image&gt;
</code></pre>
<p>查看镜像详细信息：</p>
<pre><code class="language-bash">crictl inspecti &lt;image&gt;
</code></pre>
<h2 id="nerdctl"><a class="header" href="#nerdctl">nerdctl</a></h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/containerd/nerdctl">nerdctl</a> 是一个与 Docker CLI 风格兼容的 containerd 的客户端工具。它的命令与 Docker 非常相似，因此从 Docker 迁移到 containerd 的用户使用 nerdctl 会感到非常熟悉与方便。</p>
<blockquote>
<p>containerd 提供了一套完善的 <a target="_blank" rel="noopener noreferrer" href="https://github.com/containerd/containerd/blob/main/docs/namespaces.md">Namespaced API</a>（containerd namespace 与 K8s namespace 完全不相关），允许多个用户使用同一个 containerd 实例而不互相冲突。K8s CRI 默认使用 <code>k8s.io</code> namespace，而 Docker 默认使用 <code>moby</code> namespace。nerdctl 的默认值是 <code>k8s.io</code> namespace，你可以通过 <code>-n &lt;namespace&gt;</code> 参数来指定其他 namespace。</p>
</blockquote>
<h3 id="容器-1"><a class="header" href="#容器-1">容器</a></h3>
<p>查看 namespace 中所有容器：</p>
<pre><code class="language-bash">nerdctl ps
</code></pre>
<p>运行一个容器：</p>
<pre><code class="language-bash">nerdctl run --detach --name &lt;container_name&gt; &lt;image&gt;
</code></pre>
<p>在容器中执行额外命令：</p>
<pre><code class="language-bash">nerdctl exec -ti &lt;container_id&gt; bash
</code></pre>
<p>查看容器详情：</p>
<pre><code class="language-bash">nerdctl inspect &lt;container_id&gt;
</code></pre>
<p>查看容器日志：</p>
<pre><code class="language-bash">nerdctl logs &lt;container_id&gt;
</code></pre>
<p>查看容器状态：</p>
<pre><code class="language-bash">nerdctl stats &lt;container_id&gt;
</code></pre>
<h3 id="镜像-3"><a class="header" href="#镜像-3">镜像</a></h3>
<p>查看 namespace 中所有镜像：</p>
<pre><code class="language-bash">nerdctl image list
</code></pre>
<p>构建镜像：</p>
<pre><code class="language-bash">nerdctl build -t &lt;image&gt; &lt;dockerfile-directory&gt;
</code></pre>
<p>下载镜像：</p>
<pre><code class="language-bash">nerdctl pull &lt;image&gt;
</code></pre>
<p>导出镜像：</p>
<pre><code class="language-bash">nerdctl save -o image_name.gz &lt;image&gt;
</code></pre>
<p>导入镜像：</p>
<pre><code class="language-bash">nerdctl load -i image_name.gz
</code></pre>
<h2 id="ctr"><a class="header" href="#ctr">ctr</a></h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/containerd/containerd/tree/main/cmd/ctr">ctr</a> 是 containerd 项目自带的命令行工具。它基于 containerd API 运行，适用于管理 containerd 相关的任务、容器、镜像。</p>
<blockquote>
<p>ctr 命令的 “容器” 是一个静态的容器模板，定义了要运行的镜像和配置；“任务” 是一个容器运行的实例。</p>
</blockquote>
<h3 id="namespace-1"><a class="header" href="#namespace-1">namespace</a></h3>
<blockquote>
<p>containerd 提供了一套完善的 <a target="_blank" rel="noopener noreferrer" href="https://github.com/containerd/containerd/blob/main/docs/namespaces.md">Namespaced API</a>（containerd namespace 与 K8s namespace 完全不相关），允许多个用户使用同一个 containerd 实例而不互相冲突。K8s CRI 默认使用 <code>k8s.io</code> namespace，而 Docker 默认使用 <code>moby</code> namespace。</p>
</blockquote>
<p>查看所有 namespace：</p>
<pre><code class="language-bash">ctr namespace ls
</code></pre>
<p>创建一个 namespace：</p>
<pre><code class="language-bash">ctr namespace create &lt;name&gt;
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>下文所有命令实际使用时都需要添加参数 <code>-n &lt;namespace&gt;</code> 来指定 namespace。</p>
</aside>
<h3 id="容器和任务"><a class="header" href="#容器和任务">容器和任务</a></h3>
<p>查看 namespace 中运行的所有任务：</p>
<pre><code class="language-bash">ctr task list
</code></pre>
<p>查看 namespace 中所有容器模板：</p>
<pre><code class="language-bash">ctr task list
</code></pre>
<p>创建任务来运行一个容器模板，需要存在相应的容器模板：</p>
<pre><code class="language-bash"># 注：--detach 代表 detach from the task
ctr task start &lt;container&gt; --detach
</code></pre>
<p>在任务中执行命令，并为 exec 进程指定用户 ID：</p>
<pre><code class="language-bash">ctr task exec --exec-id=0 &lt;task&gt; ls
</code></pre>
<p>查看任务指标：</p>
<pre><code class="language-bash">ctr task metrics &lt;task&gt;
</code></pre>
<p>结束任务：</p>
<pre><code class="language-bash">ctr task kill &lt;task&gt;
</code></pre>
<p>删除任务：</p>
<pre><code class="language-bash"># 注意：删除之前需要先结束任务，或者使用 --force
ctr task rm &lt;task&gt;
</code></pre>
<h3 id="镜像-4"><a class="header" href="#镜像-4">镜像</a></h3>
<p>查看 namespace 中所有镜像：</p>
<pre><code class="language-bash">ctr image list
</code></pre>
<p>拉取镜像：</p>
<pre><code class="language-bash">ctr image pull &lt;image&gt;
</code></pre>
<p>将镜像挂载到主机的目录上：</p>
<pre><code class="language-bash">ctr image mount &lt;image&gt; ./mount
</code></pre>
<p>取消挂载：</p>
<pre><code class="language-bash">ctr image unmount ./mount
</code></pre>
<h2 id="参考-33"><a class="header" href="#参考-33">参考</a></h2>
<p><a href="https://github.com/kubernetes/cri-api">https://github.com/kubernetes/cri-api</a></p>
<p><a href="https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md">https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md</a></p>
<p><a href="https://github.com/containerd/nerdctl">https://github.com/containerd/nerdctl</a></p>
<p><a href="https://github.com/containerd/containerd/tree/main/cmd/ctr">https://github.com/containerd/containerd/tree/main/cmd/ctr</a></p>
<p><a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/">https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/</a></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="集群管理安装配置"><a class="header" href="#集群管理安装配置">集群管理安装配置</a></h1>
<h2 id="目的-15"><a class="header" href="#目的-15">目的</a></h2>
<p>记录集群管理(Cluster Admin)的安装配置方案。</p>
<h2 id="依赖关系"><a class="header" href="#依赖关系">依赖关系</a></h2>
<p>集群管理模块 (Helm Chart cluster-admin) 包含下列子模块：</p>
<ol>
<li>Cluster Admin Web：集群管理的前端服务。</li>
<li>Cluster Admin Server：集群管理的后端服务。</li>
<li>Admission Control：实现了 K8s 的准入控制器，包括验证控制器相关组件和变更控制器。包含下列组件：
<ol>
<li>验证控制器相关组件：T9k 提供的默认 ConstraintsTemplate&amp;Constraints、T9k Admission Provider。</li>
<li>变更控制器</li>
</ol>
</li>
<li>Duration Keeper：负责监听集群中使用 Queue 的工作负载的运行时长，并删除/暂停超过最大运行时长限制的工作负载</li>
<li>Resource Keeper：负责监听集群中 Notebook、Tensorborad、Explorer 工作负载，当工作负载满足回收条件时，修改工作负载的 spec，将其标记为暂停。</li>
</ol>
<h3 id="必需依赖"><a class="header" href="#必需依赖">必需依赖</a></h3>
<p>集群管理必需依赖的项目如下，不安装这些项目，无法部署集群管理：</p>
<ol>
<li>T9k Security Console</li>
<li>T9k Monitoring</li>
<li>T9k Core</li>
</ol>
<h3 id="可选依赖"><a class="header" href="#可选依赖">可选依赖</a></h3>
<p>在集群管理中，有一部分组件会依赖于某些“可选依赖”。即使这些“可选依赖”对应的项目未被安装，我们仍然可以部署集群管理的。但是，在部署之前，我们需要正确地设置集群管理的安装配置，并关闭那些依赖于这些未安装项目的集群管理组件。这样，我们就可以确保集群管理的正常运行，而不会受到缺少“可选依赖”所带来的影响。</p>
<p>可选依赖以及依赖他们的组件：</p>
<ol>
<li>集群<a href="appendix/../online/k8s-components/gatekeeper.html">预安装 Gatekeeper System</a>：依赖 gatekeeper system 的集群管理组件有
<ol>
<li>Admission Control —— 验证控制器相关组件</li>
<li>Cluster Admin Web
<ol>
<li>准入控制-&gt;验证规则</li>
</ol>
</li>
</ol>
</li>
<li>T9k Scheduler：依赖 T9k Scheduler 的集群管理组件有
<ol>
<li>Admission Control —— Policy R001 Disallow unauthorized use of queue,R002 Prohibit queue overquota, R003 Verify ResourceShape</li>
<li>Duration keeper</li>
<li>Cluster Admin Web
<ol>
<li>所有页面-&gt;工作负载列表-&gt;队列/PodGroup 筛选按钮</li>
<li>资源管理-&gt;调度</li>
<li>资源管理-&gt;调度-&gt;队列</li>
<li>资源管理-&gt;调度-&gt;资源尺寸模版</li>
<li>工作负载-&gt;PodGroup</li>
</ol>
</li>
</ol>
</li>
<li>T9k Jobs：依赖 T9k Jobs 的集群管理组件有
<ol>
<li>Cluster Admin Web
<ol>
<li>总览-&gt;工作负载章节-&gt;展示 Job 数量</li>
<li>工作负载-&gt;作业</li>
</ol>
</li>
</ol>
</li>
<li>Notebooks：依赖 Notebooks 的集群管理组件有
<ol>
<li>Cluster Admin Web
<ol>
<li>工作负载-&gt;Notebooks</li>
<li>工作负载-&gt;资源状态-&gt;Notebook</li>
</ol>
</li>
</ol>
</li>
<li>MLServices：依赖 Notebooks 的集群管理组件有
<ol>
<li>Cluster Admin Web
<ol>
<li>工作负载-&gt;资源状态-&gt;MLService</li>
</ol>
</li>
</ol>
</li>
<li>Tensorboard：依赖 Tensorboards 的集群管理组件有
<ol>
<li>Cluster Admin Web
<ol>
<li>工作负载-&gt;资源状态-&gt;Tensorboard</li>
</ol>
</li>
</ol>
</li>
<li>AutotuneExperiment：依赖 AutotuneExperiment 的集群管理组件有
<ol>
<li>Cluster Admin Web
<ol>
<li>工作负载-&gt;资源状态-&gt;AutotuneExperiment</li>
</ol>
</li>
</ol>
</li>
<li>集群预先安装 elasticserach：依赖 elasticserach 的集群管理组件有
<ol>
<li>Cluster Admin Web
<ol>
<li>监控与报警-&gt;其他工具-&gt;Kibana</li>
</ol>
</li>
</ol>
</li>
<li>T9k 审计日志：依赖 T9k 审计日志的集群管理组件有
<ol>
<li>Cluster Admin Web
<ol>
<li>审计日志</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="valuesyaml"><a class="header" href="#valuesyaml">Values.yaml</a></h2>
<p>在安装 Helm Chart cluster-admin 时，如果系统缺少部分<a href="appendix/cluster-admin-installation-configuration.html#%E5%8F%AF%E9%80%89%E4%BE%9D%E8%B5%96">可选依赖</a>，你需要按照下面说明来配置 values.yaml。</p>
<h3 id="参数说明"><a class="header" href="#参数说明">参数说明</a></h3>
<p>Admission Control 组件相关的参数：</p>
<ol>
<li>options.admissionControl：值类型 bool。设为 false 时，系统不会安装 Admission Control</li>
<li>global.t9k.admission.validation.enabled：值类型 bool。设为 false 时，系统不会安装验证控制器相关组件。</li>
<li>global.t9k.admission.validation.t9kSchedulerPolicy.enabled：值类型 bool。设为 false 时，系统不会安装 Policy R001，R002，R003。</li>
</ol>
<p>Duration Keeper 组件相关的参数：</p>
<ol>
<li>options.durationKeeper：值类型 bool。设为 false 时，系统不会安装 Duration Keeper</li>
</ol>
<p>控制 Cluster Admin Web 是否显示 UI 组件的参数：</p>
<ol>
<li>参数的键是 <code>global.t9k.clusterAdminWeb.uiComponentDisplay.&lt;ui-component-name&gt;</code></li>
<li>值类型是 bool，表明是否显示这个 UI 组件</li>
<li><code>&lt;ui-component-name&gt;</code> 及其对应的 ui 组件如下：
<ol>
<li>auditing: 审计日志</li>
<li>overviewWorkloadT9kJobs: 总览-&gt;工作负载章节-&gt;展示 Job 数量</li>
<li>t9kSchedulerWorkloadListFilterButton: 所有页面-&gt;工作负载列表-&gt;队列/PodGroup 筛选按钮</li>
<li>resourceManagementT9kScheduler: 资源管理-&gt;调度</li>
<li>resourceManagementT9kSchedulerQueue: 资源管理-&gt;调度-&gt;队列</li>
<li>resourceManagementT9kSchedulerResourceShape: 资源管理-&gt;调度-&gt;资源尺寸模版</li>
<li>resourceManagementResourceReclaim: 资源管理-&gt;调度-&gt;资源回收</li>
<li>admissionValidation: 准入控制-&gt;验证规则</li>
<li>workloadPodgroup: 工作负载-&gt;PodGroup</li>
<li>workloadT9kJobs: 工作负载-&gt;作业</li>
<li>workloadNotebook: 工作负载-&gt;Notebooks</li>
<li>workloadResourceStatus: 工作负载-&gt;资源状态</li>
<li>workloadResourceStatusNotebook: 工作负载-&gt;资源状态-&gt;Notebook</li>
<li>workloadResourceStatusMlservice: 工作负载-&gt;资源状态-&gt;MLService</li>
<li>workloadResourceStatusTensorboard: 工作负载-&gt;资源状态-&gt;Tensorboard</li>
<li>workloadResourceStatusAutotuneExperiment: 工作负载-&gt;资源状态-&gt;AutotuneExperiment</li>
<li>monitoringToolsKibana: 监控与报警-&gt;其他工具-&gt;Kibana</li>
</ol>
</li>
</ol>
<h3 id="示例-2"><a class="header" href="#示例-2">示例</a></h3>
<p>当集群中未安装 T9k Scheduler 时，如果你想要安装 Helm Chart cluster-admin，values.yaml 的下列字段必须设置为 false：</p>
<pre><code class="language-yaml">global:
 t9k:
   admission:
     validation:
       # R001,R002,R003 is policy related to t9k-scheduler
       t9kSchedulerPolicy:
         enabled: false
   clusterAdminWeb:
     uiComponentDisplay:
       t9kSchedulerWorkloadListFilterButton: false
       resourceManagementT9kScheduler: false
       resourceManagementT9kSchedulerQueue: false
       resourceManagementT9kSchedulerResourceShape: false
       workloadPodgroup: false

options:
 durationKeeper:
   enabled: false
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="theme/sidebar.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
