<!DOCTYPE HTML>
<html lang="zh-cn" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>GPU Operator - TensorStack AI 计算平台 - 管理员手册</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../../../favicon.svg">
        <link rel="shortcut icon" href="../../../favicon.png">
        <link rel="stylesheet" href="../../../css/variables.css">
        <link rel="stylesheet" href="../../../css/general.css">
        <link rel="stylesheet" href="../../../css/chrome.css">
        <link rel="stylesheet" href="../../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../../highlight.css">
        <link rel="stylesheet" href="../../../tomorrow-night.css">
        <link rel="stylesheet" href="../../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../../../theme/sidebar.css">
        <link rel="stylesheet" href="../../../theme/custom.css">

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../../../overview.html">概述</a></li><li class="spacer"></li><li class="chapter-item expanded "><a href="../../../installation/index.html"><strong aria-hidden="true">1.</strong> 安装</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../installation/online/index.html"><strong aria-hidden="true">1.1.</strong> 在线安装</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../installation/online/prepare-inventory.html"><strong aria-hidden="true">1.1.1.</strong> 设置 Inventory</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../installation/online/inventory-advanced.html"><strong aria-hidden="true">1.1.1.1.</strong> 高级设置</a></li></ol></li><li class="chapter-item expanded "><a href="../../../installation/online/prepare-nodes.html"><strong aria-hidden="true">1.1.2.</strong> 准备节点</a></li><li class="chapter-item expanded "><a href="../../../installation/online/k8s-index.html"><strong aria-hidden="true">1.1.3.</strong> 安装 K8s</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../installation/online/k8s-install.html"><strong aria-hidden="true">1.1.3.1.</strong> 基本安装</a></li><li class="chapter-item expanded "><a href="../../../installation/online/cri.html"><strong aria-hidden="true">1.1.3.2.</strong> CRI 配置</a></li><li class="chapter-item expanded "><a href="../../../installation/online/cni.html"><strong aria-hidden="true">1.1.3.3.</strong> CNI 配置</a></li><li class="chapter-item expanded "><a href="../../../installation/online/k8s-userns.html"><strong aria-hidden="true">1.1.3.4.</strong> 设置 User Namespace</a></li><li class="chapter-item expanded "><a href="../../../installation/online/k8s-storage.html"><strong aria-hidden="true">1.1.3.5.</strong> 设置集群存储</a></li><li class="chapter-item expanded "><a href="../../../installation/online/k8-ops.html"><strong aria-hidden="true">1.1.3.6.</strong> 集群维护</a></li><li class="chapter-item expanded "><a href="../../../installation/online/k8s-install-faqs.html"><strong aria-hidden="true">1.1.3.7.</strong> 常见问题</a></li><li class="chapter-item expanded "><a href="../../../installation/online/k8s-post-install.html"><strong aria-hidden="true">1.1.3.8.</strong> 其它安装后配置</a></li></ol></li><li class="chapter-item expanded "><a href="../../../installation/online/k8s-components/index.html"><strong aria-hidden="true">1.1.4.</strong> 安装 K8s 组件</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../installation/online/k8s-components/istio.html"><strong aria-hidden="true">1.1.4.1.</strong> Istio</a></li><li class="chapter-item expanded "><a href="../../../installation/online/k8s-components/knative.html"><strong aria-hidden="true">1.1.4.2.</strong> Knative</a></li><li class="chapter-item expanded "><a href="../../../installation/online/k8s-components/metrics-server.html"><strong aria-hidden="true">1.1.4.3.</strong> Metrics Server</a></li><li class="chapter-item expanded "><a href="../../../installation/online/k8s-components/elastic-search.html"><strong aria-hidden="true">1.1.4.4.</strong> Elastic Search</a></li><li class="chapter-item expanded "><a href="../../../installation/online/k8s-components/monitoring.html"><strong aria-hidden="true">1.1.4.5.</strong> 监控相关</a></li><li class="chapter-item expanded "><a href="../../../installation/online/k8s-components/gatekeeper.html"><strong aria-hidden="true">1.1.4.6.</strong> Gatekeeper</a></li></ol></li><li class="chapter-item expanded "><a href="../../../installation/hardware/hardware.html"><strong aria-hidden="true">1.1.5.</strong> 硬件支持</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../installation/hardware/nvidia/index.html"><strong aria-hidden="true">1.1.5.1.</strong> NVIDIA</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../installation/hardware/nvidia/gpu-operator.html" class="active"><strong aria-hidden="true">1.1.5.1.1.</strong> GPU Operator</a></li><li class="chapter-item expanded "><a href="../../../installation/hardware/nvidia/network-operator.html"><strong aria-hidden="true">1.1.5.1.2.</strong> Network Operator</a></li></ol></li><li class="chapter-item expanded "><a href="../../../installation/hardware/amd/index.html"><strong aria-hidden="true">1.1.5.2.</strong> AMD</a></li><li class="chapter-item expanded "><a href="../../../installation/hardware/hygon/index.html"><strong aria-hidden="true">1.1.5.3.</strong> 海光 Hygon</a></li><li class="chapter-item expanded "><a href="../../../installation/hardware/huawei/index.html"><strong aria-hidden="true">1.1.5.4.</strong> 华为</a></li><li class="chapter-item expanded "><a href="../../../installation/hardware/iluvatar/index.html"><strong aria-hidden="true">1.1.5.5.</strong> 天数智芯 iluvatar</a></li><li class="chapter-item expanded "><a href="../../../installation/hardware/metax/index.html"><strong aria-hidden="true">1.1.5.6.</strong> 沐曦 MetaX</a></li><li class="chapter-item expanded "><a href="../../../installation/hardware/enflame/index.html"><strong aria-hidden="true">1.1.5.7.</strong> 燧原 enflame</a></li></ol></li><li class="chapter-item expanded "><a href="../../../installation/online/products/index.html"><strong aria-hidden="true">1.1.6.</strong> 安装 TensorStack AI</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../installation/online/products/pre-install.html"><strong aria-hidden="true">1.1.6.1.</strong> 安装前准备</a></li><li class="chapter-item expanded "><a href="../../../installation/online/products/install.html"><strong aria-hidden="true">1.1.6.2.</strong> 安装产品</a></li><li class="chapter-item expanded "><a href="../../../installation/online/products/post-install.html"><strong aria-hidden="true">1.1.6.3.</strong> 安装后配置</a></li></ol></li><li class="chapter-item expanded "><a href="../../../installation/online/registry/harbor.html"><strong aria-hidden="true">1.1.7.</strong> 安装 Harbor Registry</a></li><li class="chapter-item expanded "><a href="../../../installation/online/storage-service/index.html"><strong aria-hidden="true">1.1.8.</strong> 安装存储服务</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../installation/online/storage-service/minio.html"><strong aria-hidden="true">1.1.8.1.</strong> MinIO</a></li><li class="chapter-item expanded "><a href="../../../installation/online/storage-service/nfs.html"><strong aria-hidden="true">1.1.8.2.</strong> NFS</a></li><li class="chapter-item expanded "><a href="../../../installation/online/storage-service/ceph.html"><strong aria-hidden="true">1.1.8.3.</strong> Ceph</a></li><li class="chapter-item expanded "><a href="../../../installation/online/storage-service/lustre.html"><strong aria-hidden="true">1.1.8.4.</strong> Lustre</a></li><li class="chapter-item expanded "><a href="../../../installation/online/storage-service/gpfs.html"><strong aria-hidden="true">1.1.8.5.</strong> GPFS</a></li></ol></li><li class="chapter-item expanded "><a href="../../../installation/online/correctness-checking.html"><strong aria-hidden="true">1.1.9.</strong> 正确性检查</a></li></ol></li><li class="chapter-item expanded "><a href="../../../installation/offline/index.html"><strong aria-hidden="true">1.2.</strong> 离线安装</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../installation/offline/prepare-offline-packages/index.html"><strong aria-hidden="true">1.2.1.</strong> 准备离线安装包</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../installation/offline/prepare-offline-packages/kubespray.html"><strong aria-hidden="true">1.2.1.1.</strong> Kubespray</a></li><li class="chapter-item expanded "><a href="../../../installation/offline/prepare-offline-packages/k8s-components.html"><strong aria-hidden="true">1.2.1.2.</strong> K8s 组件</a></li><li class="chapter-item expanded "><a href="../../../installation/offline/prepare-offline-packages/products.html"><strong aria-hidden="true">1.2.1.3.</strong> 产品</a></li></ol></li><li class="chapter-item expanded "><a href="../../../installation/offline/install/index.html"><strong aria-hidden="true">1.2.2.</strong> 安装</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../installation/offline/install/k8s.html"><strong aria-hidden="true">1.2.2.1.</strong> K8s</a></li><li class="chapter-item expanded "><a href="../../../installation/offline/install/k8s-components.html"><strong aria-hidden="true">1.2.2.2.</strong> K8s 组件</a></li><li class="chapter-item expanded "><a href="../../../installation/offline/install/products.html"><strong aria-hidden="true">1.2.2.3.</strong> 产品</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="../../../installation/update/index.html"><strong aria-hidden="true">1.3.</strong> 产品升级</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/index.html"><strong aria-hidden="true">1.4.</strong> 附录</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../installation/appendix/install-docker.html"><strong aria-hidden="true">1.4.1.</strong> 在线安装 Docker</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/install-docker-compose.html"><strong aria-hidden="true">1.4.2.</strong> 在线安装 Docker Compose</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/configure-docker-insecure-registry.html"><strong aria-hidden="true">1.4.3.</strong> 配置 Docker Insecure Registry</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/install-s3cmd.html"><strong aria-hidden="true">1.4.4.</strong> 在线安装 s3cmd</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/k8s-install-notes.html"><strong aria-hidden="true">1.4.5.</strong> 安装 K8s 注释</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/generate-k8s-file-and-image-list.html"><strong aria-hidden="true">1.4.6.</strong> 生成 K8s 文件和镜像列表</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/generate-t9k-product-image-list.html"><strong aria-hidden="true">1.4.7.</strong> 生成 T9k 产品镜像列表</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/modify-helm-chart.html"><strong aria-hidden="true">1.4.8.</strong> Helm Chart 修改</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/manually-install-mlnx-ofed-driver.html"><strong aria-hidden="true">1.4.9.</strong> 手动安装 MLNX_OFED 驱动</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/ansible-vars.html"><strong aria-hidden="true">1.4.10.</strong> ansible vars</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/ansible-debugging.html"><strong aria-hidden="true">1.4.11.</strong> ansible debugging</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/manage-domain-certificate.html"><strong aria-hidden="true">1.4.12.</strong> 管理域名证书</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/container-runtime-cli.html"><strong aria-hidden="true">1.4.13.</strong> CRI 命令行工具</a></li><li class="chapter-item expanded "><a href="../../../installation/appendix/cluster-admin-installation-configuration.html"><strong aria-hidden="true">1.4.14.</strong> 集群管理安装配置</a></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">TensorStack AI 计算平台 - 管理员手册</h1>

                    <div class="right-buttons">
                        <a href="../../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="安装-nvidia-gpu-operator"><a class="header" href="#安装-nvidia-gpu-operator">安装 NVIDIA GPU Operator</a></h1>
<pre><code>TODO: 1. 支持更多 OS/Kernel 版本组合
</code></pre>
<h2 id="目标"><a class="header" href="#目标">目标</a></h2>
<p>在集群中安装 NVIDIA GPU Operator v24.3.0<sup><a href="#参考">[1]</a></sup>，以支持在集群内使用 NVIDIA GPU。</p>
<h2 id="前置条件"><a class="header" href="#前置条件">前置条件</a></h2>
<p>节点需要满足以下条件：</p>
<ol>
<li>已安装 K8s 集群</li>
<li>集群中含有安装了 NVIDIA GPU 硬件的节点</li>
</ol>
<h2 id="兼容性"><a class="header" href="#兼容性">兼容性</a></h2>
<h3 id="gpu-operator-v2430"><a class="header" href="#gpu-operator-v2430">GPU Operator v24.3.0</a></h3>
<p>GPU Operator v24.3.0 兼容性<sup><a href="#参考">[2]</a></sup>如下所示：</p>
<div class="table-wrapper"><table><thead><tr><th>Operating System</th><th>Kubernetes</th><th>Red Hat OpenShift</th><th>VMWare vSphere with Tanzu</th><th>Rancher Kubernetes Engine2</th><th>HPE Ezmeral Runtime Enterprise</th><th>Canonical MicroK8s</th></tr></thead><tbody>
<tr><td>Ubuntu 20.04 LTS</td><td>1.22—1.30</td><td></td><td>7.0 U3c, 8.0 U2</td><td>1.22—1.30</td><td></td><td></td></tr>
<tr><td>Ubuntu 22.04 LTS</td><td>1.22—1.30</td><td></td><td>8.0 U2</td><td>1.22—1.30</td><td></td><td>1.26</td></tr>
<tr><td>Red Hat Core OS</td><td></td><td>4.12—4.15</td><td></td><td></td><td></td><td></td></tr>
<tr><td>Red Hat Enterprise Linux 8.4,8.6—8.9</td><td>1.22—1.30</td><td></td><td></td><td>1.22—1.30</td><td></td><td></td></tr>
<tr><td>Red Hat Enterprise Linux 8.4, 8.5</td><td></td><td></td><td></td><td></td><td>5.5</td><td></td></tr>
</tbody></table>
</div>
<h3 id="驱动兼容性"><a class="header" href="#驱动兼容性">驱动兼容性</a></h3>
<p>GPU Operator v24.3.0 可以通过在节点上部署 GPU 驱动容器来安装 GPU 驱动， 这种方式安装的驱动版本<sup><a href="#参考">[3]</a></sup>有：</p>
<ol>
<li><a href="https://docs.nvidia.com/datacenter/tesla/tesla-release-notes-550-90-07/index.html">550.90.07</a> (推荐)</li>
<li><a href="https://docs.nvidia.com/datacenter/tesla/tesla-release-notes-550-54-15/index.html">550.54.15</a> (默认)</li>
<li><a href="https://docs.nvidia.com/datacenter/tesla/tesla-release-notes-535-183-01/index.html">535.183.01</a></li>
<li><a href="https://docs.nvidia.com/datacenter/tesla/tesla-release-notes-470-256-02/index.html">470.256.02</a></li>
</ol>
<p>目前的 GPU 驱动容器兼容下列系统<sup><a href="#参考">[2]</a></sup>：</p>
<ul>
<li>Ubuntu 22.04 LTS, 内核版本 5.15</li>
<li>Ubuntu 20.04 LTS, 内核版本 5.4 和 5.15
如果 GPU 驱动容器无法兼容你的系统，请在节点上<a href="#可选-nvidia-驱动">手动安装 GPU 驱动</a>：</li>
</ul>
<h2 id="ansible-脚本安装"><a class="header" href="#ansible-脚本安装">ansible 脚本安装</a></h2>
<h3 id="nvidia-驱动"><a class="header" href="#nvidia-驱动">NVIDIA 驱动</a></h3>
<p>使用 ansible 简化 NVIDIA 驱动的安装。</p>
<p>首先进入 inventory 所在的目录：</p>
<pre><code class="language-bash">cd ~/ansible/$T9K_CLUSTER
</code></pre>
<p>运行以下命令安装 GPU 驱动：</p>
<pre><code class="language-bash">ansible-playbook ../ks-clusters/t9k-playbooks/3-install-gpu-driver.yml \
    -i inventory/inventory.ini \
    --become -K \
    -e nvidia_driver_skip_reboot=false \
    --limit node01,node02
</code></pre>
<aside class="note info">
<div class="title">命令行参数说明</div>
<ul>
<li><code>-e nvidia_driver_skip_reboot=false</code> 参数的作用是在安装驱动完成后，重启安装了 GPU 驱动的节点（这也是默认设置）。</li>
<li><code>--limit node01,node02</code> 参数的作用是限制只在 node01 和 node02 节点上安装 GPU 驱动。</li>
</ul>
</aside>
<p>这个 Playbook 执行的任务包括<a href="#%E5%AE%89%E8%A3%85">安装 GPU 驱动</a>，<a href="#%E5%85%B3%E9%97%AD-gsp">关闭 GSP</a> 和重启节点。</p>
<h3 id="gpu-operator"><a class="header" href="#gpu-operator">GPU Operator</a></h3>
<p>使用 ansible 简化 GPU Operator 的安装。</p>
<p>首先进入 inventory 所在的目录：</p>
<pre><code class="language-bash">cd ~/ansible/$T9K_CLUSTER
</code></pre>
<p>查看预设的变量：</p>
<pre><code class="language-bash">cat ../ks-clusters/t9k-playbooks/roles/gpu-operator/defaults/main.yml
</code></pre>
<aside class="note info">
<div class="title">自定义版本</div>
<p>请参考 <a target="_blank" rel="noopener noreferrer" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/release-notes.html">GPU Operator Release Notes</a> 来设置变量 <code>nvidia_gpu_operator_version</code>。你需要确保相应版本的 Helm Chart 存在于 <code>nvidia_gpu_operator_charts</code> 中，且相应版本的镜像存在于 <code>nvidia_gpu_operator_image_registry</code> 中。</p>
<p>GPU Operator 会用到许多镜像，你可以通过命令行参数指定这些镜像的版本（后面有实际例子）。部分镜像名称中带有操作系统的后缀，常见的有 <code>ubi8</code> 和 <code>ubuntu20.04</code>。其中 ubi 是 <a target="_blank" rel="noopener noreferrer" href="https://www.redhat.com/en/blog/introducing-red-hat-universal-base-image">Red Hat Universal Base Image</a> 的缩写，ubi8 是 RHEL 8 的基础镜像。我们推荐使用与实际操作系统一致的镜像。</p>
</aside>
<p>通过命令行参数 &quot;-e&quot; 来指定需要修改的变量，运行以下命令安装 GPU Operator：</p>
<pre><code class="language-bash">ansible-playbook ../ks-clusters/t9k-playbooks/3-install-gpu-operator.yml \
    -i inventory/inventory.ini \
    --become -K \
    -e nvidia_gpu_operator_image_registry=&quot;t9kpublic&quot; \
    -e nvidia_gpu_operator_version=&quot;v24.3.0&quot; \
    -e nvidia_node_feature_discovery_tag=&quot;v0.15.4&quot; \
    -e device_plugin_version=&quot;v0.15.0&quot; \
    -e enable_install_gpu_driver=false
</code></pre>
<p>这个 Playbook 执行的任务包括 <a href="#helm-template">helm template</a>，<a href="#%E5%AE%89%E8%A3%85-1">安装 GPU Operator</a>，以及<a href="#%E9%85%8D%E7%BD%AE-prometheus">配置 Prometheus</a>。</p>
<h2 id="手动安装"><a class="header" href="#手动安装">手动安装</a></h2>
<h3 id="可选-nvidia-驱动"><a class="header" href="#可选-nvidia-驱动">[可选] NVIDIA 驱动</a></h3>
<p>你可以选择在节点上手动安装 NVIDIA 驱动，然后再安装 GPU Operator。在下面的演示中，安装的驱动版本是 <code>nvidia-driver-525-server</code>，你可以根据系统兼容性、GPU 硬件兼容性自行选择驱动版本。</p>
<h4 id="安装"><a class="header" href="#安装">安装</a></h4>
<p>查看节点上 NVIDIA GPU 硬件：</p>
<pre><code class="language-bash">sudo lshw -C display
</code></pre>
<pre><code class="language-console">  *-display                 
       description: VGA compatible controller
       product: GP102 [TITAN X]
       vendor: NVIDIA Corporation
       physical id: 0
       bus info: pci@0000:41:00.0
       version: a1
       width: 64 bits
       clock: 33MHz
       capabilities: pm msi pciexpress vga_controller bus_master cap_list rom
       configuration: driver=nouveau latency=0
       resources: irq:58 memory:ea000000-eaffffff memory:d0000000-dfffffff memory:e0000000-e1ffffff ioport:a000(size=128) memory:c0000-dffff
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>如果计划安装 CUDA Toolkit，也可以使用 <a target="_blank" rel="noopener noreferrer" href="https://developer.nvidia.com/cuda-downloads">CUDA Toolkit 的安装包</a>同时完成 NVIDIA 驱动和 CUDA Toolkit 的安装。</p>
</aside>
<p>安装 NVIDIA 驱动：</p>
<pre><code class="language-bash">sudo apt update
sudo apt list nvidia-driver-*
</code></pre>
<pre><code class="language-console">...
nvidia-driver-515-server/focal-updates,focal-security 515.86.01-0ubuntu0.20.04.2 amd64
nvidia-driver-515/focal-updates,focal-security 515.86.01-0ubuntu0.20.04.1 amd64
nvidia-driver-520-open/focal-updates,focal-security 525.60.11-0ubuntu0.20.04.2 amd64
nvidia-driver-520/focal-updates,focal-security 525.60.11-0ubuntu0.20.04.2 amd64
nvidia-driver-525-open/focal-updates,focal-security 525.60.11-0ubuntu0.20.04.2 amd64
nvidia-driver-525-server/focal-updates,focal-security 525.60.13-0ubuntu0.20.04.1 amd64
...
</code></pre>
<pre><code class="language-bash">sudo apt install -y nvidia-driver-525-server
sudo apt-hold mark nvidia-driver-525-server
sudo reboot
</code></pre>
<p>开启 nvidia persistenced mode：</p>
<blockquote>
<p>NVIDIA 驱动安装后，会在集群内添加 system unit <code>nvidia-persistenced.service</code>，我们需要修改这个 unit 以启用 persistenced mode。</p>
</blockquote>
<pre><code class="language-bash">sudo systemctl status nvidia-persistenced.service
</code></pre>
<pre><code class="language-console">● nvidia-persistenced.service - NVIDIA Persistence Daemon
     Loaded: loaded (/lib/systemd/system/nvidia-persistenced.service; static; vendor preset: enabled)
     Active: active (running) since Wed 2023-08-02 05:11:57 UTC; 1h 33min ago
   Main PID: 2748 (nvidia-persiste)
      Tasks: 1 (limit: 618539)
     Memory: 1.0M
     CGroup: /system.slice/nvidia-persistenced.service
             └─2748 /usr/bin/nvidia-persistenced --user nvidia-persistenced --no-persistence-mode --verbose
</code></pre>
<p>修改 <code>nvidia-persistenced.service</code> 的启动命令，删除 --no-persistence-mode 参数。</p>
<pre><code class="language-bash">cat /lib/systemd/system/nvidia-persistenced.service
</code></pre>
<p>修改后的文件内容：</p>
<pre><code class="language-console">[Unit]
Description=NVIDIA Persistence Daemon
Wants=syslog.target
StopWhenUnneeded=true
Before=systemd-backlight@backlight:nvidia_0.service
[Service]
Type=forking
ExecStart=/usr/bin/nvidia-persistenced --user nvidia-persistenced --verbose
ExecStopPost=/bin/rm -rf /var/run/nvidia-persistenced
</code></pre>
<p>重启 nvidia-persistenced.service，重启之后运行 nvidia-smi 可以发现已经开启 nvidia persistenced mode：</p>
<pre><code class="language-bash">sudo systemctl daemon-reload 
sudo systemctl restart nvidia-persistenced.service

nvidia-smi
</code></pre>
<details><summary><code class="hljs">nvidia-smi output</code></summary>
<pre><code class="language-console">+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A40          On   | 00000000:19:00.0 Off |                    0 |
|  0%   21C    P8    12W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A40          On   | 00000000:1A:00.0 Off |                    0 |
|  0%   22C    P8    20W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A40          On   | 00000000:1B:00.0 Off |                    0 |
|  0%   22C    P8    19W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A40          On   | 00000000:1C:00.0 Off |                    0 |
|  0%   23C    P8    18W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  NVIDIA A40          On   | 00000000:B3:00.0 Off |                    0 |
|  0%   22C    P8    20W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  NVIDIA A40          On   | 00000000:B4:00.0 Off |                    0 |
|  0%   22C    P8    18W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  NVIDIA A40          On   | 00000000:B5:00.0 Off |                    0 |
|  0%   22C    P8    19W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  NVIDIA A40          On   | 00000000:B6:00.0 Off |                    0 |
|  0%   22C    P8    18W / 300W |      0MiB / 46068MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
</details>
<h4 id="关闭-gsp"><a class="header" href="#关闭-gsp">关闭 GSP</a></h4>
<p>注意：510.x.x 及之后的 driver，在 nvidia driver bug 未修复前，还需要 <a href="#disable-gsp">Disable GSP</a>。</p>
<h4 id="验证"><a class="header" href="#验证">验证</a></h4>
<p>Driver 安装后，可使用 nvidia-smi 查看 GPU 信息：</p>
<pre><code class="language-bash">nvidia-smi -L
</code></pre>
<pre><code class="language-console">GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-2032b4e2-30cb-f9e6-6a8a-fb0204e5b966)
GPU 1: NVIDIA A100-SXM4-80GB (UUID: GPU-656b12e4-119e-322d-3133-8a9c8e0cce83)
GPU 2: NVIDIA A100-SXM4-80GB (UUID: GPU-2c855dbe-1b55-094c-52e0-7382cfa3ea1e)
GPU 3: NVIDIA A100-SXM4-80GB (UUID: GPU-71761943-ea45-5827-0031-02c86c0c8b43)
GPU 4: NVIDIA A100-SXM4-80GB (UUID: GPU-53ddb134-1b6e-9978-e593-c3c7ff844768)
GPU 5: NVIDIA A100-SXM4-80GB (UUID: GPU-28146b32-c3b4-3184-ceb6-57690d90e386)
GPU 6: NVIDIA A100-SXM4-80GB (UUID: GPU-37809c96-b96e-5889-203a-38c24bde66d0)
GPU 7: NVIDIA A100-SXM4-80GB (UUID: GPU-95977b14-3e55-936e-b275-bb0f5bc60b39)
</code></pre>
<p>运行 <a target="_blank" rel="noopener noreferrer" href="https://github.com/NVIDIA/cuda-samples/tree/v12.2/Samples/5_Domain_Specific/p2pBandwidthLatencyTest">P2P Bandwidth Latency Test</a> 以进行进一步的测试。</p>
<p>首先安装 <a target="_blank" rel="noopener noreferrer" href="https://developer.nvidia.com/cuda-downloads">CUDA Toolkit</a>。如果你已经安装了 NVIDIA 驱动，建议根据 <code>nvidia-smi</code> 的结果选择相同的 CUDA Toolkit 版本，例如 <a target="_blank" rel="noopener noreferrer" href="https://developer.nvidia.com/cuda-12-2-0-download-archive">https://developer.nvidia.com/cuda-12-2-0-download-archive</a>，并且在安装过程中不要再次安装 NVIDIA Driver。根据安装后的提示信息设置适当的环境变量。</p>
<p>验证：</p>
<pre><code class="language-bash">nvcc --version
</code></pre>
<pre><code class="language-console">nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Tue_Jun_13_19:16:58_PDT_2023
Cuda compilation tools, release 12.2, V12.2.91
Build cuda_12.2.r12.2/compiler.32965470_0
</code></pre>
<p>然后下载 <a target="_blank" rel="noopener noreferrer" href="https://github.com/NVIDIA/cuda-samples/tree/master">CUDA Samples</a>，并切换到与 CUDA 版本一致的 tag：</p>
<pre><code class="language-bash">git clone https://github.com/NVIDIA/cuda-samples.git &amp;&amp; cd cuda-samples
git checkout tags/v12.2
</code></pre>
<p>安装必要的 Packages：</p>
<pre><code class="language-bash">sudo apt update &amp;&amp; sudo apt-get install -y \
    freeglut3-dev \
    build-essential \
    libx11-dev \
    libxmu-dev \
    libxi-dev \
    libgl1-mesa-glx \
    libglu1-mesa \
    libglu1-mesa-dev \
    libglfw3-dev \
    libgles2-mesa-dev
</code></pre>
<p>编译可执行文件：</p>
<pre><code class="language-bash">cd Samples/5_Domain_Specific/p2pBandwidthLatencyTest
make
</code></pre>
<p>运行测试：</p>
<pre><code class="language-bash">./p2pBandwidthLatencyTest
</code></pre>
<p>输出结果的示例如下：</p>
<pre><code class="language-console">[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]
Device: 0, NVIDIA A100-SXM4-80GB, pciBusID: 4f, pciDeviceID: 0, pciDomainID:0
Device: 1, NVIDIA A100-SXM4-80GB, pciBusID: 52, pciDeviceID: 0, pciDomainID:0
Device: 2, NVIDIA A100-SXM4-80GB, pciBusID: 56, pciDeviceID: 0, pciDomainID:0
Device: 3, NVIDIA A100-SXM4-80GB, pciBusID: 57, pciDeviceID: 0, pciDomainID:0
Device: 4, NVIDIA A100-SXM4-80GB, pciBusID: ce, pciDeviceID: 0, pciDomainID:0
Device: 5, NVIDIA A100-SXM4-80GB, pciBusID: d1, pciDeviceID: 0, pciDomainID:0
Device: 6, NVIDIA A100-SXM4-80GB, pciBusID: d5, pciDeviceID: 0, pciDomainID:0
Device: 7, NVIDIA A100-SXM4-80GB, pciBusID: d6, pciDeviceID: 0, pciDomainID:0
Device=0 CAN Access Peer Device=1
Device=0 CAN Access Peer Device=2
Device=0 CAN Access Peer Device=3
Device=0 CAN Access Peer Device=4
Device=0 CAN Access Peer Device=5
Device=0 CAN Access Peer Device=6
Device=0 CAN Access Peer Device=7
Device=1 CAN Access Peer Device=0
Device=1 CAN Access Peer Device=2
Device=1 CAN Access Peer Device=3
Device=1 CAN Access Peer Device=4
Device=1 CAN Access Peer Device=5
Device=1 CAN Access Peer Device=6
Device=1 CAN Access Peer Device=7
Device=2 CAN Access Peer Device=0
Device=2 CAN Access Peer Device=1
Device=2 CAN Access Peer Device=3
Device=2 CAN Access Peer Device=4
Device=2 CAN Access Peer Device=5
Device=2 CAN Access Peer Device=6
Device=2 CAN Access Peer Device=7
Device=3 CAN Access Peer Device=0
Device=3 CAN Access Peer Device=1
Device=3 CAN Access Peer Device=2
Device=3 CAN Access Peer Device=4
Device=3 CAN Access Peer Device=5
Device=3 CAN Access Peer Device=6
Device=3 CAN Access Peer Device=7
Device=4 CAN Access Peer Device=0
Device=4 CAN Access Peer Device=1
Device=4 CAN Access Peer Device=2
Device=4 CAN Access Peer Device=3
Device=4 CAN Access Peer Device=5
Device=4 CAN Access Peer Device=6
Device=4 CAN Access Peer Device=7
Device=5 CAN Access Peer Device=0
Device=5 CAN Access Peer Device=1
Device=5 CAN Access Peer Device=2
Device=5 CAN Access Peer Device=3
Device=5 CAN Access Peer Device=4
Device=5 CAN Access Peer Device=6
Device=5 CAN Access Peer Device=7
Device=6 CAN Access Peer Device=0
Device=6 CAN Access Peer Device=1
Device=6 CAN Access Peer Device=2
Device=6 CAN Access Peer Device=3
Device=6 CAN Access Peer Device=4
Device=6 CAN Access Peer Device=5
Device=6 CAN Access Peer Device=7
Device=7 CAN Access Peer Device=0
Device=7 CAN Access Peer Device=1
Device=7 CAN Access Peer Device=2
Device=7 CAN Access Peer Device=3
Device=7 CAN Access Peer Device=4
Device=7 CAN Access Peer Device=5
Device=7 CAN Access Peer Device=6

***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.
So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.

P2P Connectivity Matrix
     D\D     0     1     2     3     4     5     6     7
     0	     1     1     1     1     1     1     1     1
     1	     1     1     1     1     1     1     1     1
     2	     1     1     1     1     1     1     1     1
     3	     1     1     1     1     1     1     1     1
     4	     1     1     1     1     1     1     1     1
     5	     1     1     1     1     1     1     1     1
     6	     1     1     1     1     1     1     1     1
     7	     1     1     1     1     1     1     1     1
Unidirectional P2P=Disabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3      4      5      6      7 
     0 1540.93  17.55  18.02  18.12  19.68  21.03  21.01  21.00 
     1  18.17 1539.41  18.01  18.15  19.70  21.02  21.03  21.02 
     2  18.04  18.34 1548.56  18.24  20.26  21.02  20.96  21.00 
     3  18.22  18.38  18.05 1540.93  19.65  19.73  20.96  20.97 
     4  19.77  19.77  19.77  19.74 1386.42  18.14  18.14  18.14 
     5  19.81  19.80  21.01  21.05  18.13 1568.78  18.06  18.12 
     6  19.80  19.77  19.80  20.85  18.15  18.17 1575.10  18.17 
     7  19.68  19.80  19.80  19.76  18.07  18.19  18.17 1579.88 
Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)
   D\D     0      1      2      3      4      5      6      7 
     0 1536.38  20.56  24.18  24.18  18.46  18.55  18.60  18.60 
     1  24.18 1550.10  20.56  24.18  18.59  18.59  18.52  18.42 
     2  24.18  24.18 1550.10  20.56  18.60  18.60  18.60  18.60 
     3  24.18  24.18  24.18 1543.97  18.51  18.54  17.32  18.59 
     4  18.57  18.60  18.58  18.60 1393.84  20.56  24.18  25.01 
     5  18.60  18.58  18.59  18.60  24.53 1587.91  20.56  25.21 
     6  18.59  18.60  18.60  18.60  25.22  25.22 1586.29  20.56 
     7  18.54  18.48  18.34  18.55  24.18  24.18  25.15 1587.91 
Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3      4      5      6      7 
     0 1560.16  19.96  20.23  20.07  29.62  29.64  29.64  29.68 
     1  20.04 1563.28  20.20  20.26  29.74  29.64  29.66  29.67 
     2  20.28  20.31 1602.56  20.25  28.36  28.36  29.60  29.66 
     3  20.16  20.09  20.11 1564.06  28.32  28.34  28.33  28.34 
     4  28.47  28.43  28.45  28.42 1414.03  20.08  20.05  20.05 
     5  27.56  28.46  29.14  29.64  19.93 1601.74  20.11  20.02 
     6  27.52  28.41  29.59  29.58  20.09  20.12 1605.86  20.04 
     7  27.53  27.44  28.12  28.30  20.07  20.12  20.12 1606.68 
Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3      4      5      6      7 
     0 1562.50  41.11  41.11  41.11  37.19  36.79  37.16  37.14 
     1  41.10 1563.28  41.10  41.10  37.15  37.19  37.18  37.12 
     2  41.11  41.10 1564.06  41.11  37.19  37.18  37.14  37.17 
     3  41.10  41.11  41.11 1560.16  37.16  37.17  36.99  37.18 
     4  37.19  37.18  37.19  37.18 1444.75  50.43  50.41  50.26 
     5  37.17  37.15  37.19  37.19  50.40 1595.20  50.43  50.41 
     6  37.16  37.19  37.17  37.19  50.42  50.40 1602.56  41.12 
     7  37.17  37.17  37.18  37.00  50.12  50.42  50.42 1596.83 
P2P=Disabled Latency Matrix (us)
   GPU     0      1      2      3      4      5      6      7 
     0   2.84  20.50  20.47  20.54  21.28  21.49  21.29  20.48 
     1  20.37   2.48  20.54  20.54  21.44  12.71  13.13  21.43 
     2  20.49  17.73   2.33  20.53  20.18  13.08  17.06  15.57 
     3  19.78  20.31  20.47   2.37  20.04  14.41  21.45  21.38 
     4  21.25  21.38  18.55  21.44   2.43  15.51  15.46  17.60 
     5  21.15  12.89  17.77  16.36  18.93   2.29  12.54  19.32 
     6  21.48  14.99  17.08  21.06  17.89  14.23   2.25  18.75 
     7  21.48  21.01  19.98  21.47  20.09  14.09  17.62   2.51 

   CPU     0      1      2      3      4      5      6      7 
     0   2.38   5.89   5.65   5.55   5.96   5.93   5.89   5.73 
     1   5.73   2.24   5.43   5.30   5.69   5.78   5.75   5.65 
     2   5.41   5.32   2.24   5.28   5.64   5.66   5.69   5.58 
     3   5.40   5.29   5.22   2.24   5.68   5.72   5.75   5.57 
     4   5.66   5.49   5.48   5.43   2.36   5.86   5.97   5.89 
     5   5.60   5.49   5.46   5.41   5.77   2.34   5.94   5.79 
     6   5.59   5.50   5.48   5.44   5.82   5.89   2.33   5.84 
     7   5.53   5.45   5.39   5.37   5.77   5.83   5.92   2.31 
P2P=Enabled Latency (P2P Writes) Matrix (us)
   GPU     0      1      2      3      4      5      6      7 
     0   2.85   1.99   1.96   1.99   2.48   2.49   2.48   2.49 
     1   1.74   2.51   1.69   1.67   2.25   2.25   2.23   2.25 
     2   1.79   1.78   2.32   1.79   2.25   2.25   2.31   2.25 
     3   1.78   1.79   1.84   2.38   2.25   2.24   2.24   2.25 
     4   2.30   2.25   2.30   2.26   2.43   1.70   1.70   1.68 
     5   2.26   2.26   2.25   2.25   1.70   2.29   1.70   1.70 
     6   2.27   2.25   2.27   2.26   1.70   1.73   2.24   1.73 
     7   2.27   2.27   2.25   2.30   1.72   1.76   1.72   2.49 

   CPU     0      1      2      3      4      5      6      7 
     0   2.26   1.59   1.63   1.60   1.62   1.60   1.63   1.56 
     1   1.74   2.30   1.65   1.66   1.68   1.69   1.65   1.63 
     2   1.76   1.68   2.34   1.67   1.69   1.64   1.59   1.59 
     3   1.68   1.63   1.65   2.30   1.67   1.64   1.69   1.66 
     4   1.88   1.76   1.79   1.77   2.39   1.75   1.76   1.77 
     5   1.91   1.80   1.80   1.80   1.77   2.38   1.71   1.72 
     6   1.84   1.75   1.76   1.83   1.81   1.79   2.40   1.75 
     7   1.92   1.79   1.79   1.77   1.76   1.79   1.78   2.44 

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
</code></pre>
<h4 id="其他"><a class="header" href="#其他">其他</a></h4>
<pre><code class="language-bash"># 检查安装状态
dpkg -l nvidia-driver-525-server
</code></pre>
<pre><code>Desired=Unknown/Install/Remove/Purge/Hold
| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)
||/ Name                     Version                     Architecture Description
+++-========================-===========================-============-=================================
hi  nvidia-driver-525-server 525.125.06-0ubuntu0.20.04.2 amd64        NVIDIA Server Driver metapackage
</code></pre>
<h3 id="gpu-operator-1"><a class="header" href="#gpu-operator-1">GPU Operator</a></h3>
<h4 id="安装-1"><a class="header" href="#安装-1">安装</a></h4>
<p>运行下列命令即可通过 Helm Chart 安装 GPU Operator</p>
<pre><code class="language-bash">helm repo add nvidia https://helm.ngc.nvidia.com/nvidia 
helm repo update
helm install --wait --generate-name \
    --version v24.3.0 \
    -n gpu-operator --create-namespace \
    nvidia/gpu-operator
</code></pre>
<aside class="note">
<div class="title">注意</div>
<p>GPU Operator 安装的组件使用的镜像无法从国内直接访问，如果你的集群节点无法访问外网，请参考<a href="#集群节点无法访问外网
">附录-&gt;集群节点无法访问外网</a>，将这些镜像拷贝到国内容器镜像服务中，然后再安装 GPU Operator</p>
</aside>
<h4 id="验证-1"><a class="header" href="#验证-1">验证</a></h4>
<p>GPU Operator 安装完成后，运行下列命令查看安装的组件：</p>
<pre><code class="language-bash">$ kubectl -n gpu-operator get deploy
NAME                                         READY   UP-TO-DATE   AVAILABLE   AGE
gpu-operator                                 1/1     1            1           37d
release-name-node-feature-discovery-gc       1/1     1            1           37d
release-name-node-feature-discovery-master   1/1     1            1           37d
$ kubectl -n gpu-operator get ds
NAME                                         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                                          AGE
gpu-feature-discovery                        2         2         2       2            2           nvidia.com/gpu.deploy.gpu-feature-discovery=true                       37d
nvidia-container-toolkit-daemonset           2         2         2       2            2           nvidia.com/gpu.deploy.container-toolkit=true                           37d
nvidia-dcgm-exporter                         2         2         2       2            2           nvidia.com/gpu.deploy.dcgm-exporter=true                               37d
nvidia-device-plugin-daemonset               2         2         2       2            2           nvidia.com/gpu.deploy.device-plugin=true                               37d
nvidia-driver-daemonset                      0         0         0       0            0           nvidia.com/gpu.deploy.driver=true                                      37d
nvidia-mig-manager                           1         1         1       1            1           nvidia.com/gpu.deploy.mig-manager=true                                 37d
nvidia-operator-validator                    2         2         2       2            2           nvidia.com/gpu.deploy.operator-validator=true                          37d
release-name-node-feature-discovery-worker   10        10        10      10           10          &lt;none&gt;                                                                 37d
</code></pre>
<p>查看 GPU Operator 的配置<sup><a href="#参考">[4]</a></sup>：</p>
<pre><code class="language-bash">$ kubectl -n gpu-operator get clusterpolicy cluster-policy  
NAME             STATUS   AGE
cluster-policy   ready    2024-05-21T07:00:15Z
</code></pre>
<h4 id="组件"><a class="header" href="#组件">组件</a></h4>
<p>GPU Operator 会在集群内安装的多个组件<sup><a href="#参考">[3]</a></sup>，下面对一些重要的组件进行说明。</p>
<h5 id="全局组件"><a class="header" href="#全局组件">全局组件</a></h5>
<p>Deployment gpu-operator：</p>
<ul>
<li>GPU Operator 的运行主体，他会在集群中部署与 NVIDIA GPU 相关的组件。</li>
<li>如何确认正常工作？Pod 运行正常，并且 NVIDIA GPU 相关的组件已经被部署在集群中。</li>
</ul>
<p>node-feature-discovery（master &amp; worker）：</p>
<ul>
<li>GPU Operator 依赖的第三方组件。运行在所有节点上，检测集群节点的硬件信息、系统信息，并将这些信息记录在节点标签上，这些标签前缀是 feature.node.kubernetes.io/。GPU Operator 依赖 node feature discovery 添加的节点标签。</li>
<li>如何确认正常工作？Pod 运行正常，并且可以在节点上查看到相关的节点标签。</li>
</ul>
<h5 id="nvidia-gpu-节点"><a class="header" href="#nvidia-gpu-节点">NVIDIA GPU 节点</a></h5>
<p>下面的组件只能运行在含有 NVIDIA GPU 的节点上</p>
<p><a href="https://github.com/NVIDIA/gpu-feature-discovery">gpu-feature-discovery</a>：</p>
<ul>
<li>根据节点上的 GPU 信息来生成节点标签，标签前缀是 nvidia.com/</li>
<li>如何确认正常工作？Pod 运行正常，并且可以在节点上查看到相关的节点标签。</li>
</ul>
<p><a href="https://github.com/NVIDIA/nvidia-container-toolkit">nvidia-container-toolkit</a>:</p>
<ul>
<li>运行在含有 NVIDIA GPU 的节点上，在节点上安装 nvidia container toolkit</li>
<li>如何确认正常工作？Pod 运行正常，并且可以在节点主机上查看到安装的 nvidia container toolkit</li>
</ul>
<p><a href="https://github.com/NVIDIA/dcgm-exporter">nvidia-dcgm-exporter</a>：</p>
<ul>
<li>在节点上安装 dcgm-exporter，将 GPU 的监控数据以 <a href="https://prometheus.io/">Prometheus</a> metrics 形式暴露出来。</li>
<li>如何确认正常工作？Pod 运行正常，并且可以通过 Pod 上 dcgm exporter 服务查询 GPU metrics。</li>
</ul>
<p><a href="https://github.com/NVIDIA/k8s-device-plugin">nvidia-device-plugin</a>：</p>
<ul>
<li>将 NVIDIA GPU 注册为 K8s 扩展资源。</li>
<li>如何确认正常工作？Pod 运行正常，可以在含有 GPU 的节点上查看到 NVIDIA GPU 扩展资源。</li>
</ul>
<pre><code class="language-bash">$ kubectl get node z02 -o json | jq .status.capacity
{
  &quot;cpu&quot;: &quot;32&quot;,
  &quot;ephemeral-storage&quot;: &quot;59643812Ki&quot;,
  &quot;hugepages-1Gi&quot;: &quot;0&quot;,
  &quot;hugepages-2Mi&quot;: &quot;0&quot;,
  &quot;memory&quot;: &quot;131942876Ki&quot;,
  &quot;nvidia.com/gpu&quot;: &quot;1&quot;,
  &quot;pods&quot;: &quot;110&quot;,
  &quot;tensorstack.dev/test&quot;: &quot;100&quot;
}
</code></pre>
<p>nvidia-driver-daemonset</p>
<ul>
<li>nvidia-driver-daemonset 只会运行在没有安装 GPU 驱动的节点上，作用是为节点安装 GPU 驱动容器。nvidia-driver-daemonset 中运行了下列两个组件：
<ul>
<li><a href="https://github.com/NVIDIA/k8s-driver-manager">k8s-driver-manager</a>：为 GPU Driver Container 的安装做准备工作。</li>
<li><a href="https://github.com/NVIDIA/gpu-driver-container">GPU Driver Container</a>：通过容器提供 NVIDIA GPU 驱动。</li>
</ul>
</li>
<li>如何确认正常工作？GPU 驱动容器可以正常运行在未安装 GPU 驱动的节点上。</li>
</ul>
<p><a href="https://github.com/NVIDIA/mig-parted">nvidia-mig-manager</a></p>
<ul>
<li>只会运行在 GPU 支持 <a href="https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html">MIG</a> 模式的节点上，作用是支持以 MIG 形式共享 GPU。具体地，当节点启用 MIG GPU 共享模式时，nvidia-mig-manager 会根据配置将一个 MIG GPU 划分为多个 <a href="https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#concepts:~:text=A%20GPU%20Instance,number%20of%20SMs.">MIG GPU 实例</a>。</li>
<li>如何确认正常工作？Pod 运行正常</li>
</ul>
<p><a href="https://github.com/NVIDIA/gpu-operator/tree/v24.3.0/validator">nvidia-operator-validator</a></p>
<ul>
<li>验证 GPU Operator 的多个组件是否正常工作。</li>
<li>如何确认正常工作？Pod 运行正常，Pod 日志显示 all validations are successful。</li>
</ul>
<h2 id="安装后配置"><a class="header" href="#安装后配置">安装后配置</a></h2>
<h3 id="设置-time-slicing"><a class="header" href="#设置-time-slicing">设置 time-slicing</a></h3>
<p>GPU Operator 安装完成后，可以通过以下设置，让 GPU 以 <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/gpu-sharing.html#configuration-for-shared-access-to-gpus-with-gpu-time-slicing">time-slicing 方式</a>被共享使用。</p>
<h4 id="gpu-operator-配置"><a class="header" href="#gpu-operator-配置">GPU Operator 配置</a></h4>
<p>首先需要配置 GPU Operator 启用 time-slicing。</p>
<p>创建 <code>config.yaml</code> 文件来定义 time-slicing config。</p>
<details><summary><code class="hljs">config.yaml</code></summary>
<pre><code class="language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
 name: time-slicing-config
 namespace: gpu-operator
data:
   a100-40gb: |-
       version: v1
       sharing:
         timeSlicing:
           renameByDefault: true
           resources:
           - name: nvidia.com/gpu
             replicas: 8
           - name: nvidia.com/mig-1g.5gb
             replicas: 2
           - name: nvidia.com/mig-2g.10gb
             replicas: 2
           - name: nvidia.com/mig-3g.20gb
             replicas: 3
           - name: nvidia.com/mig-7g.40gb
             replicas: 7
   common: |-
       version: v1
       sharing:
         timeSlicing:
           renameByDefault: true
           resources:
           - name: nvidia.com/gpu
             replicas: 4
</code></pre>
</details>
<p>在本示例中，ConfigMap 定义了 2 个 time-slicing config（config 设置<a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/gpu-sharing.html#configuration-for-shared-access-to-gpus-with-gpu-time-slicing">参考</a>）：<code>a100-40gb</code> 和 <code>common</code>。</p>
<p>创建 ConfigMap time-slicing-config：</p>
<pre><code class="language-bash">kubectl create -f config.yaml
</code></pre>
<p>然后修改 GPU Operator 配置：</p>
<pre><code class="language-bash">kubectl patch clusterpolicy/cluster-policy \
  -n gpu-operator --type merge \
  -p '{&quot;spec&quot;: {&quot;devicePlugin&quot;: {&quot;config&quot;: {&quot;name&quot;: &quot;time-slicing-config&quot;}}}}'
</code></pre>
<h4 id="节点设置-time-slicing"><a class="header" href="#节点设置-time-slicing">节点设置 time-slicing</a></h4>
<p>GPU Operator 启用 time-slicing 后，你可以在 GPU 节点上添加标签 <code>nvidia.com/device-plugin.config=&lt;config-name&gt;</code> 来表明想要将这个节点上的 GPU 以共享形式提供给 K8s Pod 使用，<config-name> 表明节点使用的 time-slicing config，对应 ConfigMap time-slicing-config 中定义的 time-slicing config 的名称（a100-40gb 或 common）。</p>
<p>例如：</p>
<pre><code class="language-bash">kubectl label node z02 nvidia.com/device-plugin.config=common
</code></pre>
<p>z02 上只有一个物理 GPU，在节点上设置了 time-slicing config common 后，你可以看见 4 个 K8s GPU extended-resources:</p>
<pre><code class="language-bash">$ kubectl get node z02 -o json | jq .status.capacity
{
  …
  &quot;nvidia.com/gpu.shared&quot;: &quot;4&quot;,
  …
}
</code></pre>
<h4 id="optional-设置-t9k-scheduler-queue"><a class="header" href="#optional-设置-t9k-scheduler-queue">(optional) 设置 T9k Scheduler Queue</a></h4>
<p>设置了共享 GPU 的节点后，你可以创建 T9k Scheduler Queue <code>shared-gpu</code>，让 Queue 中的 Pod 只运行在共享 GPU 节点上。如果你想要创建使用共享 GPU 的 Pod，你只需要在 Pod 中设置扩展资源 <code>nvidia.com/gpu.shared</code>，并设置 Pod 使用 <code>t9k-scheduler</code>，同时指定 Pod Queue 为 <code>shared-gpu</code> 即可。</p>
<p>Queue 的 YAML 示例如下：</p>
<pre><code class="language-yaml">apiVersion: scheduler.tensorstack.dev/v1beta1
kind: Queue
metadata:
 name: shared-gpu
 namespace: t9k-system
spec:
 closed: false
 nodeSelector:
   matchExpressions:
   - key: nvidia.com/device-plugin.config
     operator: Exists
 preemptible: false
 priority: 80
 quota:
   requests:
     cpu: &quot;400&quot;
     memory: 800Gi
     nvidia.com/gpu.shared: &quot;12&quot;
</code></pre>
<h3 id="节点禁用-gpu-operator"><a class="header" href="#节点禁用-gpu-operator">节点禁用 GPU Operator</a></h3>
<p>如果不想让 GPU Operator 运行在某个节点上，可运行下列命令：</p>
<pre><code class="language-bash">kubectl label nodes $NODE nvidia.com/gpu.deploy.operands=false
</code></pre>
<p>参考：<a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operands">https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operands</a></p>
<h3 id="配置-prometheus"><a class="header" href="#配置-prometheus">配置 Prometheus</a></h3>
<p>GPU Operator 默认会在集群内部署 dcgm exporter，你需要创建 CRD <code>ServiceMonitor </code> 示例来配置 Prometheus 收集 dcgm exporter 提供的 metrics 数据。</p>
<p>nvidia dcgm exporter 的 service 如下所示：</p>
<pre><code class="language-bash">kubectl -n gpu-operator get svc nvidia-dcgm-exporter  -o yaml
</code></pre>
<details><summary><code class="hljs">svc-nvidia-dcgm-exporter.yaml</code></summary>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: &quot;true&quot;
  labels:
    app: nvidia-dcgm-exporter
  name: nvidia-dcgm-exporter
  namespace: gpu-operator
  ownerReferences:
  - apiVersion: nvidia.com/v1
    blockOwnerDeletion: true
    controller: true
    kind: ClusterPolicy
    name: cluster-policy
    uid: aa21a324-8efc-43c9-b1fc-6e7b5e0869fd
spec:
  clusterIP: 10.233.51.234
  clusterIPs:
  - 10.233.51.234
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: gpu-metrics
    port: 9400
    protocol: TCP
    targetPort: 9400
  selector:
    app: nvidia-dcgm-exporter
  sessionAffinity: None
  type: ClusterIP
</code></pre>
</details>
<p>创建 ServiceMonitor ：</p>
<pre><code class="language-bash">kubectl -n t9k-monitoring create -f - &lt;&lt; EOF
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    tensorstack.dev/default-config: &quot;true&quot;
    tensorstack.dev/metrics-collected-by: t9k-monitoring
  name: nvidia-dcgm-exporter
  namespace: t9k-monitoring
spec:
  endpoints:
  - interval: 30s
    port: gpu-metrics
  jobLabel: app
  namespaceSelector:
    matchNames:
    - gpu-operator
  selector:
    matchLabels:
      app: nvidia-dcgm-exporter
EOF
</code></pre>
<h2 id="附录"><a class="header" href="#附录">附录</a></h2>
<h3 id="disable-gsp"><a class="header" href="#disable-gsp">Disable GSP</a></h3>
<p>在 NVIDIA Driver 510.x.x 版本之后，会有一个 Bug。当 Driver 产生 &quot;Timeout waiting for RPC from GSP!&quot; 错误时，通过 Disable GSP 可能解决这个错误。</p>
<h4 id="什么是-gsp"><a class="header" href="#什么是-gsp">什么是 GSP？</a></h4>
<blockquote>
<p>Some GPUs include a GPU System Processor (GSP) which can be used to offload GPU initialization and management tasks. This processor is driven by the firmware file <code>/lib/firmware/nvidia/510.39.01/gsp.bin</code>. A few select products currently use GSP by default, and more products will take advantage of GSP in future driver releases.
Offloading tasks which were traditionally performed by the driver on the CPU can improve performance due to lower latency access to GPU hardware internals.</p>
</blockquote>
<h4 id="why-disable-gsp"><a class="header" href="#why-disable-gsp">Why disable GSP</a></h4>
<p>从 510 版本开始，NVIDIA Driver 引入了 GSP Feature，但是他有 Bug。这个 Bug 可能会导致在使用/查询 GPU 时产生错误：&quot;Timeout waiting for RPC from GSP!&quot;（详情：<a href="https://github.com/NVIDIA/open-gpu-kernel-modules/issues/446">https://github.com/NVIDIA/open-gpu-kernel-modules/issues/446</a>）。</p>
<p>关闭 GSP 可以解决上述 Bug。</p>
<h4 id="how-to-disable-gsp"><a class="header" href="#how-to-disable-gsp">How to disable GSP</a></h4>
<p>命令如下：</p>
<pre><code class="language-bash">sudo su -c 'echo options nvidia NVreg_EnableGpuFirmware=0 &gt; /etc/modprobe.d/nvidia-gsp.conf'
sudo update-initramfs -u
sudo reboot
</code></pre>
<p>检查：</p>
<pre><code class="language-bash"># EnableGpuFirmware is 0 means GSP feature is disabled
cat /proc/driver/nvidia/params | grep EnableGpuFirmware
</code></pre>
<pre><code>EnableGpuFirmware: 0
EnableGpuFirmwareLogs: 2
</code></pre>
<h3 id="集群节点无法访问外网"><a class="header" href="#集群节点无法访问外网">集群节点无法访问外网</a></h3>
<p>当你的集群节点无法下载外网的镜像时，你可以参考下面的示例，先将镜像拷贝到国内的容器镜像服务中，然后再安装 gpu operator。下面的示例使用的国内镜像仓库是 tsz.io，请将其替换为你的镜像仓库。</p>
<p>将下列的镜像列表放入文件 <code>image.mirror.txt</code> 中，每一行 # 前面是 GPU Operator 使用的镜像名称，# 后面是你想要复制的容器镜像名称：</p>
<pre><code class="language-text">registry.k8s.io/nfd/node-feature-discovery:v0.15.4#tsz.io/t9kmirror/node-feature-discovery:v0.15.4
nvcr.io/nvidia/gpu-operator:v24.3.0#tsz.io/t9kmirror/gpu-operator:v24.3.0
nvcr.io/nvidia/cuda:12.4.1-base-ubi8#tsz.io/t9kmirror/cuda:12.4.1-base-ubi8
nvcr.io/nvidia/cloud-native/gpu-operator-validator:v24.3.0#tsz.io/t9kmirror/cloud-native/gpu-operator-validator:v24.3.0
nvcr.io/nvidia/driver:550.54.15-ubuntu20.04#tsz.io/t9kmirror/driver:550.54.15-ubuntu20.04
nvcr.io/nvidia/driver:550.54.15-ubuntu22.04#tsz.io/t9kmirror/driver:550.54.15-ubuntu22.04
nvcr.io/nvidia/cloud-native/k8s-driver-manager:v0.6.8#tsz.io/t9kmirror/cloud-native/k8s-driver-manager:v0.6.8
nvcr.io/nvidia/cloud-native/k8s-kata-manager:v0.2.0#tsz.io/t9kmirror/cloud-native/k8s-kata-manager:v0.2.0
nvcr.io/nvidia/cloud-native/vgpu-device-manager:v0.2.6#tsz.io/t9kmirror/cloud-native/vgpu-device-manager:v0.2.6
nvcr.io/nvidia/cloud-native/k8s-cc-manager:v0.1.1#tsz.io/t9kmirror/cloud-native/k8s-cc-manager:v0.1.1
nvcr.io/nvidia/k8s/container-toolkit:v1.15.0-ubuntu20.04#tsz.io/t9kmirror/k8s/container-toolkit:v1.15.0-ubuntu20.04
nvcr.io/nvidia/k8s-device-plugin:v0.15.0#tsz.io/t9kmirror/k8s-device-plugin:v0.15.0
nvcr.io/nvidia/cloud-native/dcgm:3.3.5-1-ubuntu22.04#tsz.io/t9kmirror/cloud-native/dcgm:3.3.5-1-ubuntu22.04
nvcr.io/nvidia/k8s/dcgm-exporter:3.3.5-3.4.1-ubuntu22.04#tsz.io/t9kmirror/k8s/dcgm-exporter:3.3.5-3.4.1-ubuntu22.04
nvcr.io/nvidia/cloud-native/k8s-mig-manager:v0.7.0-ubuntu20.04#tsz.io/t9kmirror/cloud-native/k8s-mig-manager:v0.7.0-ubuntu20.04
nvcr.io/nvidia/kubevirt-gpu-device-plugin:v1.2.7#tsz.io/t9kmirror/kubevirt-gpu-device-plugin:v1.2.7
</code></pre>
<p>然后运行下列脚本完成镜像拷贝</p>
<pre><code class="language-bash">#!/bin/bash

# Specify the file to read
file=&quot;image.mirror.txt&quot;

# Check if the file exists
if [[ -f &quot;$file&quot; ]]; then
   # Read the file line by line
   while IFS= read -r line
   do
       # Print each line
       oldImage=&quot;${line%%#*}&quot;
       newImage=&quot;${line##*#}&quot;
       docker pull $oldImage
       docker tag $oldImage $newImage
       docker push $newImage
   done &lt; &quot;$file&quot;
else
   echo &quot;$file not found.&quot;
fi
</code></pre>
<p>最后运行下列命令安装 GPU Operator：</p>
<pre><code class="language-bash">helm repo add nvidia https://helm.ngc.nvidia.com/nvidia 
helm repo update
helm install --wait --generate-name \
    --version v24.3.0 \
    -n gpu-operator --create-namespace \
 --set &quot;node-feature-discovery.image.repository=tsz.io/t9kmirror/node-feature-discovery&quot;,&quot;node-feature-discovery.image.tag=v0.15.4&quot; \
 --set &quot;validator.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;operator.repository=tsz.io/t9kmirror&quot; \
 --set &quot;driver.repository=tsz.io/t9kmirror&quot; \
 --set &quot;driver.manager.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;toolkit.repository=tsz.io/t9kmirror/k8s&quot; \
 --set &quot;devicePlugin.repository=tsz.io/t9kmirror&quot;,&quot;devicePlugin.version=v0.15.0&quot; \
 --set &quot;dcgm.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;dcgmExporter.repository=tsz.io/t9kmirror/k8s&quot; \
 --set &quot;gfd.repository=tsz.io/t9kmirror&quot; \
 --set &quot;migManager.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;nodeStatusExporter.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;gds.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;vgpuManager.driverManager.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;vgpuDeviceManager.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;vfioManager.repository=tsz.io/t9kmirror&quot; \
 --set &quot;vfioManager.driverManager.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;kataManager.repository=tsz.io/t9kmirror/cloud-native&quot; \
 --set &quot;sandboxDevicePlugin.repository=tsz.io/t9kmirror&quot; \
 --set &quot;ccManager.repository=tsz.io/t9kmirror/cloud-native&quot; \
    nvidia/gpu-operator
</code></pre>
<h3 id="安装-gpu-operator-其他版本"><a class="header" href="#安装-gpu-operator-其他版本">安装 GPU Operator 其他版本</a></h3>
<p>如果你想安装其他版本的 GPU Operator，运行 helm install 命令时，添加命令行参数 --version 来指定你想要安装的版本。</p>
<pre><code class="language-bash">helm install --wait --generate-name \
    --version &lt;version&gt;\
    -n gpu-operator --create-namespace \
    nvidia/gpu-operator
</code></pre>
<h3 id="升级-gpu-operator"><a class="header" href="#升级-gpu-operator">升级 GPU Operator</a></h3>
<p>参考 NVIDIA GPU Operator <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/upgrade.html#option-1-manually-upgrading-crds">官方文档</a></p>
<p>注意：更新可能会导致正在使用 GPU 的工作负载出错。</p>
<h3 id="修改组件版本"><a class="header" href="#修改组件版本">修改组件版本</a></h3>
<p>你可以通过 ClusterPolicy 来修改 GPU Operator 组件的版本，但请先确保组件版本与 GPU Operator 版本兼容。</p>
<p>下面是修改 Device Plugin 版本的示例：
首先运行下列命令查看当前的 Device Plugin 版本</p>
<pre><code class="language-bash">$ k get clusterpolicy cluster-policy  -o yaml
spec:
  devicePlugin:
    image: k8s-device-plugin
    imagePullPolicy: IfNotPresent
    repository: tsz.io/t9kmirror
    version: v0.15.0
</code></pre>
<p>然后使用 <code>kubectl edit clusterpolicy cluster-policy</code> 来修改 <code>spec.devicePlugin.version</code> 字段。</p>
<h2 id="下一步"><a class="header" href="#下一步">下一步</a></h2>
<p>如何管理 NVIDIA GPU： <a href="../../../resource-management/GPU/nvidia.html">配置 NVIDIA GPU</a></p>
<h2 id="参考"><a class="header" href="#参考">参考</a></h2>
<ul>
<li>[1] <a href="https://github.com/NVIDIA/gpu-operator/tree/v24.3.0">NVIDIA GPU Operator/v24.3.0 Github</a></li>
<li>[2] <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-operating-systems-and-kubernetes-platforms">GPU Operator 兼容平台</a></li>
<li>[3] <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#gpu-operator-component-matrix">GPU Operator Component Matrix</a></li>
<li>[4] <a href="https://github.com/NVIDIA/gpu-operator/blob/v24.3.0/api/v1/clusterpolicy_types.go#L1669">ClusterPolicy 定义</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../../installation/hardware/nvidia/index.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../../../installation/hardware/nvidia/network-operator.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../../installation/hardware/nvidia/index.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../../../installation/hardware/nvidia/network-operator.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../../../elasticlunr.min.js"></script>
        <script src="../../../mark.min.js"></script>
        <script src="../../../searcher.js"></script>

        <script src="../../../clipboard.min.js"></script>
        <script src="../../../highlight.js"></script>
        <script src="../../../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../../../theme/sidebar.js"></script>


    </div>
    </body>
</html>
